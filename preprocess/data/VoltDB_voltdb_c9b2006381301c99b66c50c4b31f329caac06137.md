Refactoring Types: ['Extract Method']
PBDMMapSegment.java
/* This file is part of VoltDB.
 * Copyright (C) 2008-2015 VoltDB Inc.
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as
 * published by the Free Software Foundation, either version 3 of the
 * License, or (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Affero General Public License for more details.
 *
 * You should have received a copy of the GNU Affero General Public License
 * along with VoltDB.  If not, see <http://www.gnu.org/licenses/>.
 */
package org.voltdb.utils;

import java.io.File;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;
import java.nio.channels.FileChannel.MapMode;

import org.voltcore.logging.VoltLogger;
import org.voltcore.utils.Bits;
import org.voltcore.utils.DBBPool;
import org.voltcore.utils.DBBPool.BBContainer;
import org.voltcore.utils.DBBPool.MBBContainer;
import org.voltcore.utils.DeferredSerialization;
import org.voltdb.utils.BinaryDeque.OutputContainerFactory;
import org.xerial.snappy.Snappy;

/**
 * Objects placed in the queue are stored in file segments that are up to 64 megabytes.
 * Segments only support appending objects. A segment will throw an IOException if an attempt
 * to insert an object that exceeds the remaining space is made. A segment can be used
 * for reading and writing, but not both at the same time.
 *
 */
class PBDMMapSegment implements PBDSegment {
    private static final VoltLogger LOG = new VoltLogger("HOST");

    //Avoid unecessary sync with this flag
    private boolean m_syncedSinceLastEdit;
    final File m_file;
    private RandomAccessFile m_ras;
    private FileChannel m_fc;
    private MBBContainer m_buf;
    private ByteBuffer m_readBuf;

    //If this is the first time polling a segment, madvise the entire thing
    //into memory
    private boolean m_haveMAdvised;

    //Index of the next object to read, not an offset into the file
    //The offset is maintained by the ByteBuffer. Used to determine if there is another object
    int m_objectReadIndex;
    private int m_bytesRead;

    //ID of this segment
    private final Long m_index;

    private boolean m_closed = true;

    //How many entries that have been polled have from this file have been discarded.
    //Convenient to let PBQ maintain the counter here
    private int m_discardCount;

    public PBDMMapSegment(Long index, File file) {
        m_index = index;
        m_file = file;
        reset();
        if (LOG.isDebugEnabled()) {
            LOG.debug("Creating Segment: " + file.getName() + " At Index: " + m_index);
        }
    }

    @Override
    public long segmentId() {
        return m_index;
    }

    @Override
    public File file() {
        return m_file;
    }

    @Override
    public void reset() {
        m_syncedSinceLastEdit = true;
        m_haveMAdvised = false;
        m_objectReadIndex = 0;
        m_bytesRead = 0;
        m_discardCount = 0;
    }

    @Override
    public int getNumEntries() throws IOException {
        if (m_closed) {
            open(false);
        }
        if (m_fc.size() > SEGMENT_HEADER_BYTES) {
            final int numEntries = m_buf.b().getInt(0);
            return numEntries;
        } else {
            return 0;
        }
    }

    @Override
    public boolean isBeingPolled() {
        return m_objectReadIndex != 0;
    }

    @Override
    public int readIndex() {
        return m_objectReadIndex;
    }

    private void initNumEntries() throws IOException {
        final ByteBuffer buf = m_buf.b();
        buf.putInt(0, 0);
        buf.putInt(4, 0);
        m_syncedSinceLastEdit = false;
    }

    private void incrementNumEntries(int size) throws IOException {
        final ByteBuffer buf = m_buf.b();
        //First read the existing amount
        buf.putInt(COUNT_OFFSET, buf.getInt(COUNT_OFFSET) + 1);
        buf.putInt(SIZE_OFFSET, buf.getInt(SIZE_OFFSET) + size);
        m_syncedSinceLastEdit = false;
    }

    @Override
    public void open(boolean forWrite) throws IOException {
        if (!m_closed) {
            throw new IOException("Segment is already opened");
        }

        if (!m_file.exists()) {
            m_syncedSinceLastEdit = false;
        }
        assert(m_ras == null);
        m_ras = new RandomAccessFile(m_file, "rw");
        m_fc = m_ras.getChannel();

        if (forWrite) {
            //If this is for writing, map the chunk size RW and put the buf positions at the start
            m_buf = DBBPool.wrapMBB(m_fc.map(MapMode.READ_WRITE, 0, CHUNK_SIZE));
            m_buf.b().position(SIZE_OFFSET + 4);
            m_readBuf = m_buf.b().duplicate();
            initNumEntries();
        } else {
            //If it isn't for write, map read only to the actual size and put the write buf position at the end
            //so size is reported correctly
            final long size = m_fc.size();
            m_buf = DBBPool.wrapMBB(m_fc.map(MapMode.READ_ONLY, 0, size));
            m_readBuf = m_buf.b().duplicate();
            m_buf.b().position((int) size);
            m_readBuf.position(SIZE_OFFSET + 4);
        }

        m_closed = false;
    }

    @Override
    public void closeAndDelete() throws IOException {
        close();
        if (LOG.isDebugEnabled()) {
            LOG.debug("Deleting segment at Index " + m_index + " File: " + m_file.getAbsolutePath());
        }
        m_file.delete();
    }

    @Override
    public boolean isClosed() {
        return m_closed;
    }

    @Override
    public void close() throws IOException {
        try {
            if (m_fc != null) {
                m_fc.close();
                m_ras = null;
                m_fc = null;
                m_buf.discard();
                m_buf = null;
                m_readBuf = null;
            }
        } finally {
            m_closed = true;
            reset();
        }
    }

    @Override
    public void sync() throws IOException {
        if (m_closed) throw new IOException("closed");
        if (!m_syncedSinceLastEdit) {
            m_buf.b().force();
        }
        m_syncedSinceLastEdit = true;
    }

    @Override
    public boolean hasMoreEntries() throws IOException {
        if (m_closed) throw new IOException("closed");
        return m_objectReadIndex < m_buf.b().getInt(COUNT_OFFSET);
    }

    @Override
    public boolean isEmpty() throws IOException {
        if (m_closed) throw new IOException("closed");
        return m_discardCount == getNumEntries();
    }

    @Override
    public boolean offer(BBContainer cont, boolean compress) throws IOException {
        if (m_closed) throw new IOException("closed");
        final ByteBuffer buf = cont.b();
        final int remaining = buf.remaining();
        if (remaining < 32 || !buf.isDirect()) compress = false;
        final int maxCompressedSize = compress ? Snappy.maxCompressedLength(remaining) : remaining;
        final ByteBuffer mbuf = m_buf.b();
        if (mbuf.remaining() < maxCompressedSize + OBJECT_HEADER_BYTES) return false;


        m_syncedSinceLastEdit = false;
        try {
            //Leave space for length prefix and flags
            final int objSizePosition = mbuf.position();
            mbuf.position(mbuf.position() + OBJECT_HEADER_BYTES);

            int written = maxCompressedSize;
            if (compress) {
                //Calculate destination pointer and compress directly to file
                final long destAddr = m_buf.address() + mbuf.position();
                written = (int)Snappy.rawCompress(cont.address() + buf.position(), remaining, destAddr);
                mbuf.position(mbuf.position() + written);
            } else {
                mbuf.put(buf);
            }

            //Record the size of the compressed object and update buffer positions
            //and whether the object was compressed
            mbuf.putInt(objSizePosition, written);
            mbuf.putInt(objSizePosition + 4, compress ? FLAG_COMPRESSED: NO_FLAGS);
            buf.position(buf.limit());
            incrementNumEntries(remaining);
        } finally {
            cont.discard();
        }

        return true;
    }

    @Override
    public int offer(DeferredSerialization ds) throws IOException {
        if (m_closed) throw new IOException("closed");
        final ByteBuffer mbuf = m_buf.b();
        if (mbuf.remaining() < ds.getSerializedSize() + OBJECT_HEADER_BYTES) return -1;

        m_syncedSinceLastEdit = false;
        int written = PBDUtils.writeDeferredSerialization(mbuf, ds);
        incrementNumEntries(written);
        return written;
    }

    @Override
    public BBContainer poll(OutputContainerFactory factory) throws IOException {
        if (m_closed) throw new IOException("closed");
        final long mBufAddr = m_buf.address();
        if (!m_haveMAdvised) {
            final ByteBuffer mbuf = m_buf.b();
            m_haveMAdvised = true;
            final long retval = PosixAdvise.madvise(
                    m_buf.address(),
                    mbuf.position(),
                    PosixAdvise.POSIX_MADV_WILLNEED);
            if (retval != 0) {
                LOG.warn("madvise will need failed: " + retval);
            }
        }

        //No more entries to read
        if (!hasMoreEntries()) {
            return null;
        }

        m_objectReadIndex++;

        //Get the length prefix and then read the object
        final int nextCompressedLength = m_readBuf.getInt();
        final int nextFlags = m_readBuf.getInt();

        //Check for compression
        final boolean compressed = (nextFlags & FLAG_COMPRESSED) != 0;
        //Determine the length of the object if uncompressed
        final int nextUncompressedLength = compressed ? (int)Snappy.uncompressedLength(mBufAddr + m_readBuf.position(), nextCompressedLength) : nextCompressedLength;
        m_bytesRead += nextUncompressedLength;

        final BBContainer retcont;
        if (compressed) {
            //Get storage for output
            retcont = factory.getContainer(nextUncompressedLength);
            final ByteBuffer retbuf = retcont.b();

            //Limit to appropriate uncompressed size
            retbuf.limit(nextUncompressedLength);

            //Uncompress to output buffer
            final long sourceAddr = mBufAddr + m_readBuf.position();
            final long destAddr = retcont.address();
            Snappy.rawUncompress(sourceAddr, nextCompressedLength, destAddr);
            m_readBuf.position(m_readBuf.position() + nextCompressedLength);
        } else {
            //Return a slice
            final int oldLimit = m_readBuf.limit();
            m_readBuf.limit(m_readBuf.position() + nextUncompressedLength);
            ByteBuffer retbuf = m_readBuf.slice();
            m_readBuf.position(m_readBuf.limit());
            m_readBuf.limit(oldLimit);

            /*
             * For uncompressed data, touch all the pages to make 100% sure
             * they are available since they will be accessed directly.
             *
             * This code mimics MappedByteBuffer.load, but without the expensive
             * madvise call for data we are 99% sure was already madvised.
             *
             * This would only ever be an issue in the unlikely event that the page cache
             * is trashed at the wrong moment or we are very low on memory
             */
            retcont = DBBPool.dummyWrapBB(retbuf);
            Bits.readEveryPage(retcont);
        }

        return new BBContainer(retcont.b()) {
            private boolean m_discarded = false;

            @Override
            public void discard()
            {
                checkDoubleFree();
                if (m_discarded) {
                    LOG.error("PBD Container discarded more than once");
                    return;
                }
                m_discarded = true;
                retcont.discard();
                m_discardCount++;
            }
        };
    }

    /*
     * Don't use size in bytes to determine empty, could potentially
     * diverge from object count on crash or power failure
     * although incredibly unlikely
     */
    @Override
    public int uncompressedBytesToRead() {
        if (m_closed) throw new RuntimeException("closed");
        return Math.max(0, m_buf.b().getInt(SIZE_OFFSET) - m_bytesRead);
    }
}


File: src/frontend/org/voltdb/utils/PBDRegularSegment.java
/* This file is part of VoltDB.
 * Copyright (C) 2008-2015 VoltDB Inc.
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as
 * published by the Free Software Foundation, either version 3 of the
 * License, or (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU Affero General Public License for more details.
 *
 * You should have received a copy of the GNU Affero General Public License
 * along with VoltDB.  If not, see <http://www.gnu.org/licenses/>.
 */

package org.voltdb.utils;

import org.voltcore.logging.VoltLogger;
import org.voltcore.utils.DBBPool;
import org.voltcore.utils.DeferredSerialization;

import java.io.EOFException;
import java.io.File;
import java.io.IOException;
import java.io.RandomAccessFile;
import java.nio.ByteBuffer;
import java.nio.channels.FileChannel;

/**
 * Objects placed in the deque are stored in file segments that are up to 64 megabytes.
 * Segments only support appending objects. A segment will throw an IOException if an attempt
 * to insert an object that exceeds the remaining space is made. A segment can be used
 * for reading and writing, but not both at the same time.
 */
public class PBDRegularSegment implements PBDSegment {
    private static final VoltLogger LOG = new VoltLogger("HOST");

    //Avoid unecessary sync with this flag
    private boolean m_syncedSinceLastEdit = true;
    private final File m_file;
    private RandomAccessFile m_ras;
    private FileChannel m_fc;
    private boolean m_closed = true;

    //Index of the next object to read, not an offset into the file
    private int m_objectReadIndex = 0;
    private int m_bytesRead = 0;
    // Maintains the read byte offset
    private long m_readOffset = SEGMENT_HEADER_BYTES;

    //ID of this segment
    private final Long m_index;

    private int m_discardCount;
    private int m_numOfEntries = -1;
    private int m_size = -1;

    private DBBPool.BBContainer m_tmpHeaderBuf = null;

    public PBDRegularSegment(Long index, File file) {
        m_index = index;
        m_file = file;
        reset();
    }

    @Override
    public long segmentId()
    {
        return m_index;
    }

    @Override
    public File file()
    {
        return m_file;
    }

    @Override
    public void reset()
    {
        m_syncedSinceLastEdit = false;
        m_objectReadIndex = 0;
        m_bytesRead = 0;
        m_readOffset = SEGMENT_HEADER_BYTES;
        m_discardCount = 0;
        if (m_tmpHeaderBuf != null) {
            m_tmpHeaderBuf.discard();
            m_tmpHeaderBuf = null;
        }
    }

    @Override
    public int getNumEntries() throws IOException
    {
        if (m_closed) {
            open(false);
        }
        if (m_fc.size() > 0) {
            m_tmpHeaderBuf.b().clear();
            PBDUtils.readBufferFully(m_fc, m_tmpHeaderBuf.b(), COUNT_OFFSET);
            m_numOfEntries = m_tmpHeaderBuf.b().getInt();
            m_size = m_tmpHeaderBuf.b().getInt();
            return m_numOfEntries;
        } else {
            m_numOfEntries = 0;
            m_size = 0;
            return 0;
        }
    }

    @Override
    public boolean isBeingPolled()
    {
        return m_objectReadIndex != 0;
    }

    @Override
    public int readIndex()
    {
        return m_objectReadIndex;
    }

    @Override
    public void open(boolean forWrite) throws IOException
    {
        if (!m_closed) {
            throw new IOException("Segment is already opened");
        }

        if (!m_file.exists()) {
            if (!forWrite) {
                throw new IOException("File " + m_file + " does not exist");
            }
            m_syncedSinceLastEdit = false;
        }
        assert(m_ras == null);
        m_ras = new RandomAccessFile( m_file, forWrite ? "rw" : "r");
        m_fc = m_ras.getChannel();
        m_tmpHeaderBuf = DBBPool.allocateDirect(SEGMENT_HEADER_BYTES);

        if (forWrite) {
            initNumEntries();
        }
        m_fc.position(SEGMENT_HEADER_BYTES);

        m_closed = false;
    }

    private void initNumEntries() throws IOException {
        m_numOfEntries = 0;
        m_size = 0;

        m_tmpHeaderBuf.b().clear();
        m_tmpHeaderBuf.b().putInt(m_numOfEntries);
        m_tmpHeaderBuf.b().putInt(m_size);
        m_tmpHeaderBuf.b().flip();
        PBDUtils.writeBuffer(m_fc, m_tmpHeaderBuf.bDR(), COUNT_OFFSET);
        m_syncedSinceLastEdit = false;
    }

    private void incrementNumEntries(int size) throws IOException
    {
        m_numOfEntries++;
        m_size += size;

        m_tmpHeaderBuf.b().clear();
        m_tmpHeaderBuf.b().putInt(m_numOfEntries);
        m_tmpHeaderBuf.b().putInt(m_size);
        m_tmpHeaderBuf.b().flip();
        PBDUtils.writeBuffer(m_fc, m_tmpHeaderBuf.bDR(), COUNT_OFFSET);
        m_syncedSinceLastEdit = false;
    }

    /**
     * Bytes of space available for inserting more entries
     * @return
     */
    private int remaining() throws IOException {
        //Subtract 8 for the length and size prefix
        return (int)(PBDSegment.CHUNK_SIZE - m_fc.position()) - SEGMENT_HEADER_BYTES;
    }

    @Override
    public void closeAndDelete() throws IOException {
        close();
        m_file.delete();

        m_numOfEntries = -1;
        m_size = -1;
    }

    @Override
    public boolean isClosed()
    {
        return m_closed;
    }

    @Override
    public void close() throws IOException {
        try {
            if (m_fc != null) {
                m_fc.close();
            }
        } finally {
            m_ras = null;
            m_fc = null;
            m_closed = true;
            reset();
        }
    }

    @Override
    public void sync() throws IOException {
        if (m_closed) throw new IOException("Segment closed");
        if (!m_syncedSinceLastEdit) {
            m_fc.force(true);
        }
        m_syncedSinceLastEdit = true;
    }

    @Override
    public boolean hasMoreEntries() throws IOException
    {
        if (m_closed) throw new IOException("Segment closed");
        return m_objectReadIndex < m_numOfEntries;
    }

    @Override
    public boolean isEmpty() throws IOException
    {
        if (m_closed) throw new IOException("Segment closed");
        return m_discardCount == m_numOfEntries;
    }

    @Override
    public boolean offer(DBBPool.BBContainer cont, boolean compress) throws IOException
    {
        if (m_closed) throw new IOException("Segment closed");
        final ByteBuffer buf = cont.b();
        final int remaining = buf.remaining();
        if (remaining < 32 || !buf.isDirect()) compress = false;
        final int maxCompressedSize = (compress ? CompressionService.maxCompressedLength(remaining) : remaining) + OBJECT_HEADER_BYTES;
        if (remaining() < maxCompressedSize) return false;

        m_syncedSinceLastEdit = false;
        DBBPool.BBContainer destBuf = cont;

        try {
            m_tmpHeaderBuf.b().clear();

            if (compress) {
                destBuf = DBBPool.allocateDirectAndPool(maxCompressedSize);
                final int compressedSize = CompressionService.compressBuffer(buf, destBuf.b());
                destBuf.b().limit(compressedSize);

                m_tmpHeaderBuf.b().putInt(compressedSize);
                m_tmpHeaderBuf.b().putInt(FLAG_COMPRESSED);
            } else {
                destBuf = cont;
                m_tmpHeaderBuf.b().putInt(remaining);
                m_tmpHeaderBuf.b().putInt(NO_FLAGS);
            }

            m_tmpHeaderBuf.b().flip();
            while (m_tmpHeaderBuf.b().hasRemaining()) {
                m_fc.write(m_tmpHeaderBuf.b());
            }

            while (destBuf.b().hasRemaining()) {
                m_fc.write(destBuf.b());
            }

            incrementNumEntries(remaining);
        } finally {
            destBuf.discard();
            if (compress) {
                cont.discard();
            }
        }

        return true;
    }

    @Override
    public int offer(DeferredSerialization ds) throws IOException
    {
        if (m_closed) throw new IOException("closed");
        final int fullSize = ds.getSerializedSize() + OBJECT_HEADER_BYTES;
        if (remaining() < fullSize) return -1;

        m_syncedSinceLastEdit = false;
        DBBPool.BBContainer destBuf = DBBPool.allocateDirectAndPool(fullSize);

        try {
            final int written = PBDUtils.writeDeferredSerialization(destBuf.b(), ds);
            destBuf.b().flip();

            while (destBuf.b().hasRemaining()) {
                m_fc.write(destBuf.b());
            }

            incrementNumEntries(written);
            return written;
        } finally {
            destBuf.discard();
        }
    }

    @Override
    public DBBPool.BBContainer poll(BinaryDeque.OutputContainerFactory factory) throws IOException
    {
        if (m_closed) throw new IOException("closed");

        if (!hasMoreEntries()) {
            return null;
        }

        final long writePos = m_fc.position();
        m_fc.position(m_readOffset);
        m_objectReadIndex++;

        try {
            //Get the length and size prefix and then read the object
            m_tmpHeaderBuf.b().clear();
            while (m_tmpHeaderBuf.b().hasRemaining()) {
                int read = m_fc.read(m_tmpHeaderBuf.b());
                if (read == -1) {
                    throw new EOFException();
                }
            }
            m_tmpHeaderBuf.b().flip();
            final int length = m_tmpHeaderBuf.b().getInt();
            final int flags = m_tmpHeaderBuf.b().getInt();
            final boolean compressed = (flags & FLAG_COMPRESSED) != 0;
            final int uncompressedLen;

            if (length < 1) {
                throw new IOException("Read an invalid length");
            }

            final DBBPool.BBContainer retcont;
            if (compressed) {
                final DBBPool.BBContainer compressedBuf = DBBPool.allocateDirectAndPool(length);
                try {
                    while (compressedBuf.b().hasRemaining()) {
                        int read = m_fc.read(compressedBuf.b());
                        if (read == -1) {
                            throw new EOFException();
                        }
                    }
                    compressedBuf.b().flip();

                    uncompressedLen = CompressionService.uncompressedLength(compressedBuf.bDR());
                    retcont = factory.getContainer(uncompressedLen);
                    retcont.b().limit(uncompressedLen);
                    CompressionService.decompressBuffer(compressedBuf.bDR(), retcont.b());
                } finally {
                    compressedBuf.discard();
                }
            } else {
                uncompressedLen = length;
                retcont = factory.getContainer(length);
                retcont.b().limit(length);
                while (retcont.b().hasRemaining()) {
                    int read = m_fc.read(retcont.b());
                    if (read == -1) {
                        throw new EOFException();
                    }
                }
                retcont.b().flip();
            }

            m_bytesRead += uncompressedLen;

            return new DBBPool.BBContainer(retcont.b()) {
                private boolean m_discarded = false;

                @Override
                public void discard() {
                    checkDoubleFree();
                    if (m_discarded) {
                        LOG.error("PBD Container discarded more than once");
                        return;
                    }

                    m_discarded = true;
                    retcont.discard();
                    m_discardCount++;
                }
            };
        } finally {
            m_readOffset = m_fc.position();
            m_fc.position(writePos);
        }
    }

    @Override
    public int uncompressedBytesToRead() {
        if (m_closed) throw new RuntimeException("Segment closed");
        return m_size - m_bytesRead;
    }
}
