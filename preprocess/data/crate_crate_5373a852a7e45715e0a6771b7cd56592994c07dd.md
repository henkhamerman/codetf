Refactoring Types: ['Push Down Method']
n/sql/TransportBaseSQLAction.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.action.sql;

import com.google.common.cache.CacheBuilder;
import com.google.common.cache.CacheLoader;
import com.google.common.cache.LoadingCache;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.analyze.Analysis;
import io.crate.analyze.Analyzer;
import io.crate.exceptions.*;
import io.crate.executor.Executor;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.metadata.PartitionName;
import io.crate.metadata.TableIdent;
import io.crate.operation.collect.StatsTables;
import io.crate.planner.Plan;
import io.crate.planner.PlanPrinter;
import io.crate.planner.Planner;
import io.crate.planner.symbol.Field;
import io.crate.sql.parser.ParsingException;
import io.crate.sql.parser.SqlParser;
import io.crate.sql.tree.Statement;
import io.crate.types.DataType;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.action.support.ActionFilters;
import org.elasticsearch.action.support.TransportAction;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Provider;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.engine.DocumentAlreadyExistsException;
import org.elasticsearch.index.mapper.MapperParsingException;
import org.elasticsearch.indices.IndexAlreadyExistsException;
import org.elasticsearch.indices.IndexMissingException;
import org.elasticsearch.indices.InvalidIndexNameException;
import org.elasticsearch.indices.InvalidIndexTemplateException;
import org.elasticsearch.rest.RestStatus;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.transport.NodeDisconnectedException;

import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import java.io.PrintWriter;
import java.io.StringWriter;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.CancellationException;

import static com.google.common.base.MoreObjects.firstNonNull;

public abstract class TransportBaseSQLAction<TRequest extends SQLBaseRequest, TResponse extends SQLBaseResponse>
        extends TransportAction<TRequest, TResponse> {

    private static final DataType[] EMPTY_TYPES = new DataType[0];
    private static final String[] EMPTY_NAMES = new String[0];


    private final LoadingCache<String, Statement> statementCache = CacheBuilder.newBuilder()
            .maximumSize(100)
            .build(
                    new CacheLoader<String, Statement>() {
                        @Override
                        public Statement load(@Nonnull String statement) throws Exception {
                            return SqlParser.createStatement(statement);
                        }
                    }
            );

    private final ClusterService clusterService;
    protected final Analyzer analyzer;
    protected final Planner planner;
    private final Provider<Executor> executorProvider;
    private final StatsTables statsTables;
    private volatile boolean disabled;

    public TransportBaseSQLAction(ClusterService clusterService,
                                  Settings settings,
                                  String actionName,
                                  ThreadPool threadPool,
                                  Analyzer analyzer,
                                  Planner planner,
                                  Provider<Executor> executorProvider,
                                  StatsTables statsTables,
                                  ActionFilters actionFilters) {
        super(settings, actionName, threadPool, actionFilters);
        this.clusterService = clusterService;
        this.analyzer = analyzer;
        this.planner = planner;
        this.executorProvider = executorProvider;
        this.statsTables = statsTables;
    }

    public abstract Analysis getAnalysis(Statement statement, TRequest request);


    /**
     * create an empty SQLBaseResponse instance with no rows
     * and a rowCount of 0
     *
     * @param request     the request that results in the response to be created
     * @param outputNames an array of output column names
     * @param types       an array of types of the output columns,
     *                    if not null it must be of the same length as <code>outputNames</code>
     */
    protected abstract TResponse emptyResponse(TRequest request, String[] outputNames, @Nullable DataType[] types);

    /**
     * create an instance of SQLBaseResponse from a plan and a TaskResult
     *
     * @param outputNames an array of output column names
     * @param outputTypes the DataTypes of the columns/rows in the response
     * @param result      the result of the executed plan
     * @param request     the request that created which issued the execution
     */
    protected abstract TResponse createResponseFromResult(String[] outputNames,
                                                          DataType[] outputTypes,
                                                          List<TaskResult> result,
                                                          boolean expectsAffectedRows,
                                                          TRequest request);

    /**
     * create an instance of SQLBaseResponse
     *
     * @param result   the result of the executed plan
     * @param analysis the analysis for the result
     * @param request  the request that created which issued the execution
     */
    private TResponse createResponseFromResult(@Nullable List<TaskResult> result, Analysis analysis, TRequest request) {
        String[] outputNames;
        DataType[] outputTypes;
        if (analysis.expectsAffectedRows()) {
            outputNames = EMPTY_NAMES;
            outputTypes = EMPTY_TYPES;
        } else {
            assert analysis.rootRelation() != null;
            outputNames = new String[analysis.rootRelation().fields().size()];
            outputTypes = new DataType[analysis.rootRelation().fields().size()];
            for (int i = 0; i < analysis.rootRelation().fields().size(); i++) {
                Field field = analysis.rootRelation().fields().get(i);
                outputNames[i] = field.path().outputName();
                outputTypes[i] = field.valueType();
            }
        }
        if (result == null) {
            return emptyResponse(request, outputNames, outputTypes);
        } else {
            return createResponseFromResult(outputNames, outputTypes, result, analysis.expectsAffectedRows(), request);
        }

    }

    @Override
    protected void doExecute(TRequest request, ActionListener<TResponse> listener) {
        logger.debug("{}", request);
        statsTables.activeRequestsInc();
        if (disabled) {
            sendResponse(listener, new NodeDisconnectedException(clusterService.localNode(), actionName));
            return;
        }
        try {
            Statement statement = statementCache.get(request.stmt());
            Analysis analysis = getAnalysis(statement, request);
            processAnalysis(analysis, request, listener);
        } catch (Throwable e) {
            logger.debug("Error executing SQLRequest", e);
            sendResponse(listener, buildSQLActionException(e));
        }
    }

    private void sendResponse(ActionListener<TResponse> listener, Throwable throwable) {
        listener.onFailure(throwable);
        statsTables.activeRequestsDec();
    }

    private void sendResponse(ActionListener<TResponse> listener, TResponse response) {
        listener.onResponse(response);
        statsTables.activeRequestsDec();
    }

    private void processAnalysis(Analysis analysis, TRequest request, ActionListener<TResponse> listener) {
        final Plan plan = planner.plan(analysis);
        tracePlan(plan);
        executePlan(analysis, plan, listener, request);
    }

    private void executePlan(final Analysis analysis,
                             final Plan plan,
                             final ActionListener<TResponse> listener,
                             final TRequest request) {
        Executor executor = executorProvider.get();
        Job job = executor.newJob(plan);

        final UUID jobId = job.id();
        assert jobId != null;
        statsTables.jobStarted(jobId, request.stmt());
        List<? extends ListenableFuture<TaskResult>> resultFutureList = executor.execute(job);
        Futures.addCallback(Futures.allAsList(resultFutureList), new FutureCallback<List<TaskResult>>() {
                    @Override
                    public void onSuccess(@Nullable List<TaskResult> result) {
                        TResponse response;
                        try {
                            response = createResponseFromResult(result, analysis, request);
                        } catch (Throwable e) {
                            sendResponse(listener, buildSQLActionException(e));
                            return;
                        }
                        statsTables.jobFinished(jobId, null);
                        sendResponse(listener, response);
                    }

                    @Override
                    public void onFailure(@Nonnull Throwable t) {
                        String message;
                        if (t instanceof CancellationException) {
                            message = Constants.KILLED_MESSAGE;
                            logger.debug("KILLED: [{}]", request.stmt());
                        } else {
                            message = Exceptions.messageOf(t);
                            logger.debug("Error processing SQLRequest", t);
                        }
                        statsTables.jobFinished(jobId, message);
                        sendResponse(listener, buildSQLActionException(t));
                    }
                }

        );
    }

    private void tracePlan(Plan plan) {
        if (logger.isTraceEnabled()) {
            PlanPrinter printer = new PlanPrinter();
            logger.trace(printer.print(plan));
        }
    }


    /**
     * Returns the cause throwable of a {@link org.elasticsearch.transport.RemoteTransportException}
     * and {@link org.elasticsearch.action.search.ReduceSearchPhaseException}.
     * Also transform throwable to {@link io.crate.exceptions.CrateException}.
     */
    public Throwable esToCrateException(Throwable e) {
        e = Exceptions.unwrap(e);

        if (e instanceof IllegalArgumentException || e instanceof ParsingException) {
            return new SQLParseException(e.getMessage(), (Exception) e);
        } else if (e instanceof UnsupportedOperationException) {
            return new UnsupportedFeatureException(e.getMessage(), (Exception) e);
        } else if (e instanceof DocumentAlreadyExistsException) {
            return new DuplicateKeyException(
                    "A document with the same primary key exists already", e);
        } else if (e instanceof IndexAlreadyExistsException) {
            return new TableAlreadyExistsException(((IndexAlreadyExistsException) e).index().name(), e);
        } else if ((e instanceof InvalidIndexNameException)) {
            if (e.getMessage().contains("already exists as alias")) {
                // treat an alias like a table as aliases are not officially supported
                return new TableAlreadyExistsException(((InvalidIndexNameException) e).index().getName(),
                        e);
            }
            return new InvalidTableNameException(((InvalidIndexNameException) e).index().getName(), e);
        } else if (e instanceof InvalidIndexTemplateException) {
            Tuple<String, String> schemaAndTable = PartitionName.schemaAndTableName(((InvalidIndexTemplateException) e).name());
            return new InvalidTableNameException(new TableIdent(schemaAndTable.v1(), schemaAndTable.v2()).fqn(), e);
        } else if (e instanceof IndexMissingException) {
            return new TableUnknownException(((IndexMissingException) e).index().name(), e);
        } else if (e instanceof org.elasticsearch.common.breaker.CircuitBreakingException) {
            return new CircuitBreakingException(e.getMessage());
        } else if (e instanceof CancellationException) {
            return new JobKilledException();
        }
        return e;
    }

    /**
     * Create a {@link io.crate.action.sql.SQLActionException} out of a {@link java.lang.Throwable}.
     * If concrete {@link org.elasticsearch.ElasticsearchException} is found, first transform it
     * to a {@link io.crate.exceptions.CrateException}
     */
    public SQLActionException buildSQLActionException(Throwable e) {
        if (e instanceof SQLActionException) {
            return (SQLActionException) e;
        }
        e = esToCrateException(e);

        int errorCode = 5000;
        RestStatus restStatus = RestStatus.INTERNAL_SERVER_ERROR;
        String message = e.getMessage();
        StringWriter stackTrace = new StringWriter();
        e.printStackTrace(new PrintWriter(stackTrace));

        if (e instanceof CrateException) {
            CrateException crateException = (CrateException) e;
            if (e instanceof ValidationException) {
                errorCode = 4000 + crateException.errorCode();
                restStatus = RestStatus.BAD_REQUEST;
            } else if (e instanceof ResourceUnknownException) {
                errorCode = 4040 + crateException.errorCode();
                restStatus = RestStatus.NOT_FOUND;
            } else if (e instanceof ConflictException) {
                errorCode = 4090 + crateException.errorCode();
                restStatus = RestStatus.CONFLICT;
            } else if (e instanceof UnhandledServerException) {
                errorCode = 5000 + crateException.errorCode();
            }
        } else if (e instanceof ParsingException) {
            errorCode = 4000;
            restStatus = RestStatus.BAD_REQUEST;
        } else if (e instanceof MapperParsingException) {
            errorCode = 4000;
            restStatus = RestStatus.BAD_REQUEST;
        }

        if (e instanceof NullPointerException && message == null) {
            StackTraceElement[] stackTrace1 = e.getStackTrace();
            if (stackTrace1.length > 0) {
                message = String.format("NPE in %s", stackTrace1[0]);
            }
        } else if (e instanceof ArrayIndexOutOfBoundsException) {
            // in case of ArrayIndexOutOfBoundsExceptions the message is just the index number ...
            StackTraceElement[] stackTrace1 = e.getStackTrace();
            if (stackTrace1.length > 0) {
                message = String.format("ArrayIndexOutOfBoundsException in %s", stackTrace1[0]);
            }
        }
        if (logger.isTraceEnabled()) {
            message = firstNonNull(message, stackTrace.toString());
        } else if (Constants.DEBUG_MODE) {
            // will be optimized/removed at compile time
            Throwable t;
            if (e instanceof CrateException && e.getCause() != null) {
                // CrateException stackTrace will most likely just show a stackTrace which leads to some kind of transport execution
                // the cause will probably have a more helpful stackTrace;
                t = e.getCause();
            } else {
                t = e;
            }
            StringWriter stringWriter = new StringWriter();
            t.printStackTrace(new PrintWriter(stringWriter));
            stackTrace = stringWriter;
            message = firstNonNull(message, stackTrace.toString());
        }
        return new SQLActionException(message, errorCode, restStatus, stackTrace.toString());
    }

    public void enable() {
        disabled = false;
    }

    public void disable() {
        disabled = true;
    }
}


File: sql/src/main/java/io/crate/executor/Job.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class Job {

    private final UUID id;
    private List<Task> tasks = new ArrayList<>();

    public Job() {
        this(UUID.randomUUID());
    }

    public Job(UUID id) {
        this.id = id;
    }

    public UUID id() {
        return id;
    }

    public void addTasks(Collection<Task> tasks) {
        this.tasks.addAll(tasks);
    }

    public List<Task> tasks() {
        return tasks;
    }
}


File: sql/src/main/java/io/crate/executor/transport/TransportExecutor.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.action.sql.DDLStatementDispatcher;
import io.crate.breaker.CrateCircuitBreakerService;
import io.crate.executor.*;
import io.crate.executor.task.DDLTask;
import io.crate.executor.task.NoopTask;
import io.crate.executor.transport.task.CreateTableTask;
import io.crate.executor.transport.task.DropTableTask;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.*;
import io.crate.jobs.JobContextService;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceResolver;
import io.crate.operation.ImplementationSymbolVisitor;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.planner.*;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.*;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Provider;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.threadpool.ThreadPool;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class TransportExecutor implements Executor, TaskExecutor {

    private final Functions functions;
    private final TaskCollectingVisitor planVisitor;
    private Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider;
    private final NodeVisitor nodeVisitor;
    private final ThreadPool threadPool;

    private final ClusterService clusterService;
    private final JobContextService jobContextService;
    private final ContextPreparer contextPreparer;
    private final TransportActionProvider transportActionProvider;
    private final BulkRetryCoordinatorPool bulkRetryCoordinatorPool;

    private final ProjectionToProjectorVisitor globalProjectionToProjectionVisitor;

    // operation for handler side collecting
    private final CircuitBreaker circuitBreaker;

    private final PageDownstreamFactory pageDownstreamFactory;

    private final StreamerVisitor streamerVisitor;

    @Inject
    public TransportExecutor(Settings settings,
                             JobContextService jobContextService,
                             ContextPreparer contextPreparer,
                             TransportActionProvider transportActionProvider,
                             ThreadPool threadPool,
                             Functions functions,
                             ReferenceResolver referenceResolver,
                             PageDownstreamFactory pageDownstreamFactory,
                             Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider,
                             ClusterService clusterService,
                             CrateCircuitBreakerService breakerService,
                             BulkRetryCoordinatorPool bulkRetryCoordinatorPool,
                             StreamerVisitor streamerVisitor) {
        this.jobContextService = jobContextService;
        this.contextPreparer = contextPreparer;
        this.transportActionProvider = transportActionProvider;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.functions = functions;
        this.ddlAnalysisDispatcherProvider = ddlAnalysisDispatcherProvider;
        this.clusterService = clusterService;
        this.bulkRetryCoordinatorPool = bulkRetryCoordinatorPool;
        this.streamerVisitor = streamerVisitor;
        this.nodeVisitor = new NodeVisitor();
        this.planVisitor = new TaskCollectingVisitor();
        this.circuitBreaker = breakerService.getBreaker(CrateCircuitBreakerService.QUERY_BREAKER);
        ImplementationSymbolVisitor globalImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver, functions, RowGranularity.CLUSTER);
        this.globalProjectionToProjectionVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                globalImplementationSymbolVisitor);
    }

    @Override
    public Job newJob(Plan plan) {
        final Job job = new Job();
        List<Task> tasks = planVisitor.process(plan, job);
        job.addTasks(tasks);
        return job;
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Job job) {
        assert job.tasks().size() > 0;
        return execute(job.tasks());

    }

    @Override
    public List<Task> newTasks(PlanNode planNode, UUID jobId) {
        return planNode.accept(nodeVisitor, jobId);
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Collection<Task> tasks) {
        Task lastTask = null;
        assert tasks.size() > 0 : "need at least one task to execute";
        for (Task task : tasks) {
            // chaining tasks
            if (lastTask != null) {
                task.upstreamResult(lastTask.result());
            }
            task.start();
            lastTask = task;
        }
        assert lastTask != null;
        return lastTask.result();
    }

    class TaskCollectingVisitor extends PlanVisitor<Job, List<Task>> {

        @Override
        public List<Task> visitIterablePlan(IterablePlan plan, Job job) {
            List<Task> tasks = new ArrayList<>();
            for (PlanNode planNode : plan) {
                tasks.addAll(planNode.accept(nodeVisitor, job.id()));
            }
            return tasks;
        }

        @Override
        public List<Task> visitNoopPlan(NoopPlan plan, Job job) {
            return ImmutableList.<Task>of(NoopTask.INSTANCE);
        }

        @Override
        public List<Task> visitGlobalAggregate(GlobalAggregate plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitCollectAndMerge(CollectAndMerge plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitQueryAndFetch(QueryAndFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitCountPlan(CountPlan countPlan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, countPlan.countNode(), countPlan.mergeNode()));
        }

        private Task createExecutableNodesTask(Job job, ExecutionNode executionNode, @Nullable MergeNode localMergeNode) {
            return createExecutableNodesTask(job,
                    ImmutableList.<List<ExecutionNode>>of(ImmutableList.of(executionNode)),
                    localMergeNode == null ? null : ImmutableList.of(localMergeNode));
        }

        private ExecutionNodesTask createExecutableNodesTask(Job job, List<List<ExecutionNode>> groupedExecutionNodes, @Nullable List<MergeNode> localMergeNodes) {
            for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
                for (ExecutionNode executionNode : executionNodeGroup) {
                    executionNode.jobId(job.id());
                }
            }
            if (localMergeNodes != null) {
                for (MergeNode localMergeNode : localMergeNodes) {
                    localMergeNode.jobId(job.id());
                }
            }
            return new ExecutionNodesTask(
                    job.id(),
                    clusterService,
                    contextPreparer,
                    jobContextService,
                    pageDownstreamFactory,
                    threadPool,
                    transportActionProvider.transportJobInitAction(),
                    transportActionProvider.transportCloseContextNodeAction(),
                    streamerVisitor,
                    circuitBreaker,
                    localMergeNodes,
                    groupedExecutionNodes
            );
        }

        @Override
        public List<Task> visitNonDistributedGroupBy(NonDistributedGroupBy plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitUpsert(Upsert plan, Job job) {
            if (plan.nodes().size() == 1 && plan.nodes().get(0) instanceof IterablePlan) {
                return process(plan.nodes().get(0), job);
            }

            List<List<ExecutionNode>> groupedExecutionNodes = new ArrayList<>(plan.nodes().size());
            List<MergeNode> mergeNodes = new ArrayList<>(plan.nodes().size());
            for (Plan subPlan : plan.nodes()) {
                assert subPlan instanceof CollectAndMerge;
                groupedExecutionNodes.add(ImmutableList.<ExecutionNode>of(((CollectAndMerge) subPlan).collectNode()));
                mergeNodes.add(((CollectAndMerge) subPlan).localMergeNode());
            }
            ExecutionNodesTask task = createExecutableNodesTask(job, groupedExecutionNodes, mergeNodes);
            task.rowCountResult(true);
            return ImmutableList.<Task>of(task);
        }

        @Override
        public List<Task> visitDistributedGroupBy(DistributedGroupBy plan, Job job) {
            plan.collectNode().jobId(job.id());
            plan.reducerMergeNode().jobId(job.id());
            MergeNode localMergeNode = plan.localMergeNode();
            List<MergeNode> mergeNodes = null;
            if (localMergeNode != null) {
                localMergeNode.jobId(job.id());
                mergeNodes = ImmutableList.of(localMergeNode);
            }
            return ImmutableList.<Task>of(
                    createExecutableNodesTask(job,
                            ImmutableList.<List<ExecutionNode>>of(
                                    ImmutableList.<ExecutionNode>of(
                                            plan.collectNode(),
                                            plan.reducerMergeNode())),
                            mergeNodes));
        }

        @Override
        public List<Task> visitInsertByQuery(InsertFromSubQuery node, Job job) {
            List<Task> tasks = process(node.innerPlan(), job);
            if(node.handlerMergeNode().isPresent()) {
                // TODO: remove this hack
                Task previousTask = Iterables.getLast(tasks);
                if (previousTask instanceof ExecutionNodesTask) {
                    ((ExecutionNodesTask) previousTask).mergeNodes(ImmutableList.of(node.handlerMergeNode().get()));
                } else {
                    ArrayList<Task> tasks2 = new ArrayList<>(tasks);
                    tasks2.addAll(nodeVisitor.visitMergeNode(node.handlerMergeNode().get(), job.id()));
                    return tasks2;
                }
            }
            return tasks;
        }

        @Override
        public List<Task> visitQueryThenFetch(QueryThenFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitKillPlan(KillPlan killPlan, Job job) {
            return ImmutableList.<Task>of(new KillTask(
                    clusterService,
                    transportActionProvider.transportKillAllNodeAction(),
                    job.id()));
        }
    }

    class NodeVisitor extends PlanNodeVisitor<UUID, ImmutableList<Task>> {

        private ImmutableList<Task> singleTask(Task task) {
            return ImmutableList.of(task);
        }

        @Override
        public ImmutableList<Task> visitGenericDDLNode(GenericDDLNode node, UUID jobId) {
            return singleTask(new DDLTask(jobId, ddlAnalysisDispatcherProvider.get(), node));
        }

        @Override
        public ImmutableList<Task> visitESGetNode(ESGetNode node, UUID jobId) {
            return singleTask(new ESGetTask(
                    jobId,
                    functions,
                    globalProjectionToProjectionVisitor,
                    transportActionProvider.transportMultiGetAction(),
                    transportActionProvider.transportGetAction(),
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteByQueryNode(ESDeleteByQueryNode node, UUID jobId) {
            return singleTask(new ESDeleteByQueryTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteByQueryAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteNode(ESDeleteNode node, UUID jobId) {
            return singleTask(new ESDeleteTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitCreateTableNode(CreateTableNode node, UUID jobId) {
            return singleTask(new CreateTableTask(
                            jobId,
                            clusterService,
                            transportActionProvider.transportCreateIndexAction(),
                            transportActionProvider.transportDeleteIndexAction(),
                            transportActionProvider.transportPutIndexTemplateAction(),
                            node)
            );
        }

        @Override
        public ImmutableList<Task> visitESCreateTemplateNode(ESCreateTemplateNode node, UUID jobId) {
            return singleTask(new ESCreateTemplateTask(jobId,
                    node,
                    transportActionProvider.transportPutIndexTemplateAction()));
        }

        @Override
        public ImmutableList<Task> visitSymbolBasedUpsertByIdNode(SymbolBasedUpsertByIdNode node, UUID jobId) {
            return singleTask(new SymbolBasedUpsertByIdTask(jobId,
                    clusterService,
                    clusterService.state().metaData().settings(),
                    transportActionProvider.symbolBasedTransportShardUpsertActionDelegate(),
                    transportActionProvider.transportCreateIndexAction(),
                    transportActionProvider.transportBulkCreateIndicesAction(),
                    bulkRetryCoordinatorPool,
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitDropTableNode(DropTableNode node, UUID jobId) {
            return singleTask(new DropTableTask(jobId,
                    transportActionProvider.transportDeleteIndexTemplateAction(),
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESDeletePartitionNode(ESDeletePartitionNode node, UUID jobId) {
            return singleTask(new ESDeletePartitionTask(jobId,
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESClusterUpdateSettingsNode(ESClusterUpdateSettingsNode node, UUID jobId) {
            return singleTask(new ESClusterUpdateSettingsTask(
                    jobId,
                    transportActionProvider.transportClusterUpdateSettingsAction(),
                    node));
        }

        @Override
        protected ImmutableList<Task> visitPlanNode(PlanNode node, UUID jobId) {
            throw new UnsupportedOperationException(
                    String.format("Can't generate job/task for planNode %s", node));
        }
    }
}


File: sql/src/main/java/io/crate/planner/IterablePlan.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner;

import com.google.common.collect.Lists;
import io.crate.planner.node.PlanNode;

import java.util.ArrayList;
import java.util.Iterator;

/**
 * A plan which describes its nodes as an iteration.
 */
public class IterablePlan implements Iterable<PlanNode>, Plan {

    private ArrayList<PlanNode> nodes;

    public IterablePlan(PlanNode... nodes) {
        this.nodes = Lists.newArrayList(nodes);
    }

    public void add(PlanNode node) {
        nodes.add(node);
    }

    @Override
    public Iterator<PlanNode> iterator() {
        return nodes.iterator();
    }

    public boolean isEmpty() {
        return nodes.isEmpty();
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitIterablePlan(this, context);
    }
}


File: sql/src/main/java/io/crate/planner/NoopPlan.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner;

/**
 * A plan with an empty result
 */
public class NoopPlan implements Plan {

    public static final NoopPlan INSTANCE = new NoopPlan();

    private NoopPlan() {
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitNoopPlan(this, context);
    }
}


File: sql/src/main/java/io/crate/planner/Plan.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner;

public interface Plan {

    <C, R> R accept(PlanVisitor<C, R> visitor, C context);
}


File: sql/src/main/java/io/crate/planner/PlanNodeBuilder.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner;

import com.google.common.base.MoreObjects;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Iterables;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.PartitionName;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.consumer.OrderByPositionVisitor;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Symbols;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Map;
import java.util.TreeMap;

public class PlanNodeBuilder {

    public static CollectNode distributingCollect(TableInfo tableInfo,
                                                  Planner.Context plannerContext,
                                                  WhereClause whereClause,
                                                  List<Symbol> toCollect,
                                                  List<String> downstreamNodes,
                                                  ImmutableList<Projection> projections) {
        Routing routing = tableInfo.getRouting(whereClause, null);
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "distributing collect",
                routing);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.downstreamNodes(downstreamNodes);
        node.toCollect(toCollect);
        node.projections(projections);

        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        return node;
    }

    public static MergeNode distributedMerge(CollectNode collectNode,
                                             Planner.Context plannerContext,
                                             List<Projection> projections) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "distributed merge",
                collectNode.executionNodes().size());
        node.projections(projections);

        assert collectNode.hasDistributingDownstreams();
        node.executionNodes(ImmutableSet.copyOf(collectNode.downstreamNodes()));
        connectTypes(collectNode, node);
        return node;
    }

    public static MergeNode localMerge(List<Projection> projections,
                                       DQLPlanNode previousNode,
                                       Planner.Context plannerContext) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "localMerge",
                previousNode.executionNodes().size());
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }

    /**
     * Create a MergeNode which uses a {@link io.crate.operation.merge.SortingBucketMerger}
     * as it expects sorted input and produces sorted output.
     *
     * @param projections the projections to include in the resulting MergeNode
     * @param orderBy {@linkplain io.crate.analyze.OrderBy} containing sorting parameters
     * @param sourceSymbols the input symbols for this mergeNode
     * @param orderBySymbols the symbols to sort on. If this is null,
     *                       {@linkplain io.crate.analyze.OrderBy#orderBySymbols()}
     *                       will be used
     * @param previousNode the previous planNode to derive inputtypes from
     */
    public static MergeNode sortedLocalMerge(List<Projection> projections,
                                             OrderBy orderBy,
                                             List<Symbol> sourceSymbols,
                                             @Nullable List<Symbol> orderBySymbols,
                                             DQLPlanNode previousNode,
                                             Planner.Context plannerContext) {
        int[] orderByIndices = OrderByPositionVisitor.orderByPositions(
                MoreObjects.firstNonNull(orderBySymbols, orderBy.orderBySymbols()),
                sourceSymbols
        );
        MergeNode node = MergeNode.sortedMergeNode(
                plannerContext.nextExecutionNodeId(),
                "sortedLocalMerge",
                previousNode.executionNodes().size(),
                orderByIndices,
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }

    /**
     * calculates the outputTypes using the projections and input types.
     * must be called after projections have been set.
     */
    public static void setOutputTypes(CollectNode node) {
        if (node.projections().isEmpty()) {
            node.outputTypes(Symbols.extractTypes(node.toCollect()));
        } else {
            node.outputTypes(Planner.extractDataTypes(node.projections(), Symbols.extractTypes(node.toCollect())));
        }
    }

    /**
     * sets the inputTypes from the previousNode's outputTypes
     * and calculates the outputTypes using the projections and input types.
     * <p>
     * must be called after projections have been set
     * </p>
     */
    public static void connectTypes(DQLPlanNode previousNode, DQLPlanNode nextNode) {
        nextNode.inputTypes(previousNode.outputTypes());
        nextNode.outputTypes(Planner.extractDataTypes(nextNode.projections(), nextNode.inputTypes()));
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        assert !Iterables.any(toCollect, Predicates.instanceOf(InputColumn.class)) : "cannot collect inputcolumns";
        Routing routing = tableInfo.getRouting(whereClause, routingPreference);
        if (partitionIdent != null && routing.hasLocations()) {
            routing = filterRouting(routing, PartitionName.fromPartitionIdent(
                    tableInfo.ident().schema(), tableInfo.ident().name(), partitionIdent).stringValue());
        }
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "collect",
                routing,
                toCollect,
                projections);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        node.orderBy(orderBy);
        node.limit(limit);
        return node;
    }

    private static Routing filterRouting(Routing routing, String includeTableName) {
        assert routing.hasLocations();
        assert includeTableName != null;
        Map<String, Map<String, List<Integer>>> newLocations = new TreeMap<>();

        for (Map.Entry<String, Map<String, List<Integer>>> entry : routing.locations().entrySet()) {
            Map<String, List<Integer>> tableMap = new TreeMap<>();
            for (Map.Entry<String, List<Integer>> tableEntry : entry.getValue().entrySet()) {
                if (includeTableName.equals(tableEntry.getKey())) {
                    tableMap.put(tableEntry.getKey(), tableEntry.getValue());
                }
            }
            if (tableMap.size()>0){
                newLocations.put(entry.getKey(), tableMap);
            }

        }
        if (newLocations.size()>0) {
            return new Routing(newLocations);
        } else {
            return new Routing();
        }

    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, routingPreference, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, orderBy, limit);
    }
}


File: sql/src/main/java/io/crate/planner/Planner.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner;

import com.carrotsearch.hppc.IntObjectOpenHashMap;
import com.carrotsearch.hppc.procedures.ObjectProcedure;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import io.crate.analyze.*;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnhandledServerException;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.ConsumingPlanner;
import io.crate.planner.consumer.UpdateConsumer;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.ESDeleteNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.SourceIndexWriterProjection;
import io.crate.planner.projection.WriterProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;
import java.util.concurrent.atomic.AtomicInteger;

import static com.google.common.base.MoreObjects.firstNonNull;

@Singleton
public class Planner extends AnalyzedStatementVisitor<Planner.Context, Plan> {

    private final ConsumingPlanner consumingPlanner;
    private final ClusterService clusterService;
    private UpdateConsumer updateConsumer;

    public static class Context {

        private final IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard = new IntObjectOpenHashMap<>();
        private final IntObjectOpenHashMap<String> jobSearchContextIdToNode = new IntObjectOpenHashMap<>();
        private final ClusterService clusterService;
        private int jobSearchContextIdBaseSeq = 0;
        private int executionNodeId = 0;

        public Context(ClusterService clusterService) {
            this.clusterService = clusterService;
        }

        public ClusterService clusterService() {
            return clusterService;
        }

        /**
         * Increase current {@link #jobSearchContextIdBaseSeq} by number of shards affected by given
         * <code>routing</code> parameter and register a {@link org.elasticsearch.index.shard.ShardId}
         * under each incremented jobSearchContextId.
         * The current {@link #jobSearchContextIdBaseSeq} is set on the {@link io.crate.metadata.Routing} instance,
         * in order to be able to re-generate jobSearchContextId's for every shard in a deterministic way.
         *
         * Skip generating jobSearchContextId's if {@link io.crate.metadata.Routing#jobSearchContextIdBase} is already
         * set on the given <code>routing</code>.
         */
        public void allocateJobSearchContextIds(Routing routing) {
            if (routing.jobSearchContextIdBase() > -1 || routing.hasLocations() == false
                    || routing.numShards() == 0) {
                return;
            }
            int jobSearchContextId = jobSearchContextIdBaseSeq;
            jobSearchContextIdBaseSeq += routing.numShards();
            routing.jobSearchContextIdBase(jobSearchContextId);
            for (Map.Entry<String, Map<String, List<Integer>>> nodeEntry : routing.locations().entrySet()) {
                String nodeId = nodeEntry.getKey();
                Map<String, List<Integer>> nodeRouting = nodeEntry.getValue();
                if (nodeRouting != null) {
                    for (Map.Entry<String, List<Integer>> entry : nodeRouting.entrySet()) {
                        for (Integer shardId : entry.getValue()) {
                            jobSearchContextIdToShard.put(jobSearchContextId, new ShardId(entry.getKey(), shardId));
                            jobSearchContextIdToNode.put(jobSearchContextId, nodeId);
                            jobSearchContextId++;
                        }
                    }
                }
            }
        }

        @Nullable
        public ShardId shardId(int jobSearchContextId) {
            return jobSearchContextIdToShard.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<ShardId> jobSearchContextIdToShard() {
            return jobSearchContextIdToShard;
        }

        @Nullable
        public String nodeId(int jobSearchContextId) {
            return jobSearchContextIdToNode.get(jobSearchContextId);
        }

        public IntObjectOpenHashMap<String> jobSearchContextIdToNode() {
            return jobSearchContextIdToNode;
        }

        public int nextExecutionNodeId() {
            return executionNodeId++;
        }
    }

    @Inject
    public Planner(ClusterService clusterService, ConsumingPlanner consumingPlanner, UpdateConsumer updateConsumer) {
        this.clusterService = clusterService;
        this.updateConsumer = updateConsumer;
        this.consumingPlanner = consumingPlanner;
    }

    /**
     * dispatch plan creation based on analysis type
     *
     * @param analysis analysis to create plan from
     * @return plan
     */
    public Plan plan(Analysis analysis) {
        AnalyzedStatement analyzedStatement = analysis.analyzedStatement();
        return process(analyzedStatement, new Context(clusterService));
    }

    @Override
    protected Plan visitAnalyzedStatement(AnalyzedStatement analyzedStatement, Context context) {
        throw new UnsupportedOperationException(String.format("AnalyzedStatement \"%s\" not supported.", analyzedStatement));
    }

    @Override
    protected Plan visitSelectStatement(SelectAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement.relation(), context);
    }

    @Override
    protected Plan visitInsertFromValuesStatement(InsertFromValuesAnalyzedStatement statement, Context context) {
        Preconditions.checkState(!statement.sourceMaps().isEmpty(), "no values given");
        return processInsertStatement(statement, context);
    }

    @Override
    protected Plan visitInsertFromSubQueryStatement(InsertFromSubQueryAnalyzedStatement statement, Context context) {
        return consumingPlanner.plan(statement, context);
    }

    @Override
    protected Plan visitUpdateStatement(UpdateAnalyzedStatement statement, Context context) {
        ConsumerContext consumerContext = new ConsumerContext(statement, context);
        if (updateConsumer.consume(statement, consumerContext)) {
            return ((PlannedAnalyzedRelation) consumerContext.rootRelation()).plan();
        }
        throw new IllegalArgumentException("Couldn't plan Update statement");
    }

    @Override
    protected Plan visitDeleteStatement(DeleteAnalyzedStatement analyzedStatement, Context context) {
        IterablePlan plan = new IterablePlan();
        TableRelation tableRelation = analyzedStatement.analyzedRelation();
        List<WhereClause> whereClauses = new ArrayList<>(analyzedStatement.whereClauses().size());
        List<DocKeys.DocKey> docKeys = new ArrayList<>(analyzedStatement.whereClauses().size());
        for (WhereClause whereClause : analyzedStatement.whereClauses()) {
            if (whereClause.noMatch()) {
                continue;
            }
            if (whereClause.docKeys().isPresent() && whereClause.docKeys().get().size() == 1) {
                docKeys.add(whereClause.docKeys().get().getOnlyKey());
            } else if (!whereClause.noMatch()) {
                whereClauses.add(whereClause);
            }
        }
        if (!docKeys.isEmpty()) {
            plan.add(new ESDeleteNode(context.nextExecutionNodeId(), tableRelation.tableInfo(), docKeys));
        } else if (!whereClauses.isEmpty()) {
            createESDeleteByQueryNode(tableRelation.tableInfo(), whereClauses, plan, context);
        }

        if (plan.isEmpty()) {
            return NoopPlan.INSTANCE;
        }
        return plan;
    }

    @Override
    protected Plan visitCopyStatement(final CopyAnalyzedStatement analysis, Context context) {
        switch (analysis.mode()) {
            case FROM:
                return copyFromPlan(analysis, context);
            case TO:
                return copyToPlan(analysis, context);
            default:
                throw new UnsupportedOperationException("mode not supported: " + analysis.mode());
        }
    }

    private Plan copyToPlan(CopyAnalyzedStatement analysis, Context context) {
        TableInfo tableInfo = analysis.table();
        WriterProjection projection = new WriterProjection();
        projection.uri(analysis.uri());
        projection.isDirectoryUri(analysis.directoryUri());
        projection.settings(analysis.settings());

        List<Symbol> outputs;
        if (analysis.selectedColumns() != null && !analysis.selectedColumns().isEmpty()) {
            outputs = new ArrayList<>(analysis.selectedColumns().size());
            List<Symbol> columnSymbols = new ArrayList<>(analysis.selectedColumns().size());
            for (int i = 0; i < analysis.selectedColumns().size(); i++) {
                outputs.add(DocReferenceConverter.convertIfPossible(analysis.selectedColumns().get(i), analysis.table()));
                columnSymbols.add(new InputColumn(i, null));
            }
            projection.inputs(columnSymbols);
        } else {
            Reference sourceRef;
            if (analysis.table().isPartitioned() && analysis.partitionIdent() == null) {
                // table is partitioned, insert partitioned columns into the output
                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.DOC));
                Map<ColumnIdent, Symbol> overwrites = new HashMap<>();
                for (ReferenceInfo referenceInfo : analysis.table().partitionedByColumns()) {
                    overwrites.put(referenceInfo.ident().columnIdent(), new Reference(referenceInfo));
                }
                projection.overwrites(overwrites);
            } else {
                sourceRef = new Reference(analysis.table().getReferenceInfo(DocSysColumns.RAW));
            }
            outputs = ImmutableList.<Symbol>of(sourceRef);
        }
        CollectNode collectNode = PlanNodeBuilder.collect(
                tableInfo,
                context,
                WhereClause.MATCH_ALL,
                outputs,
                ImmutableList.<Projection>of(projection),
                analysis.partitionIdent()
        );

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context);
        return new CollectAndMerge(collectNode, mergeNode);
    }

    private Plan copyFromPlan(CopyAnalyzedStatement analysis, Context context) {
        /**
         * copy from has two "modes":
         *
         * 1: non-partitioned tables or partitioned tables with partition ident --> import into single es index
         *    -> collect raw source and import as is
         *
         * 2: partitioned table without partition ident
         *    -> collect document and partition by values
         *    -> exclude partitioned by columns from document
         *    -> insert into es index (partition determined by partition by value)
         */

        TableInfo table = analysis.table();
        int clusteredByPrimaryKeyIdx = table.primaryKey().indexOf(analysis.table().clusteredBy());
        List<String> partitionedByNames;
        String partitionIdent = null;

        List<BytesRef> partitionValues;
        if (analysis.partitionIdent() == null) {

            if (table.isPartitioned()) {
                partitionedByNames = Lists.newArrayList(
                        Lists.transform(table.partitionedBy(), ColumnIdent.GET_FQN_NAME_FUNCTION));
            } else {
                partitionedByNames = Collections.emptyList();
            }
            partitionValues = ImmutableList.of();
        } else {
            assert table.isPartitioned() : "table must be partitioned if partitionIdent is set";
            // partitionIdent is present -> possible to index raw source into concrete es index
            PartitionName partitionName = PartitionName.fromPartitionIdent(table.ident().schema(), table.ident().name(), analysis.partitionIdent());
            partitionValues = partitionName.values();

            partitionIdent = partitionName.ident();
            partitionedByNames = Collections.emptyList();
        }

        SourceIndexWriterProjection sourceIndexWriterProjection = new SourceIndexWriterProjection(
                table.ident(),
                partitionIdent,
                new Reference(table.getReferenceInfo(DocSysColumns.RAW)),
                table.primaryKey(),
                table.partitionedBy(),
                partitionValues,
                table.clusteredBy(),
                clusteredByPrimaryKeyIdx,
                analysis.settings(),
                null,
                partitionedByNames.size() > 0 ? partitionedByNames.toArray(new String[partitionedByNames.size()]) : null,
                table.isPartitioned() // autoCreateIndices
        );
        List<Projection> projections = Arrays.<Projection>asList(sourceIndexWriterProjection);
        partitionedByNames.removeAll(Lists.transform(table.primaryKey(), ColumnIdent.GET_FQN_NAME_FUNCTION));
        int referencesSize = table.primaryKey().size() + partitionedByNames.size() + 1;
        referencesSize = clusteredByPrimaryKeyIdx == -1 ? referencesSize + 1 : referencesSize;

        List<Symbol> toCollect = new ArrayList<>(referencesSize);
        // add primaryKey columns
        for (ColumnIdent primaryKey : table.primaryKey()) {
            toCollect.add(new Reference(table.getReferenceInfo(primaryKey)));
        }

        // add partitioned columns (if not part of primaryKey)
        for (String partitionedColumn : partitionedByNames) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(ColumnIdent.fromPath(partitionedColumn)))
            );
        }
        // add clusteredBy column (if not part of primaryKey)
        if (clusteredByPrimaryKeyIdx == -1) {
            toCollect.add(
                    new Reference(table.getReferenceInfo(table.clusteredBy())));
        }
        // finally add _raw or _doc
        if (table.isPartitioned() && analysis.partitionIdent() == null) {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.DOC)));
        } else {
            toCollect.add(new Reference(table.getReferenceInfo(DocSysColumns.RAW)));
        }

        DiscoveryNodes allNodes = clusterService.state().nodes();
        FileUriCollectNode collectNode = new FileUriCollectNode(
                context.nextExecutionNodeId(),
                "copyFrom",
                generateRouting(allNodes, analysis.settings().getAsInt("num_readers", allNodes.getSize())),
                analysis.uri(),
                toCollect,
                projections,
                analysis.settings().get("compression", null),
                analysis.settings().getAsBoolean("shared", null)
        );
        PlanNodeBuilder.setOutputTypes(collectNode);

        return new CollectAndMerge(collectNode, PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode, context));
    }

    private Routing generateRouting(DiscoveryNodes allNodes, int maxNodes) {
        final AtomicInteger counter = new AtomicInteger(maxNodes);
        final Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();
        allNodes.dataNodes().keys().forEach(new ObjectProcedure<String>() {
            @Override
            public void apply(String value) {
                if (counter.getAndDecrement() > 0) {
                    locations.put(value, TreeMapBuilder.<String, List<Integer>>newMapBuilder().map());
                }
            }
        });
        return new Routing(locations);
    }

    @Override
    protected Plan visitDDLAnalyzedStatement(AbstractDDLAnalyzedStatement statement, Context context) {
        return new IterablePlan(new GenericDDLNode(statement));
    }

    @Override
    public Plan visitDropBlobTableStatement(DropBlobTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return NoopPlan.INSTANCE;
        }
        return visitDDLAnalyzedStatement(analysis, context);
    }

    @Override
    protected Plan visitDropTableStatement(DropTableAnalyzedStatement analysis, Context context) {
        if (analysis.noop()) {
            return NoopPlan.INSTANCE;
        }
        return new IterablePlan(new DropTableNode(analysis.table(), analysis.dropIfExists()));
    }

    @Override
    protected Plan visitCreateTableStatement(CreateTableAnalyzedStatement analysis, Context context) {
        if (analysis.noOp()) {
            return NoopPlan.INSTANCE;
        }
        TableIdent tableIdent = analysis.tableIdent();

        CreateTableNode createTableNode;
        if (analysis.isPartitioned()) {
            createTableNode = CreateTableNode.createPartitionedTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings().getByPrefix("index."),
                    analysis.mapping(),
                    analysis.templateName(),
                    analysis.templatePrefix()
            );
        } else {
            createTableNode = CreateTableNode.createTableNode(
                    tableIdent,
                    analysis.ifNotExists(),
                    analysis.tableParameter().settings(),
                    analysis.mapping()
            );
        }
        return new IterablePlan(createTableNode);
    }

    @Override
    protected Plan visitCreateAnalyzerStatement(CreateAnalyzerAnalyzedStatement analysis, Context context) {
        Settings analyzerSettings;
        try {
            analyzerSettings = analysis.buildSettings();
        } catch (IOException ioe) {
            throw new UnhandledServerException("Could not build analyzer Settings", ioe);
        }

        ESClusterUpdateSettingsNode node = new ESClusterUpdateSettingsNode(analyzerSettings);
        return new IterablePlan(node);
    }

    @Override
    public Plan visitSetStatement(SetAnalyzedStatement analysis, Context context) {
        ESClusterUpdateSettingsNode node = null;
        if (analysis.isReset()) {
            // always reset persistent AND transient settings
            if (analysis.settingsToRemove() != null) {
                node = new ESClusterUpdateSettingsNode(analysis.settingsToRemove(), analysis.settingsToRemove());
            }
        } else {
            if (analysis.settings() != null) {
                if (analysis.isPersistent()) {
                    node = new ESClusterUpdateSettingsNode(analysis.settings());
                } else {
                    node = new ESClusterUpdateSettingsNode(ImmutableSettings.EMPTY, analysis.settings());
                }
            }
        }
        return node != null ? new IterablePlan(node) : NoopPlan.INSTANCE;
    }

    @Override
    public Plan visitKillAnalyzedStatement(KillAnalyzedStatement analysis, Context context) {
        return KillPlan.INSTANCE;
    }

    private void createESDeleteByQueryNode(TableInfo tableInfo,
                                           List<WhereClause> whereClauses,
                                           IterablePlan plan,
                                           Context context) {

        List<String[]> indicesList = new ArrayList<>(whereClauses.size());
        for (WhereClause whereClause : whereClauses) {
            String[] indices = indices(tableInfo, whereClauses.get(0));
            if (indices.length > 0) {
                if (!whereClause.hasQuery() && tableInfo.isPartitioned()) {
                    plan.add(new ESDeletePartitionNode(indices));
                } else {
                    indicesList.add(indices);
                }
            }
        }
        // TODO: if we allow queries like 'partitionColumn=X or column=Y' which is currently
        // forbidden through analysis, we must issue deleteByQuery request in addition
        // to above deleteIndex request(s)
        if (!indicesList.isEmpty()) {
            plan.add(new ESDeleteByQueryNode(context.nextExecutionNodeId(), indicesList, whereClauses));
        }
    }

    private Upsert processInsertStatement(InsertFromValuesAnalyzedStatement analysis, Context context) {
        String[] onDuplicateKeyAssignmentsColumns = null;
        if (analysis.onDuplicateKeyAssignmentsColumns().size() > 0) {
            onDuplicateKeyAssignmentsColumns = analysis.onDuplicateKeyAssignmentsColumns().get(0);
        }
        SymbolBasedUpsertByIdNode upsertByIdNode = new SymbolBasedUpsertByIdNode(
                context.nextExecutionNodeId(),
                analysis.tableInfo().isPartitioned(),
                analysis.isBulkRequest(),
                onDuplicateKeyAssignmentsColumns,
                analysis.columns().toArray(new Reference[analysis.columns().size()])
        );
        if (analysis.tableInfo().isPartitioned()) {
            List<String> partitions = analysis.generatePartitions();
            String[] indices = partitions.toArray(new String[partitions.size()]);
            for (int i = 0; i < indices.length; i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignmentsColumns().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        indices[i],
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        } else {
            for (int i = 0; i < analysis.ids().size(); i++) {
                Symbol[] onDuplicateKeyAssignments = null;
                if (analysis.onDuplicateKeyAssignments().size() > i) {
                    onDuplicateKeyAssignments = analysis.onDuplicateKeyAssignments().get(i);
                }
                upsertByIdNode.add(
                        analysis.tableInfo().ident().esName(),
                        analysis.ids().get(i),
                        analysis.routingValues().get(i),
                        onDuplicateKeyAssignments,
                        null,
                        analysis.sourceMaps().get(i));
            }
        }

        return new Upsert(ImmutableList.<Plan>of(new IterablePlan(upsertByIdNode)));
    }

    static List<DataType> extractDataTypes(List<Projection> projections, @Nullable List<DataType> inputTypes) {
        if (projections.size() == 0) {
            return inputTypes;
        }
        int projectionIdx = projections.size() - 1;
        Projection lastProjection = projections.get(projectionIdx);
        List<DataType> types = new ArrayList<>(lastProjection.outputs().size());
        List<DataType> dataTypes = firstNonNull(inputTypes, ImmutableList.<DataType>of());

        for (int c = 0; c < lastProjection.outputs().size(); c++) {
            types.add(resolveType(projections, projectionIdx, c, dataTypes));
        }
        return types;
    }

    private static DataType resolveType(List<Projection> projections, int projectionIdx, int columnIdx, List<DataType> inputTypes) {
        Projection projection = projections.get(projectionIdx);
        Symbol symbol = projection.outputs().get(columnIdx);
        DataType type = symbol.valueType();
        if (type == null || (type.equals(DataTypes.UNDEFINED) && symbol instanceof InputColumn)) {
            if (projectionIdx > 0) {
                if (symbol instanceof InputColumn) {
                    columnIdx = ((InputColumn) symbol).index();
                }
                return resolveType(projections, projectionIdx - 1, columnIdx, inputTypes);
            } else {
                assert symbol instanceof InputColumn; // otherwise type shouldn't be null
                return inputTypes.get(((InputColumn) symbol).index());
            }
        }

        return type;
    }


    /**
     * return the ES index names the query should go to
     */
    public static String[] indices(TableInfo tableInfo, WhereClause whereClause) {
        String[] indices;

        if (whereClause.noMatch()) {
            indices = org.elasticsearch.common.Strings.EMPTY_ARRAY;
        } else if (!tableInfo.isPartitioned()) {
            // table name for non-partitioned tables
            indices = new String[]{tableInfo.ident().esName()};
        } else if (whereClause.partitions().isEmpty()) {
            if (whereClause.noMatch()) {
                return new String[0];
            }

            // all partitions
            indices = new String[tableInfo.partitions().size()];
            int i = 0;
            for (PartitionName partitionName: tableInfo.partitions()) {
                indices[i] = partitionName.stringValue();
                i++;
            }
        } else {
            indices = whereClause.partitions().toArray(new String[whereClause.partitions().size()]);
        }
        return indices;
    }
}



File: sql/src/main/java/io/crate/planner/consumer/CountConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.consumer;

import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.Planner;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.CountPlan;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;

import java.util.Collections;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class CountConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        AnalyzedRelation analyzedRelation = VISITOR.process(rootRelation, context);
        if (analyzedRelation != null) {
            context.rootRelation(analyzedRelation);
            return true;
        }
        return false;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            QuerySpec querySpec = table.querySpec();
            if (!querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema()) {
                return null;
            }
            if (!hasOnlyGlobalCount(querySpec.outputs())) {
                return null;
            }
            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (firstNonNull(querySpec.limit(), 1) < 1 ||
                    querySpec.offset() > 0){
                return new NoopPlannedAnalyzedRelation(table);
            }

            Routing routing = tableInfo.getRouting(querySpec.where(), null);
            Planner.Context plannerContext = context.plannerContext();
            CountNode countNode = new CountNode(plannerContext.nextExecutionNodeId(), routing, querySpec.where());
            MergeNode mergeNode = new MergeNode(
                    plannerContext.nextExecutionNodeId(),
                    "count-merge",
                    countNode.executionNodes().size());
            mergeNode.inputTypes(Collections.<DataType>singletonList(DataTypes.LONG));
            mergeNode.projections(Collections.<Projection>singletonList(
                    CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION));
            return new CountPlan(countNode, mergeNode);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }

        private boolean hasOnlyGlobalCount(List<Symbol> symbols) {
            if (symbols.size() != 1) {
                return false;
            }
            Symbol symbol = symbols.get(0);
            if (!(symbol instanceof Function)) {
                return false;
            }
            Function function = (Function) symbol;
            return function.info().equals(CountAggregation.COUNT_STAR_FUNCTION);
        }
    }
}


File: sql/src/main/java/io/crate/planner/consumer/DistributedGroupByConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.consumer;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.collect.Sets;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DistributedGroupBy;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.LinkedList;
import java.util.List;

public class DistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            List<Symbol> groupBy = table.querySpec().groupBy();
            if (groupBy == null) {
                return table;
            }

            TableInfo tableInfo = table.tableRelation().tableInfo();
            if(table.querySpec().where().hasVersions()){
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());

            SplitPoints splitPoints = projectionBuilder.getSplitPoints();

            // start: Map/Collect side
            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.distributingCollect(
                    tableInfo,
                    context.consumerContext.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    Lists.newArrayList(routing.nodes()),
                    ImmutableList.<Projection>of(groupProjection)
            );
            // end: Map/Collect side

            // start: Reducer
            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            List<Projection> reducerProjections = new LinkedList<>();
            reducerProjections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL)
            );

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()) {
                    reducerProjections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }

            boolean isRootRelation = context.consumerContext.rootRelation() == table;
            if (isRootRelation) {
                reducerProjections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0,
                        MoreObjects.firstNonNull(table.querySpec().limit(),
                                Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()));
            }
            MergeNode mergeNode = PlanNodeBuilder.distributedMerge(
                    collectNode,
                    context.consumerContext.plannerContext(),
                    reducerProjections
            );
            // end: Reducer

            MergeNode localMergeNode = null;
            String localNodeId = context.consumerContext.plannerContext().clusterService().state().nodes().localNodeId();
            if(isRootRelation) {
                TopNProjection topN = projectionBuilder.topNProjection(
                        table.querySpec().outputs(),
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        null);
                localMergeNode = PlanNodeBuilder.localMerge(ImmutableList.<Projection>of(topN),
                        mergeNode, context.consumerContext.plannerContext());
                localMergeNode.executionNodes(Sets.newHashSet(localNodeId));

                mergeNode.downstreamNodes(localMergeNode.executionNodes());
                mergeNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            } else {
                mergeNode.downstreamNodes(Sets.newHashSet(localNodeId));
                mergeNode.downstreamExecutionNodeId(mergeNode.executionNodeId() + 1);
            }
            context.result = true;

            collectNode.downstreamExecutionNodeId(mergeNode.executionNodeId());
            return new DistributedGroupBy(
                    collectNode,
                    mergeNode,
                    localMergeNode
            );
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

    }
}


File: sql/src/main/java/io/crate/planner/consumer/ESGetConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.consumer;


import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.ESGetNode;

public class ESGetConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation relation = VISITOR.process(rootRelation, context);
        if (relation == null) {
            return false;
        }
        context.rootRelation(relation);
        return true;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().hasAggregates() || table.querySpec().groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.isAlias()
                    || tableInfo.schemaInfo().systemSchema()
                    || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if (!table.querySpec().where().docKeys().isPresent()) {
                return null;
            }

            if(table.querySpec().where().docKeys().get().withVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            Integer limit = table.querySpec().limit();
            if (limit != null){
                if (limit == 0){
                    return new NoopPlannedAnalyzedRelation(table);
                }
            }

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null){
                table.tableRelation().validateOrderBy(orderBy);
            }
            return new ESGetNode(context.plannerContext().nextExecutionNodeId(), tableInfo, table.querySpec());
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}


File: sql/src/main/java/io/crate/planner/consumer/GlobalAggregateConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.FunctionInfo;
import io.crate.metadata.ReferenceInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GlobalAggregate;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;


public class GlobalAggregateConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final AggregationOutputValidator AGGREGATION_OUTPUT_VALIDATOR = new AggregationOutputValidator();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        AnalyzedRelation analyzedRelation = VISITOR.process(rootRelation, context);
        if (analyzedRelation != null) {
            context.rootRelation(analyzedRelation);
            return true;
        }
        return false;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            if (table.querySpec().groupBy()!=null || !table.querySpec().hasAggregates()) {
                return null;
            }
            if (firstNonNull(table.querySpec().limit(), 1) < 1 || table.querySpec().offset() > 0){
                return new NoopPlannedAnalyzedRelation(table);
            }

            if (table.querySpec().where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }
            return globalAggregates(table, table.tableRelation(),  table.querySpec().where(), context);
        }

        @Override
        public PlannedAnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, ConsumerContext context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return null;
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    private static boolean noGroupBy(List<Symbol> groupBy) {
        return groupBy == null || groupBy.isEmpty();
    }

    public static PlannedAnalyzedRelation globalAggregates(QueriedTable table,
                                                           TableRelation tableRelation,
                                                           WhereClause whereClause,
                                                           ConsumerContext context) {
        assert noGroupBy(table.querySpec().groupBy()) : "must not have group by clause for global aggregate queries";
        validateAggregationOutputs(tableRelation, table.querySpec().outputs());
        // global aggregate: collect and partial aggregate on C and final agg on H

        ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
        SplitPoints splitPoints = projectionBuilder.getSplitPoints();

        AggregationProjection ap = projectionBuilder.aggregationProjection(
                splitPoints.leaves(),
                splitPoints.aggregates(),
                Aggregation.Step.ITER,
                Aggregation.Step.PARTIAL);

        CollectNode collectNode = PlanNodeBuilder.collect(
                tableRelation.tableInfo(),
                context.plannerContext(),
                whereClause,
                splitPoints.leaves(),
                ImmutableList.<Projection>of(ap)
        );

        //// the handler stuff
        List<Projection> projections = new ArrayList<>();
        projections.add(projectionBuilder.aggregationProjection(
                splitPoints.aggregates(),
                splitPoints.aggregates(),
                Aggregation.Step.PARTIAL,
                Aggregation.Step.FINAL));

        HavingClause havingClause = table.querySpec().having();
        if(havingClause != null){
            if (havingClause.noMatch()) {
                return new NoopPlannedAnalyzedRelation(table);
            } else if (havingClause.hasQuery()){
                projections.add(projectionBuilder.filterProjection(
                        splitPoints.aggregates(),
                        havingClause.query()
                ));
            }
        }

        TopNProjection topNProjection = projectionBuilder.topNProjection(
                splitPoints.aggregates(),
                null, 0, 1,
                table.querySpec().outputs()
                );
        projections.add(topNProjection);
        MergeNode localMergeNode = PlanNodeBuilder.localMerge(projections, collectNode,
                context.plannerContext());
        return new GlobalAggregate(collectNode, localMergeNode);
    }

    private static void validateAggregationOutputs(TableRelation tableRelation, Collection<? extends Symbol> outputSymbols) {
        OutputValidatorContext context = new OutputValidatorContext(tableRelation);
        for (Symbol outputSymbol : outputSymbols) {
            context.insideAggregation = false;
            AGGREGATION_OUTPUT_VALIDATOR.process(outputSymbol, context);
        }
    }

    private static class OutputValidatorContext {
        private final TableRelation tableRelation;
        private boolean insideAggregation = false;

        public OutputValidatorContext(TableRelation tableRelation) {
            this.tableRelation = tableRelation;
        }
    }

    private static class AggregationOutputValidator extends SymbolVisitor<OutputValidatorContext, Void> {

        @Override
        public Void visitFunction(Function symbol, OutputValidatorContext context) {
            context.insideAggregation = context.insideAggregation || symbol.info().type().equals(FunctionInfo.Type.AGGREGATE);
            for (Symbol argument : symbol.arguments()) {
                process(argument, context);
            }
            context.insideAggregation = false;
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputValidatorContext context) {
            if (context.insideAggregation) {
                ReferenceInfo.IndexType indexType = symbol.info().indexType();
                if (indexType == ReferenceInfo.IndexType.ANALYZED) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select analyzed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                } else if (indexType == ReferenceInfo.IndexType.NO) {
                    throw new IllegalArgumentException(String.format(
                            "Cannot select non-indexed column '%s' within grouping or aggregations", SymbolFormatter.format(symbol)));
                }
            }
            return null;
        }

        @Override
        public Void visitField(Field field, OutputValidatorContext context) {
            return process(context.tableRelation.resolveField(field), context);
        }

        @Override
        protected Void visitSymbol(Symbol symbol, OutputValidatorContext context) {
            return null;
        }
    }
}


File: sql/src/main/java/io/crate/planner/consumer/InsertFromSubQueryConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.consumer;


import com.google.common.collect.ImmutableList;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dml.InsertFromSubQuery;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.ColumnIndexWriterProjection;
import io.crate.planner.projection.Projection;
import org.elasticsearch.common.settings.ImmutableSettings;


public class InsertFromSubQueryConsumer implements Consumer {

    private final Visitor visitor;

    public InsertFromSubQueryConsumer(){
        visitor = new Visitor();
    }

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(visitor.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context){
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {

            ColumnIndexWriterProjection indexWriterProjection = new ColumnIndexWriterProjection(
                    insertFromSubQueryAnalyzedStatement.tableInfo().ident(),
                    null,
                    insertFromSubQueryAnalyzedStatement.tableInfo().primaryKey(),
                    insertFromSubQueryAnalyzedStatement.columns(),
                    insertFromSubQueryAnalyzedStatement.onDuplicateKeyAssignments(),
                    insertFromSubQueryAnalyzedStatement.primaryKeyColumnIndices(),
                    insertFromSubQueryAnalyzedStatement.partitionedByIndices(),
                    insertFromSubQueryAnalyzedStatement.routingColumn(),
                    insertFromSubQueryAnalyzedStatement.routingColumnIndex(),
                    ImmutableSettings.EMPTY,
                    insertFromSubQueryAnalyzedStatement.tableInfo().isPartitioned()
            );

            AnalyzedRelation innerRelation = insertFromSubQueryAnalyzedStatement.subQueryRelation();
            if (innerRelation instanceof PlannedAnalyzedRelation) {
                PlannedAnalyzedRelation analyzedRelation = (PlannedAnalyzedRelation)innerRelation;
                analyzedRelation.addProjection(indexWriterProjection);

                MergeNode mergeNode = null;
                if (analyzedRelation.resultIsDistributed()) {
                    // add local merge Node which aggregates the distributed results
                    AggregationProjection aggregationProjection = CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION;
                    mergeNode = PlanNodeBuilder.localMerge(
                            ImmutableList.<Projection>of(aggregationProjection),
                            analyzedRelation.resultNode(),
                            context.consumerContext.plannerContext());
                }
                context.result = true;
                return new InsertFromSubQuery(((PlannedAnalyzedRelation) innerRelation).plan(), mergeNode);
            } else {
                return insertFromSubQueryAnalyzedStatement;
            }
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }
    }

    public static <C, R> void planInnerRelation(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement,
                                                C context, AnalyzedRelationVisitor<C,R> visitor) {
        if (insertFromSubQueryAnalyzedStatement.subQueryRelation() instanceof PlannedAnalyzedRelation) {
            // inner relation is already Planned
            return;
        }
        R innerRelation = visitor.process(insertFromSubQueryAnalyzedStatement.subQueryRelation(), context);
        if (innerRelation != null && innerRelation instanceof PlannedAnalyzedRelation) {
            insertFromSubQueryAnalyzedStatement.subQueryRelation((PlannedAnalyzedRelation)innerRelation);
        }
    }


}


File: sql/src/main/java/io/crate/planner/consumer/NonDistributedGroupByConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */
package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

public class NonDistributedGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            if (table.querySpec().groupBy() == null) {
                return table;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();

            if (table.querySpec().where().hasVersions()) {
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }

            Routing routing = tableInfo.getRouting(table.querySpec().where(), null);

            if (GroupByConsumer.requiresDistribution(tableInfo, routing) && !(tableInfo.schemaInfo().systemSchema())) {
                return table;
            }

            context.result = true;
            return nonDistributedGroupBy(table, context);
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;

        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

        /**
         * Group by on System Tables (never needs distribution)
         * or Group by on user tables (RowGranulariy.DOC) with only one node.
         *
         * produces:
         *
         * SELECT:
         * Collect ( GroupProjection ITER -> PARTIAL )
         * LocalMerge ( GroupProjection PARTIAL -> FINAL, [FilterProjection], TopN )
         *
         */
        private AnalyzedRelation nonDistributedGroupBy(QueriedTable table, Context context) {
            TableInfo tableInfo = table.tableRelation().tableInfo();

            GroupByConsumer.validateGroupBySymbols(table.tableRelation(), table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();

            // mapper / collect
            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.PARTIAL);

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.consumerContext.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.<Projection>of(groupProjection)
            );
            // handler
            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());


            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            projections.add(projectionBuilder.groupProjection(
                    collectOutputs,
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.PARTIAL,
                    Aggregation.Step.FINAL
            ));

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()){
                    projections.add(projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    ));
                }
            }

            /**
             * If this is not the rootRelation this is a subquery (e.g. Insert by Query),
             * so ordering and limiting is done by the rootRelation if required.
             *
             * If the querySpec outputs don't match the collectOutputs the query contains
             * aggregations or scalar functions which can only be resolved by a TopNProjection,
             * so a TopNProjection must be added.
             */
            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                                    collectOutputs.containsAll(table.querySpec().outputs());
            if (context.consumerContext.rootRelation() == table || !outputsMatch){
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        table.querySpec().offset(),
                        table.querySpec().limit(),
                        table.querySpec().outputs()
                ));
            }
            MergeNode localMergeNode = PlanNodeBuilder.localMerge(projections, collectNode,
                    context.consumerContext.plannerContext());
            return new NonDistributedGroupBy(collectNode, localMergeNode);
        }
    }

}


File: sql/src/main/java/io/crate/planner/consumer/QueryAndFetchConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.*;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.Functions;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.SumAggregation;
import io.crate.operation.predicate.MatchPredicate;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.dql.QueryAndFetch;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.*;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.LongType;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class QueryAndFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            TableRelation tableRelation = table.tableRelation();
            if(table.querySpec().where().hasVersions()){
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() && table.querySpec().where().hasQuery()) {
                ensureNoLuceneOnlyPredicates(table.querySpec().where().query());
            }
            if (table.querySpec().hasAggregates()) {
                context.result = true;
                return GlobalAggregateConsumer.globalAggregates(table, tableRelation, table.querySpec().where(), context.consumerContext);
            } else {
               context.result = true;
               return normalSelect(table, table.querySpec().where(), tableRelation, context);
            }
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement,
                                                     Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

        private void ensureNoLuceneOnlyPredicates(Symbol query) {
            NoPredicateVisitor noPredicateVisitor = new NoPredicateVisitor();
            noPredicateVisitor.process(query, null);
        }

        private static class NoPredicateVisitor extends SymbolVisitor<Void, Void> {
            @Override
            public Void visitFunction(Function symbol, Void context) {
                if (symbol.info().ident().name().equals(MatchPredicate.NAME)) {
                    throw new UnsupportedFeatureException("Cannot use match predicate on system tables");
                }
                for (Symbol argument : symbol.arguments()) {
                    process(argument, context);
                }
                return null;
            }
        }

        private AnalyzedRelation normalSelect(QueriedTable table,
                                              WhereClause whereClause,
                                              TableRelation tableRelation,
                                              Context context){
            QuerySpec querySpec = table.querySpec();
            TableInfo tableInfo = tableRelation.tableInfo();

            List<Symbol> outputSymbols;
            if (tableInfo.schemaInfo().systemSchema()) {
                outputSymbols = tableRelation.resolve(querySpec.outputs());
            } else {
                outputSymbols = new ArrayList<>(querySpec.outputs().size());
                for (Symbol symbol : querySpec.outputs()) {
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(tableRelation.resolve(symbol), tableInfo));
                }
            }
            CollectNode collectNode;
            MergeNode mergeNode = null;
            OrderBy orderBy = querySpec.orderBy();
            if (context.consumerContext.rootRelation() != table) {
                // insert directly from shards
                assert !querySpec.isLimited() : "insert from sub query with limit or order by is not supported. " +
                        "Analyzer should have thrown an exception already.";

                ImmutableList<Projection> projections = ImmutableList.<Projection>of();
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, outputSymbols, projections);
            } else if (querySpec.isLimited() || orderBy != null) {
                /**
                 * select id, name, order by id, date
                 *
                 * toCollect:       [id, name, date]            // includes order by symbols, that aren't already selected
                 * allOutputs:      [in(0), in(1), in(2)]       // for topN projection on shards/collectNode
                 * orderByInputs:   [in(0), in(2)]              // for topN projection on shards/collectNode AND handler
                 * finalOutputs:    [in(0), in(1)]              // for topN output on handler -> changes output to what should be returned.
                 */
                List<Symbol> toCollect;
                List<Symbol> orderByInputColumns = null;
                if (orderBy != null){
                    List<Symbol> orderBySymbols = tableRelation.resolve(orderBy.orderBySymbols());
                    toCollect = new ArrayList<>(outputSymbols.size() + orderBySymbols.size());
                    toCollect.addAll(outputSymbols);
                    // note: can only de-dup order by symbols due to non-deterministic functions like select random(), random()
                    for (Symbol orderBySymbol : orderBySymbols) {
                        if (!toCollect.contains(orderBySymbol)) {
                            toCollect.add(orderBySymbol);
                        }
                    }
                    orderByInputColumns = new ArrayList<>();
                    for (Symbol symbol : orderBySymbols) {
                        orderByInputColumns.add(new InputColumn(toCollect.indexOf(symbol), symbol.valueType()));
                    }
                } else {
                    toCollect = new ArrayList<>(outputSymbols.size());
                    toCollect.addAll(outputSymbols);
                }

                List<Symbol> allOutputs = new ArrayList<>(toCollect.size());        // outputs from collector
                for (int i = 0; i < toCollect.size(); i++) {
                    allOutputs.add(new InputColumn(i, toCollect.get(i).valueType()));
                }
                List<Symbol> finalOutputs = new ArrayList<>(outputSymbols.size());  // final outputs on handler after sort
                for (int i = 0; i < outputSymbols.size(); i++) {
                    finalOutputs.add(new InputColumn(i, outputSymbols.get(i).valueType()));
                }

                // if we have an offset we have to get as much docs from every node as we have offset+limit
                // otherwise results will be wrong
                TopNProjection tnp;
                int limit = firstNonNull(querySpec.limit(), Constants.DEFAULT_SELECT_LIMIT);
                if (orderBy == null){
                    tnp = new TopNProjection(querySpec.offset() + limit, 0);
                } else {
                    tnp = new TopNProjection(querySpec.offset() + limit, 0,
                            orderByInputColumns,
                            orderBy.reverseFlags(),
                            orderBy.nullsFirst()
                    );
                }
                tnp.outputs(allOutputs);
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, toCollect, ImmutableList.<Projection>of(tnp));

                // MERGE
                tnp = new TopNProjection(limit, querySpec.offset());
                tnp.outputs(finalOutputs);
                if (orderBy == null) {
                    // no sorting needed
                    mergeNode = PlanNodeBuilder.localMerge(ImmutableList.<Projection>of(tnp), collectNode,
                            context.consumerContext.plannerContext());
                } else {
                    // no order by needed in TopN as we already sorted on collector
                    // and we merge sorted with SortedBucketMerger
                    mergeNode = PlanNodeBuilder.sortedLocalMerge(
                            ImmutableList.<Projection>of(tnp),
                            orderBy,
                            allOutputs,
                            orderByInputColumns,
                            collectNode,
                            context.consumerContext.plannerContext()
                    );
                }
            } else {
                collectNode = PlanNodeBuilder.collect(tableInfo,
                        context.consumerContext.plannerContext(),
                        whereClause, outputSymbols, ImmutableList.<Projection>of());
                mergeNode = PlanNodeBuilder.localMerge(
                        ImmutableList.<Projection>of(), collectNode,
                        context.consumerContext.plannerContext());
            }
            return new QueryAndFetch(collectNode, mergeNode);
        }
    }
}


File: sql/src/main/java/io/crate/planner/consumer/QueryThenFetchConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.DocReferenceConverter;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.ScoreReferenceDetector;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.FetchProjector;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.MergeProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.*;
import io.crate.types.DataTypes;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

public class QueryThenFetchConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();
    private static final OutputOrderReferenceCollector OUTPUT_ORDER_REFERENCE_COLLECTOR = new OutputOrderReferenceCollector();
    private static final ReferencesCollector REFERENCES_COLLECTOR = new ReferencesCollector();
    private static final ScoreReferenceDetector SCORE_REFERENCE_DETECTOR = new ScoreReferenceDetector();
    private static final ColumnIdent DOC_ID_COLUMN_IDENT = new ColumnIdent(DocSysColumns.DOCID.name());
    private static final InputColumn DEFAULT_DOC_ID_INPUT_COLUMN = new InputColumn(0, DataTypes.STRING);

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation plannedAnalyzedRelation = VISITOR.process(rootRelation, context);
        if (plannedAnalyzedRelation == null) {
            return false;
        }
        context.rootRelation(plannedAnalyzedRelation);
        return true;
    }

    private static class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitQueriedTable(QueriedTable table, ConsumerContext context) {
            QuerySpec querySpec = table.querySpec();
            if (querySpec.hasAggregates() || querySpec.groupBy()!=null) {
                return null;
            }
            TableInfo tableInfo = table.tableRelation().tableInfo();
            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            if(querySpec.where().hasVersions()){
                context.validationException(new VersionInvalidException());
                return null;
            }

            if (querySpec.where().noMatch()) {
                return new NoopPlannedAnalyzedRelation(table);
            }

            boolean outputsAreAllOrdered = false;
            boolean needFetchProjection = REFERENCES_COLLECTOR.collect(querySpec.outputs()).containsAnyReference();
            List<Projection> collectProjections = new ArrayList<>();
            List<Projection> mergeProjections = new ArrayList<>();
            List<Symbol> collectSymbols = new ArrayList<>();
            List<Symbol> outputSymbols = new ArrayList<>();
            ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(DOC_ID_COLUMN_IDENT);

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(querySpec);
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();

            // MAP/COLLECT related
            OrderBy orderBy = querySpec.orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);

                // detect if all output columns are used in orderBy,
                // if so, no fetch projection is needed
                // TODO: if no dedicated fetchPhase is needed we should stick to QAF instead
                OutputOrderReferenceContext outputOrderContext =
                        OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(splitPoints.leaves());
                outputOrderContext.collectOrderBy = true;
                OUTPUT_ORDER_REFERENCE_COLLECTOR.collect(orderBy.orderBySymbols(), outputOrderContext);
                outputsAreAllOrdered = outputOrderContext.outputsAreAllOrdered();
                if (outputsAreAllOrdered) {
                    collectSymbols = splitPoints.toCollect();
                } else {
                    collectSymbols.addAll(orderBy.orderBySymbols());
                }
            }

            needFetchProjection = needFetchProjection & !outputsAreAllOrdered;

            if (needFetchProjection) {
                collectSymbols.add(0, new Reference(docIdRefInfo));
                for (Symbol symbol : querySpec.outputs()) {
                    // _score can only be resolved during collect
                    if (SCORE_REFERENCE_DETECTOR.detect(symbol) && !collectSymbols.contains(symbol)) {
                        collectSymbols.add(symbol);
                    }
                    outputSymbols.add(DocReferenceConverter.convertIfPossible(symbol, tableInfo));
                }
            } else {
                // no fetch projection needed, resolve all symbols during collect
                collectSymbols = splitPoints.toCollect();
            }
            if (orderBy != null) {
                MergeProjection mergeProjection = projectionBuilder.mergeProjection(
                        collectSymbols,
                        orderBy);
                collectProjections.add(mergeProjection);
            }

            Integer limit = querySpec.limit();
            // apply default limit if relation is root relation
            if ( limit == null && context.rootRelation() == table) {
                limit = Constants.DEFAULT_SELECT_LIMIT;
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.plannerContext(),
                    querySpec.where(),
                    collectSymbols,
                    ImmutableList.<Projection>of(),
                    orderBy,
                    limit == null ? null : limit + querySpec.offset()
            );


            collectNode.keepContextForFetcher(needFetchProjection);
            collectNode.projections(collectProjections);
            // MAP/COLLECT related END

            // HANDLER/MERGE/FETCH related
            TopNProjection topNProjection;
            if (needFetchProjection) {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        null);
                mergeProjections.add(topNProjection);

                // by default don't split fetch requests into pages/chunks,
                // only if record set is higher than default limit
                int bulkSize = FetchProjector.NO_BULK_REQUESTS;
                if (topNProjection.limit() > Constants.DEFAULT_SELECT_LIMIT) {
                    bulkSize = Constants.DEFAULT_SELECT_LIMIT;
                }

                FetchProjection fetchProjection = new FetchProjection(
                        collectNode.executionNodeId(),
                        DEFAULT_DOC_ID_INPUT_COLUMN, collectSymbols, outputSymbols,
                        tableInfo.partitionedByColumns(),
                        collectNode.executionNodes(),
                        bulkSize,
                        querySpec.isLimited(),
                        context.plannerContext().jobSearchContextIdToNode(),
                        context.plannerContext().jobSearchContextIdToShard()
                );
                mergeProjections.add(fetchProjection);
            } else {
                topNProjection = projectionBuilder.topNProjection(
                        collectSymbols,
                        null,
                        querySpec.offset(),
                        querySpec.limit(),
                        querySpec.outputs());
                mergeProjections.add(topNProjection);
            }

            MergeNode localMergeNode;
            if (orderBy != null) {
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        mergeProjections,
                        orderBy,
                        collectSymbols,
                        null,
                        collectNode,
                        context.plannerContext());
            } else {
                localMergeNode = PlanNodeBuilder.localMerge(
                        mergeProjections,
                        collectNode,
                        context.plannerContext());
            }
            // HANDLER/MERGE/FETCH related END

            if (limit != null && limit + querySpec.offset() > Constants.PAGE_SIZE) {
                collectNode.downstreamNodes(Collections.singletonList(context.plannerContext().clusterService().localNode().id()));
                collectNode.downstreamExecutionNodeId(localMergeNode.executionNodeId());
            }
            return new QueryThenFetch(collectNode, localMergeNode);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }

    static class OutputOrderReferenceContext {

        private List<Reference> outputReferences = new ArrayList<>();
        private List<Reference> orderByReferences = new ArrayList<>();
        public boolean collectOrderBy = false;

        public void addReference(Reference reference) {
            if (collectOrderBy) {
                orderByReferences.add(reference);
            } else {
                outputReferences.add(reference);
            }
        }

        public boolean outputsAreAllOrdered() {
            return orderByReferences.containsAll(outputReferences);
        }

    }

    static class OutputOrderReferenceCollector extends SymbolVisitor<OutputOrderReferenceContext, Void> {

        public OutputOrderReferenceContext collect(List<Symbol> symbols) {
            OutputOrderReferenceContext context = new OutputOrderReferenceContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, OutputOrderReferenceContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, OutputOrderReferenceContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, OutputOrderReferenceContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, OutputOrderReferenceContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, OutputOrderReferenceContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }
    }

    static class ReferencesCollectorContext {
        private List<Reference> outputReferences = new ArrayList<>();

        public void addReference(Reference reference) {
            outputReferences.add(reference);
        }

        public boolean containsAnyReference() {
            return !outputReferences.isEmpty();
        }
    }

    static class ReferencesCollector extends SymbolVisitor<ReferencesCollectorContext, Void> {

        public ReferencesCollectorContext collect(List<Symbol> symbols) {
            ReferencesCollectorContext context = new ReferencesCollectorContext();
            collect(symbols, context);
            return context;
        }

        public void collect(List<Symbol> symbols, ReferencesCollectorContext context) {
            for (Symbol symbol : symbols) {
                process(symbol, context);
            }
        }

        @Override
        public Void visitAggregation(Aggregation aggregation, ReferencesCollectorContext context) {
            for (Symbol symbol : aggregation.inputs()) {
                process(symbol, context);
            }
            return null;
        }

        @Override
        public Void visitReference(Reference symbol, ReferencesCollectorContext context) {
            context.addReference(symbol);
            return null;
        }

        @Override
        public Void visitDynamicReference(DynamicReference symbol, ReferencesCollectorContext context) {
            return visitReference(symbol, context);
        }

        @Override
        public Void visitFunction(Function function, ReferencesCollectorContext context) {
            for (Symbol symbol : function.arguments()) {
                process(symbol, context);
            }
            return null;
        }

    }
}


File: sql/src/main/java/io/crate/planner/consumer/ReduceOnCollectorGroupByConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.Constants;
import io.crate.analyze.HavingClause;
import io.crate.analyze.InsertFromSubQueryAnalyzedStatement;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QueriedTable;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.TableRelation;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.projectors.TopN;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.GroupByConsumer;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.NonDistributedGroupBy;
import io.crate.planner.projection.FilterProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.builder.ProjectionBuilder;
import io.crate.planner.projection.builder.SplitPoints;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Symbol;

import java.util.ArrayList;
import java.util.List;

import static com.google.common.base.MoreObjects.firstNonNull;

public class ReduceOnCollectorGroupByConsumer implements Consumer {

    private static final Visitor VISITOR = new Visitor();

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        Context ctx = new Context(context);
        context.rootRelation(VISITOR.process(context.rootRelation(), ctx));
        return ctx.result;
    }

    private static class Context {
        ConsumerContext consumerContext;
        boolean result = false;

        public Context(ConsumerContext context) {
            this.consumerContext = context;
        }
    }

    private static class Visitor extends AnalyzedRelationVisitor<Context, AnalyzedRelation> {

        @Override
        public AnalyzedRelation visitQueriedTable(QueriedTable table, Context context) {
            if (table.querySpec().groupBy() == null) {
                return table;
            }

            if (!GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    table.tableRelation(), table.querySpec().where(), table.querySpec().groupBy())) {
                return table;
            }

            if (table.querySpec().where().hasVersions()) {
                context.consumerContext.validationException(new VersionInvalidException());
                return table;
            }
            context.result = true;
            return optimizedReduceOnCollectorGroupBy(table, table.tableRelation(), context.consumerContext);
        }

        @Override
        public AnalyzedRelation visitInsertFromQuery(InsertFromSubQueryAnalyzedStatement insertFromSubQueryAnalyzedStatement, Context context) {
            InsertFromSubQueryConsumer.planInnerRelation(insertFromSubQueryAnalyzedStatement, context, this);
            return insertFromSubQueryAnalyzedStatement;
        }

        @Override
        protected AnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, Context context) {
            return relation;
        }

        /**
         * grouping on doc tables by clustered column or primary keys, no distribution needed
         * only one aggregation step as the mappers (shards) have row-authority
         *
         * produces:
         *
         * SELECT:
         * CollectNode ( GroupProjection, [FilterProjection], [TopN] )
         * LocalMergeNode ( TopN )
         */
        private AnalyzedRelation optimizedReduceOnCollectorGroupBy(QueriedTable table, TableRelation tableRelation, ConsumerContext context) {
            assert GroupByConsumer.groupedByClusteredColumnOrPrimaryKeys(
                    tableRelation, table.querySpec().where(), table.querySpec().groupBy()) : "not grouped by clustered column or primary keys";
            TableInfo tableInfo = tableRelation.tableInfo();
            GroupByConsumer.validateGroupBySymbols(tableRelation, table.querySpec().groupBy());
            List<Symbol> groupBy = table.querySpec().groupBy();

            boolean ignoreSorting = context.rootRelation() != table
                    && table.querySpec().limit() == null
                    && table.querySpec().offset() == TopN.NO_OFFSET;

            ProjectionBuilder projectionBuilder = new ProjectionBuilder(table.querySpec());
            SplitPoints splitPoints = projectionBuilder.getSplitPoints();

            // mapper / collect
            List<Symbol> collectOutputs = new ArrayList<>(
                    groupBy.size() +
                            splitPoints.aggregates().size());
            collectOutputs.addAll(groupBy);
            collectOutputs.addAll(splitPoints.aggregates());

            OrderBy orderBy = table.querySpec().orderBy();
            if (orderBy != null) {
                table.tableRelation().validateOrderBy(orderBy);
            }

            List<Projection> projections = new ArrayList<>();
            GroupProjection groupProjection = projectionBuilder.groupProjection(
                    splitPoints.leaves(),
                    table.querySpec().groupBy(),
                    splitPoints.aggregates(),
                    Aggregation.Step.ITER,
                    Aggregation.Step.FINAL
            );
            groupProjection.setRequiredGranularity(RowGranularity.SHARD);
            projections.add(groupProjection);

            HavingClause havingClause = table.querySpec().having();
            if (havingClause != null) {
                if (havingClause.noMatch()) {
                    return new NoopPlannedAnalyzedRelation(table);
                } else if (havingClause.hasQuery()) {
                    FilterProjection fp = projectionBuilder.filterProjection(
                            collectOutputs,
                            havingClause.query()
                    );
                    fp.requiredGranularity(RowGranularity.SHARD);
                    projections.add(fp);
                }
            }
            // mapper / collect
            // use topN on collector if needed
            boolean outputsMatch = table.querySpec().outputs().size() == collectOutputs.size() &&
                    collectOutputs.containsAll(table.querySpec().outputs());
            boolean collectorTopN = table.querySpec().limit() != null || table.querySpec().offset() > 0 || !outputsMatch;

            if (collectorTopN) {
                projections.add(projectionBuilder.topNProjection(
                        collectOutputs,
                        orderBy,
                        0, // no offset
                        firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT) + table.querySpec().offset(),
                        table.querySpec().outputs()
                ));
            }

            CollectNode collectNode = PlanNodeBuilder.collect(
                    tableInfo,
                    context.plannerContext(),
                    table.querySpec().where(),
                    splitPoints.leaves(),
                    ImmutableList.copyOf(projections)
            );

            // handler
            List<Projection> handlerProjections = new ArrayList<>();
            MergeNode localMergeNode;
            if (!ignoreSorting && collectorTopN && orderBy != null && orderBy.isSorted()) {
                // handler receives sorted results from collect nodes
                // we can do the sorting with a sorting bucket merger
                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                table.querySpec().outputs(),
                                null, // omit order by
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );
                localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                        handlerProjections, orderBy, table.querySpec().outputs(), null,
                        collectNode, context.plannerContext());
            } else {
                handlerProjections.add(
                        projectionBuilder.topNProjection(
                                collectorTopN ? table.querySpec().outputs() : collectOutputs,
                                orderBy,
                                table.querySpec().offset(),
                                firstNonNull(table.querySpec().limit(), Constants.DEFAULT_SELECT_LIMIT),
                                table.querySpec().outputs()
                        )
                );
                // fallback - unsorted local merge
                localMergeNode = PlanNodeBuilder.localMerge(handlerProjections, collectNode,
                        context.plannerContext());
            }
            return new NonDistributedGroupBy(collectNode, localMergeNode);
        }


    }
}


File: sql/src/main/java/io/crate/planner/consumer/UpdateConsumer.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.consumer;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.UpdateAnalyzedStatement;
import io.crate.analyze.VersionRewriter;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.analyze.relations.TableRelation;
import io.crate.analyze.where.DocKeys;
import io.crate.metadata.PartitionName;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.*;
import io.crate.planner.node.NoopPlannedAnalyzedRelation;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.CollectAndMerge;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.UpdateProjection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.ValueSymbolVisitor;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

@Singleton
public class UpdateConsumer implements Consumer {

    private final Visitor visitor;

    @Inject
    public UpdateConsumer() {
        visitor = new Visitor();
    }

    @Override
    public boolean consume(AnalyzedRelation rootRelation, ConsumerContext context) {
        PlannedAnalyzedRelation plannedAnalyzedRelation = visitor.process(rootRelation, context);
        if (plannedAnalyzedRelation == null) {
            return false;
        }
        context.rootRelation(plannedAnalyzedRelation);
        return true;
    }

    class Visitor extends AnalyzedRelationVisitor<ConsumerContext, PlannedAnalyzedRelation> {

        @Override
        public PlannedAnalyzedRelation visitUpdateAnalyzedStatement(UpdateAnalyzedStatement statement, ConsumerContext context) {
            assert statement.sourceRelation() instanceof TableRelation : "sourceRelation of update statement must be a TableRelation";
            TableRelation tableRelation = (TableRelation) statement.sourceRelation();
            TableInfo tableInfo = tableRelation.tableInfo();

            if (tableInfo.schemaInfo().systemSchema() || tableInfo.rowGranularity() != RowGranularity.DOC) {
                return null;
            }

            List<Plan> childNodes = new ArrayList<>(statement.nestedStatements().size());
            SymbolBasedUpsertByIdNode upsertByIdNode = null;
            for (UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis : statement.nestedStatements()) {
                WhereClause whereClause = nestedAnalysis.whereClause();
                if (whereClause.noMatch()){
                    continue;
                }
                if (whereClause.docKeys().isPresent()) {
                    if (upsertByIdNode == null) {
                        Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());
                        upsertByIdNode = new SymbolBasedUpsertByIdNode(context.plannerContext().nextExecutionNodeId(), false, statement.nestedStatements().size() > 1, assignments.v1(), null);
                        childNodes.add(new IterablePlan(upsertByIdNode));
                    }
                    upsertById(nestedAnalysis, tableInfo, whereClause, upsertByIdNode);
                } else {
                    Plan plan = upsertByQuery(nestedAnalysis, context, tableInfo, whereClause);
                    if (plan != null) {
                        childNodes.add(plan);
                    }
                }
            }
            if (childNodes.size() > 0){
                return new Upsert(childNodes);
            } else {
                return new NoopPlannedAnalyzedRelation(statement);
            }
        }

        private Plan upsertByQuery(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                   ConsumerContext consumerContext,
                                   TableInfo tableInfo,
                                   WhereClause whereClause) {

            Symbol versionSymbol = null;
            if(whereClause.hasVersions()){
                versionSymbol = VersionRewriter.get(whereClause.query());
                whereClause = new WhereClause(whereClause.query(), whereClause.docKeys().orNull(), whereClause.partitions());
            }


            if (!whereClause.noMatch() || !(tableInfo.isPartitioned() && whereClause.partitions().isEmpty())) {
                // for updates, we always need to collect the `_uid`
                Reference uidReference = new Reference(
                        new ReferenceInfo(
                                new ReferenceIdent(tableInfo.ident(), "_uid"),
                                RowGranularity.DOC, DataTypes.STRING));

                Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());

                Long version = null;
                if (versionSymbol != null){
                    version = ValueSymbolVisitor.LONG.process(versionSymbol);
                }

                UpdateProjection updateProjection = new UpdateProjection(
                        new InputColumn(0, DataTypes.STRING),
                        assignments.v1(),
                        assignments.v2(),
                        version);

                CollectNode collectNode = PlanNodeBuilder.collect(
                        tableInfo,
                        consumerContext.plannerContext(),
                        whereClause,
                        ImmutableList.<Symbol>of(uidReference),
                        ImmutableList.<Projection>of(updateProjection),
                        null,
                        Preference.PRIMARY.type()
                );
                MergeNode mergeNode = PlanNodeBuilder.localMerge(
                        ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode,
                        consumerContext.plannerContext());
                return new CollectAndMerge(collectNode, mergeNode);
            } else {
                return null;
            }
        }

        private void upsertById(UpdateAnalyzedStatement.NestedAnalyzedStatement nestedAnalysis,
                                             TableInfo tableInfo,
                                             WhereClause whereClause,
                                             SymbolBasedUpsertByIdNode upsertByIdNode) {
            String[] indices = Planner.indices(tableInfo, whereClause);
            assert tableInfo.isPartitioned() || indices.length == 1;

            Tuple<String[], Symbol[]> assignments = convertAssignments(nestedAnalysis.assignments());


            for (DocKeys.DocKey key : whereClause.docKeys().get()) {
                String index;
                if (key.partitionValues().isPresent()) {
                    index = new PartitionName(tableInfo.ident(), key.partitionValues().get()).stringValue();
                } else {
                    index = indices[0];
                }
                upsertByIdNode.add(
                        index,
                        key.id(),
                        key.routing(),
                        assignments.v2(),
                        key.version().orNull());
            }
        }


        private Tuple<String[], Symbol[]> convertAssignments(Map<Reference, Symbol> assignments) {
            String[] assignmentColumns = new String[assignments.size()];
            Symbol[] assignmentSymbols = new Symbol[assignments.size()];
            Iterator<Reference> it = assignments.keySet().iterator();
            int i = 0;
            while(it.hasNext()) {
                Reference key = it.next();
                assignmentColumns[i] = key.ident().columnIdent().fqn();
                assignmentSymbols[i] = assignments.get(key);
                i++;
            }
            return new Tuple<>(assignmentColumns, assignmentSymbols);
        }

        @Override
        protected PlannedAnalyzedRelation visitAnalyzedRelation(AnalyzedRelation relation, ConsumerContext context) {
            return null;
        }
    }
}


File: sql/src/main/java/io/crate/planner/node/ExecutionNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node;

import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import org.elasticsearch.common.io.stream.Streamable;

import java.util.List;
import java.util.Set;
import java.util.UUID;

public interface ExecutionNode extends Streamable {

    String DIRECT_RETURN_DOWNSTREAM_NODE = "_response";

    int NO_EXECUTION_NODE = Integer.MAX_VALUE;

    interface ExecutionNodeFactory<T extends ExecutionNode> {
        T create();
    }

    enum Type {
        COLLECT(CollectNode.FACTORY),
        COUNT(CountNode.FACTORY),
        FILE_URI_COLLECT(FileUriCollectNode.FACTORY),
        MERGE(MergeNode.FACTORY);

        private final ExecutionNodeFactory factory;

        Type(ExecutionNodeFactory factory) {
            this.factory = factory;
        }

        public ExecutionNodeFactory factory() {
            return factory;
        }
    }

    Type type();

    String name();

    int executionNodeId();

    Set<String> executionNodes();

    List<String> downstreamNodes();

    int downstreamExecutionNodeId();

    UUID jobId();

    void jobId(UUID jobId);


    <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context);
}


File: sql/src/main/java/io/crate/planner/node/NoopPlannedAnalyzedRelation.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node;

import io.crate.analyze.relations.AnalyzedRelation;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.Path;
import io.crate.planner.NoopPlan;
import io.crate.planner.Plan;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Field;

import javax.annotation.Nullable;
import java.util.List;

public class NoopPlannedAnalyzedRelation implements PlannedAnalyzedRelation {

    private final AnalyzedRelation relation;

    public NoopPlannedAnalyzedRelation(AnalyzedRelation relation) {
        this.relation = relation;
    }

    @Override
    public Plan plan() {
        return NoopPlan.INSTANCE;
    }

    @Override
    public <C, R> R accept(AnalyzedRelationVisitor<C, R> visitor, C context) {
        return visitor.visitPlanedAnalyzedRelation(this, context);
    }

    @Nullable
    @Override
    public Field getField(Path path) {
        return relation.getField(path);
    }

    @Override
    public Field getWritableField(Path path) throws UnsupportedOperationException, ColumnUnknownException {
        return relation.getWritableField(path);
    }

    @Override
    public List<Field> fields() {
        return relation.fields();
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("addingProjection not supported");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported");
    }

}


File: sql/src/main/java/io/crate/planner/node/dml/InsertFromSubQuery.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dml;


import com.google.common.base.Optional;
import io.crate.planner.Plan;
import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;

public class InsertFromSubQuery extends PlanAndPlannedAnalyzedRelation {


    private final Optional<MergeNode> handlerMergeNode;

    private final Plan innerPlan;

    public InsertFromSubQuery(Plan innerPlan, @Nullable MergeNode handlerMergeNode) {
        this.innerPlan = innerPlan;
        this.handlerMergeNode = Optional.fromNullable(handlerMergeNode);
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitInsertByQuery(this, context);
    }

    public Plan innerPlan() {
        return innerPlan;
    }

    public Optional<MergeNode> handlerMergeNode() {
        return handlerMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("addingProjection not supported");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported");
    }
}


File: sql/src/main/java/io/crate/planner/node/dml/Upsert.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dml;

import io.crate.planner.Plan;
import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.projection.Projection;

import java.util.List;

public class Upsert extends PlanAndPlannedAnalyzedRelation {

    private final List<Plan> nodes;

    public Upsert(List<Plan> nodes) {
        this.nodes = nodes;
    }

    public List<Plan> nodes() {
        return nodes;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitUpsert(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("adding projection not supported");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported");
    }

}


File: sql/src/main/java/io/crate/planner/node/dql/AbstractDQLPlanNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Optional;
import com.google.common.collect.ImmutableList;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.projection.Projection;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.common.io.stream.Streamable;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;

public abstract class AbstractDQLPlanNode implements DQLPlanNode, Streamable, ExecutionNode {

    private UUID jobId;
    private int executionNodeId;
    private String name;
    protected List<Projection> projections = ImmutableList.of();
    protected List<DataType> outputTypes = ImmutableList.of();
    private List<DataType> inputTypes;

    public AbstractDQLPlanNode() {

    }

    protected AbstractDQLPlanNode(int executionNodeId, String name) {
        this.executionNodeId = executionNodeId;
        this.name = name;
    }

    public String name() {
        return name;
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    public boolean hasProjections() {
        return projections != null && projections.size() > 0;
    }

    @Override
    public List<Projection> projections() {
        return projections;
    }

    public void projections(List<Projection> projections) {
        this.projections = projections;
    }

    @Override
    public void addProjection(Projection projection) {
        List<Projection> projections = new ArrayList<>(this.projections);
        projections.add(projection);
        this.projections = ImmutableList.copyOf(projections);
    }

    public Optional<Projection> finalProjection() {
        if (projections.size() == 0) {
            return Optional.absent();
        } else {
            return Optional.of(projections.get(projections.size()-1));
        }
    }


    public void outputTypes(List<DataType> outputTypes) {
        this.outputTypes = outputTypes;
    }

    public List<DataType> outputTypes() {
        return outputTypes;
    }

    @Override
    public void inputTypes(List<DataType> dataTypes) {
        this.inputTypes = dataTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        name = in.readString();
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            outputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                outputTypes.add(DataTypes.fromStream(in));
            }
        }

        int numProjections = in.readVInt();
        if (numProjections > 0) {
            projections = new ArrayList<>(numProjections);
            for (int i = 0; i < numProjections; i++) {
                projections.add(Projection.fromStream(in));
            }
        }

    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        out.writeString(name);
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);

        int numCols = outputTypes.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            DataTypes.toStream(outputTypes.get(i), out);
        }

        if (hasProjections()) {
            out.writeVInt(projections.size());
            for (Projection p : projections) {
                Projection.toStream(p, out);
            }
        } else {
            out.writeVInt(0);
        }
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;

        AbstractDQLPlanNode node = (AbstractDQLPlanNode) o;

        return !(name != null ? !name.equals(node.name) : node.name != null);

    }

    @Override
    public int hashCode() {
        return name != null ? name.hashCode() : 0;
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("name", name)
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .toString();
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/CollectAndMerge.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import io.crate.planner.Plan;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.PlanNode;

import java.util.Arrays;
import java.util.Iterator;

public class CollectAndMerge implements Iterable<PlanNode>, Plan {

    private final CollectNode collectNode;
    private final MergeNode localMergeNode;
    private Iterable<PlanNode> nodes;

    public CollectAndMerge(CollectNode collectNode, MergeNode localMergeNode) {
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
        nodes = Arrays.<PlanNode>asList(collectNode, localMergeNode);
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    @Override
    public Iterator<PlanNode> iterator() {
        return nodes.iterator();
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitCollectAndMerge(this, context);
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/CollectNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Sets;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Symbol;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;

/**
 * A plan node which collects data.
 */
public class CollectNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<CollectNode> FACTORY = new ExecutionNodeFactory<CollectNode>() {
        @Override
        public CollectNode create() {
            return new CollectNode();
        }
    };
    private Routing routing;
    private List<Symbol> toCollect;
    private WhereClause whereClause = WhereClause.MATCH_ALL;
    private RowGranularity maxRowGranularity = RowGranularity.CLUSTER;

    @Nullable
    private List<String> downstreamNodes;

    private int downstreamExecutionNodeId = ExecutionNode.NO_EXECUTION_NODE;

    private boolean isPartitioned = false;
    private boolean keepContextForFetcher = false;
    private @Nullable String handlerSideCollect = null;

    private @Nullable Integer limit = null;
    private @Nullable OrderBy orderBy = null;

    protected CollectNode() {
        super();
    }

    public CollectNode(int executionNodeId, String name) {
        super(executionNodeId, name);
    }

    public CollectNode(int executionNodeId, String name, Routing routing) {
        this(executionNodeId, name, routing, ImmutableList.<Symbol>of(), ImmutableList.<Projection>of());
    }

    public CollectNode(int executionNodeId, String name, Routing routing, List<Symbol> toCollect, List<Projection> projections) {
        super(executionNodeId, name);
        this.routing = routing;
        this.toCollect = toCollect;
        this.projections = projections;
        this.downstreamNodes = ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public Type type() {
        return Type.COLLECT;
    }

    /**
     * @return a set of node ids where this collect operation is executed,
     *         excluding the NULL_NODE_ID for special collect purposes
     */
    @Override
    public Set<String> executionNodes() {
        if (routing != null) {
            if (routing.isNullRouting()) {
                return routing.nodes();
            } else {
                return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
            }
        } else {
            return ImmutableSet.of();
        }
    }

    public @Nullable Integer limit() {
        return limit;
    }

    public void limit(Integer limit) {
        this.limit = limit;
    }

    public @Nullable OrderBy orderBy() {
        return orderBy;
    }

    public void orderBy(@Nullable OrderBy orderBy) {
        this.orderBy = orderBy;
    }

    @Nullable
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    /**
     * This method returns true if downstreams other than
     * {@link ExecutionNode#DIRECT_RETURN_DOWNSTREAM_NODE} are defined, which means that results
     * of this collect operation should be sent to other nodes instead of being returned directly.
     */
    public boolean hasDistributingDownstreams() {
        if (downstreamNodes != null && downstreamNodes.size() > 0) {
            if (downstreamNodes.size() == 1
                    && downstreamNodes.get(0).equals(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE)) {
                return false;
            }
            return true;
        }
        return false;
    }

    public void downstreamNodes(List<String> downStreamNodes) {
        this.downstreamNodes = downStreamNodes;
    }

    public void downstreamExecutionNodeId(int executionNodeId) {
        this.downstreamExecutionNodeId = executionNodeId;
    }

    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    public void whereClause(WhereClause whereClause) {
        assert whereClause != null;
        this.whereClause = whereClause;
    }

    public Routing routing() {
        return routing;
    }

    public List<Symbol> toCollect() {
        return toCollect;
    }

    public void toCollect(List<Symbol> toCollect) {
        assert toCollect != null;
        this.toCollect = toCollect;
    }

    public boolean isRouted() {
        return routing != null && routing.hasLocations();
    }

    /**
     * Whether collect operates on a partitioned table.
     * Only used on local collect, so no serialization is needed.
     *
     * @return true if collect operates on a partitioned table, false otherwise
     */
    public boolean isPartitioned() {
        return isPartitioned;
    }

    public void isPartitioned(boolean isPartitioned) {
        this.isPartitioned = isPartitioned;
    }

    public RowGranularity maxRowGranularity() {
        return maxRowGranularity;
    }

    public void maxRowGranularity(RowGranularity newRowGranularity) {
        if (maxRowGranularity.compareTo(newRowGranularity) < 0) {
            maxRowGranularity = newRowGranularity;
        }
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            toCollect = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                toCollect.add(Symbol.fromStream(in));
            }
        } else {
            toCollect = ImmutableList.of();
        }

        maxRowGranularity = RowGranularity.fromStream(in);

        if (in.readBoolean()) {
            routing = new Routing();
            routing.readFrom(in);
        }

        whereClause = new WhereClause(in);

        int numDownStreams = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownStreams);
        for (int i = 0; i < numDownStreams; i++) {
            downstreamNodes.add(in.readString());
        }
        keepContextForFetcher = in.readBoolean();

        if( in.readBoolean()) {
            limit = in.readVInt();
        }

        if (in.readBoolean()) {
            orderBy = OrderBy.fromStream(in);
        }
        isPartitioned = in.readBoolean();
        handlerSideCollect = in.readOptionalString();
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        int numCols = toCollect.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            Symbol.toStream(toCollect.get(i), out);
        }

        RowGranularity.toStream(maxRowGranularity, out);

        if (routing != null) {
            out.writeBoolean(true);
            routing.writeTo(out);
        } else {
            out.writeBoolean(false);
        }
        whereClause.writeTo(out);

        if (downstreamNodes != null) {
            out.writeVInt(downstreamNodes.size());
            for (String downstreamNode : downstreamNodes) {
                out.writeString(downstreamNode);
            }
        } else {
            out.writeVInt(0);
        }
        out.writeBoolean(keepContextForFetcher);
        if (limit != null ) {
            out.writeBoolean(true);
            out.writeVInt(limit);
        } else {
            out.writeBoolean(false);
        }
        if (orderBy != null) {
            out.writeBoolean(true);
            OrderBy.toStream(orderBy, out);
        } else {
            out.writeBoolean(false);
        }
        out.writeBoolean(isPartitioned);
        out.writeOptionalString(handlerSideCollect);
    }

    /**
     * normalizes the symbols of this node with the given normalizer
     *
     * @return a normalized node, if no changes occurred returns this
     */
    public CollectNode normalize(EvaluatingNormalizer normalizer) {
        assert whereClause() != null;
        CollectNode result = this;
        List<Symbol> newToCollect = normalizer.normalize(toCollect());
        boolean changed = newToCollect != toCollect();
        WhereClause newWhereClause = whereClause().normalize(normalizer);
        if (newWhereClause != whereClause()) {
            changed = changed || newWhereClause != whereClause();
        }
        if (changed) {
            result = new CollectNode(executionNodeId(), name(), routing, newToCollect, projections);
            result.downstreamNodes = downstreamNodes;
            result.maxRowGranularity = maxRowGranularity;
            result.jobId(jobId());
            result.keepContextForFetcher = keepContextForFetcher;
            result.handlerSideCollect = handlerSideCollect;
            result.isPartitioned(isPartitioned);
            result.whereClause(newWhereClause);
        }
        return result;
    }

    public void keepContextForFetcher(boolean keepContextForFetcher) {
        this.keepContextForFetcher = keepContextForFetcher;
    }

    public boolean keepContextForFetcher() {
        return keepContextForFetcher;
    }

    public void handlerSideCollect(String handlerSideCollect) {
        this.handlerSideCollect = handlerSideCollect;
    }

    @Nullable
    public String handlerSideCollect() {
        return handlerSideCollect;
    }
}

File: sql/src/main/java/io/crate/planner/node/dql/CountNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import java.io.IOException;
import java.util.List;
import java.util.Set;
import java.util.UUID;

public class CountNode implements ExecutionNode {

    public static final ExecutionNodeFactory<CountNode> FACTORY = new ExecutionNodeFactory<CountNode>() {
        @Override
        public CountNode create() {
            return new CountNode();
        }
    };
    private UUID jobId;
    private int executionNodeId;
    private Routing routing;
    private WhereClause whereClause;

    CountNode() {}

    public CountNode(int executionNodeId, Routing routing, WhereClause whereClause) {
        this.executionNodeId = executionNodeId;
        this.routing = routing;
        this.whereClause = whereClause;
    }

    @Override
    public Type type() {
        return Type.COUNT;
    }

    @Override
    public String name() {
        return "count";
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    public Routing routing() {
        return routing;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public Set<String> executionNodes() {
        if (routing.isNullRouting()) {
            return routing.nodes();
        } else {
            return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return ExecutionNode.NO_EXECUTION_NODE;
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCountNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        routing = new Routing();
        routing.readFrom(in);
        whereClause = new WhereClause(in);
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);
        routing.writeTo(out);
        whereClause.writeTo(out);
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/CountPlan.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class CountPlan extends PlanAndPlannedAnalyzedRelation{

    private final CountNode countNode;
    private final MergeNode mergeNode;

    public CountPlan(CountNode countNode, MergeNode mergeNode) {
        this.countNode = countNode;
        this.mergeNode = mergeNode;
    }

    public CountNode countNode() {
        return countNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitCountPlan(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.addProjection(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return false;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode;
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/DistributedGroupBy.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;

public class DistributedGroupBy extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private final MergeNode reducerMergeNode;
    private MergeNode localMergeNode;

    public DistributedGroupBy(CollectNode collectNode, MergeNode reducerMergeNode, @Nullable MergeNode localMergeNode) {
        this.collectNode = collectNode;
        this.reducerMergeNode = reducerMergeNode;
        this.localMergeNode = localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitDistributedGroupBy(this, context);
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode reducerMergeNode() {
        return reducerMergeNode;
    }

    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        DQLPlanNode node = resultNode();
        node.addProjection(projection);
        if (node instanceof CollectNode) {
            PlanNodeBuilder.setOutputTypes((CollectNode)node);
        } else if (node instanceof MergeNode) {
            PlanNodeBuilder.connectTypes(reducerMergeNode, node);
        }
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode != null ? localMergeNode : reducerMergeNode;
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/ESDQLPlanNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.base.Optional;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.analyze.relations.AnalyzedRelationVisitor;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.exceptions.ColumnUnknownException;
import io.crate.metadata.Path;
import io.crate.planner.IterablePlan;
import io.crate.planner.Plan;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Field;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.ValueSymbolVisitor;
import io.crate.types.DataType;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Set;

public abstract class ESDQLPlanNode implements DQLPlanNode, PlannedAnalyzedRelation {

    private static final char COMMA = ',';

    protected List<Symbol> outputs;
    private List<DataType> inputTypes;
    private List<DataType> outputTypes;

    public List<Symbol> outputs() {
        return outputs;
    }

    @Nullable
    public static String noCommaStringRouting(Optional<Set<Symbol>> clusteredBy) {
        if (clusteredBy.isPresent()){
            StringBuilder sb = new StringBuilder();
            boolean first = true;
            for (Symbol symbol : clusteredBy.get()) {
                String s = ValueSymbolVisitor.STRING.process(symbol);
                if (s.indexOf(COMMA)>-1){
                    return null;
                }
                if (!first){
                    sb.append(COMMA);
                } else {
                    first = false;
                }
                sb.append(s);
            }
            return sb.toString();
        }
        return null;
    }

    @Override
    public boolean hasProjections() {
        return false;
    }

    @Override
    public List<Projection> projections() {
        return ImmutableList.of();
    }

    @Override
    public void inputTypes(List<DataType> dataTypes) {
        this.inputTypes = dataTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public Set<String> executionNodes() {
        return ImmutableSet.of();
    }

    @Override
    public void outputTypes(List<DataType> outputTypes) {
        this.outputTypes = outputTypes;
    }

    @Override
    public List<DataType> outputTypes() {
        return outputTypes;
    }

    @Override
    public <C, R> R accept(AnalyzedRelationVisitor<C, R> visitor, C context) {
        return visitor.visitPlanedAnalyzedRelation(this, context);
    }

    @javax.annotation.Nullable
    @Override
    public Field getField(Path path) {
        throw new UnsupportedOperationException("getField is not supported on ESDQLPlanNode");
    }

    @Override
    public Field getWritableField(Path path) throws UnsupportedOperationException, ColumnUnknownException {
        throw new UnsupportedOperationException("getWritableField is not supported on ESDQLPlanNode");
    }

    @Override
    public List<Field> fields() {
        throw new UnsupportedOperationException("fields is not supported on ESDQLPlanNode");
    }

    @Override
    public void addProjection(Projection projection) {
        throw new UnsupportedOperationException("addProjection not supported on ESDQLPlanNode");
    }

    @Override
    public boolean resultIsDistributed() {
        throw new UnsupportedOperationException("resultIsDistributed is not supported on ESDQLPlanNode");
    }

    @Override
    public DQLPlanNode resultNode() {
        throw new UnsupportedOperationException("resultNode is not supported on ESDQLPLanNode");
    }

    @Override
    public Plan plan() {
        return new IterablePlan(this);
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/ESGetNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import io.crate.analyze.OrderBy;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.where.DocKeys;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Symbols;
import org.elasticsearch.common.Nullable;

import java.util.List;


public class ESGetNode extends ESDQLPlanNode implements DQLPlanNode {

    private final TableInfo tableInfo;
    private final QuerySpec querySpec;
    private final List<Symbol> sortSymbols;
    private final boolean[] reverseFlags;
    private final Boolean[] nullsFirst;
    private final int executionNodeId;

    private final static boolean[] EMPTY_REVERSE_FLAGS = new boolean[0];
    private final static Boolean[] EMPTY_NULLS_FIRST = new Boolean[0];
    private final DocKeys docKeys;

    public ESGetNode(int executionNodeId,
                     TableInfo tableInfo,
                     QuerySpec querySpec) {

        assert querySpec.where().docKeys().isPresent();
        this.tableInfo = tableInfo;
        this.querySpec = querySpec;
        this.outputs = querySpec.outputs();
        this.docKeys = querySpec.where().docKeys().get();
        this.executionNodeId = executionNodeId;


        outputTypes(Symbols.extractTypes(outputs));

        OrderBy orderBy = querySpec.orderBy();
        if (orderBy != null && orderBy.isSorted()){
            this.sortSymbols = orderBy.orderBySymbols();
            this.reverseFlags = orderBy.reverseFlags();
            this.nullsFirst = orderBy.nullsFirst();
        } else {
            this.sortSymbols = ImmutableList.<Symbol>of();
            this.reverseFlags = EMPTY_REVERSE_FLAGS;
            this.nullsFirst = EMPTY_NULLS_FIRST;
        }
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitESGetNode(this, context);
    }

    public TableInfo tableInfo() {
        return tableInfo;
    }

    public QuerySpec querySpec() {
        return querySpec;
    }

    public DocKeys docKeys() {
        return docKeys;
    }

    @Nullable
    public Integer limit() {
        return querySpec().limit();
    }

    public int offset() {
        return querySpec().offset();
    }

    public List<Symbol> sortSymbols() {
        return sortSymbols;
    }

    public boolean[] reverseFlags() {
        return reverseFlags;
    }

    public Boolean[] nullsFirst() {
        return nullsFirst;
    }

    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("docKeys", docKeys)
                .add("outputs", outputs)
                .toString();
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/FileUriCollectNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.operation.collect.files.FileReadingCollector;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Symbol;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.List;

public class FileUriCollectNode extends CollectNode {

    public static final ExecutionNodeFactory<FileUriCollectNode> FACTORY = new ExecutionNodeFactory<FileUriCollectNode>() {
        @Override
        public FileUriCollectNode create() {
            return new FileUriCollectNode();
        }
    };
    private Symbol targetUri;
    private String compression;
    private Boolean sharedStorage;

    private FileUriCollectNode() {
        super();
    }

    public FileUriCollectNode(int executionNodeId,
                              String name,
                              Routing routing,
                              Symbol targetUri,
                              List<Symbol> toCollect,
                              List<Projection> projections,
                              String compression,
                              Boolean sharedStorage) {
        super(executionNodeId, name, routing, toCollect, projections);
        this.targetUri = targetUri;
        this.compression = compression;
        this.sharedStorage = sharedStorage;
    }

    public Symbol targetUri() {
        return targetUri;
    }

    public FileReadingCollector.FileFormat fileFormat() {
        return FileReadingCollector.FileFormat.JSON;
    }

    @Override
    public Type type() {
        return Type.FILE_URI_COLLECT;
    }

    @Override
    public FileUriCollectNode normalize(EvaluatingNormalizer normalizer) {
        List<Symbol> normalizedToCollect = normalizer.normalize(toCollect());
        Symbol normalizedTargetUri = normalizer.normalize(targetUri);
        WhereClause normalizedWhereClause = whereClause().normalize(normalizer);
        boolean changed =
                (normalizedToCollect != toCollect() )
                        || (normalizedTargetUri != targetUri)
                        || (normalizedWhereClause != whereClause());
        if (!changed) {
            return this;
        }
        FileUriCollectNode result = new FileUriCollectNode(
                executionNodeId(),
                name(),
                routing(),
                normalizedTargetUri,
                normalizedToCollect,
                projections(),
                compression(),
                sharedStorage());
        result.downstreamNodes(downstreamNodes());
        result.maxRowGranularity(maxRowGranularity());
        result.jobId(jobId());
        result.isPartitioned(isPartitioned());
        result.whereClause(normalizedWhereClause);
        return result;
    }

    @Nullable
    public String compression() {
        return compression;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        compression = in.readOptionalString();
        sharedStorage = in.readOptionalBoolean();
        targetUri = Symbol.fromStream(in);
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeOptionalString(compression);
        out.writeOptionalBoolean(sharedStorage);
        Symbol.toStream(targetUri, out);
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("name", name())
                .add("targetUri", targetUri)
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .add("compression", compression)
                .add("sharedStorageDefault", sharedStorage)
                .toString();
    }

    @Nullable
    public Boolean sharedStorage() {
        return sharedStorage;
    }
}



File: sql/src/main/java/io/crate/planner/node/dql/GlobalAggregate.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class GlobalAggregate extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode mergeNode;

    public GlobalAggregate(CollectNode collectNode, MergeNode mergeNode) {
        this.collectNode = collectNode;
        this.mergeNode = mergeNode;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitGlobalAggregate(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.projections().add(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return mergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode == null ? collectNode : mergeNode;
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/MergeNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;

/**
 * A plan node which merges results from upstreams
 */
public class MergeNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<MergeNode> FACTORY = new ExecutionNodeFactory<MergeNode>() {
        @Override
        public MergeNode create() {
            return new MergeNode();
        }
    };

    private List<DataType> inputTypes;
    private int numUpstreams;
    private Set<String> executionNodes;

    /**
     * expects sorted input and produces sorted output
     */
    private boolean sortedInputOutput = false;
    private int[] orderByIndices;
    private boolean[] reverseFlags;
    private Boolean[] nullsFirst;
    private int downstreamExecutionNodeId = NO_EXECUTION_NODE;
    private List<String> downstreamNodes = ImmutableList.of();

    public MergeNode() {
        numUpstreams = 0;
    }

    public MergeNode(int executionNodeId, String name, int numUpstreams) {
        super(executionNodeId, name);
        this.numUpstreams = numUpstreams;
    }

    public static MergeNode sortedMergeNode(int executionNodeId,
                                            String name,
                                            int numUpstreams,
                                            int[] orderByIndices,
                                            boolean[] reverseFlags,
                                            Boolean[] nullsFirst) {
        Preconditions.checkArgument(
                orderByIndices.length == reverseFlags.length && reverseFlags.length == nullsFirst.length,
                "ordering parameters must be of the same length");
        MergeNode mergeNode = new MergeNode(executionNodeId, name, numUpstreams);
        mergeNode.sortedInputOutput = true;
        mergeNode.orderByIndices = orderByIndices;
        mergeNode.reverseFlags = reverseFlags;
        mergeNode.nullsFirst = nullsFirst;
        return mergeNode;
    }

    @Override
    public Type type() {
        return Type.MERGE;
    }

    @Override
    public Set<String> executionNodes() {
        if (executionNodes == null) {
            return ImmutableSet.of();
        } else {
            return executionNodes;
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    public void downstreamNodes(Set<String> nodes) {
        downstreamNodes = ImmutableList.copyOf(nodes);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public void executionNodes(Set<String> executionNodes) {
        this.executionNodes = executionNodes;
    }

    public int numUpstreams() {
        return numUpstreams;
    }

    public List<DataType> inputTypes() {
        return inputTypes;
    }

    public void inputTypes(List<DataType> inputTypes) {
        this.inputTypes = inputTypes;
    }

    public boolean sortedInputOutput() {
        return sortedInputOutput;
    }

    @Nullable
    public int[] orderByIndices() {
        return orderByIndices;
    }

    @Nullable
    public boolean[] reverseFlags() {
        return reverseFlags;
    }

    @Nullable
    public Boolean[] nullsFirst() {
        return nullsFirst;
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numDownstreamNodes = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownstreamNodes);
        for (int i = 0; i < numDownstreamNodes; i++) {
            downstreamNodes.add(in.readString());
        }

        numUpstreams = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            inputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                inputTypes.add(DataTypes.fromStream(in));
            }
        }
        int numExecutionNodes = in.readVInt();

        if (numExecutionNodes > 0) {
            executionNodes = new HashSet<>(numExecutionNodes);
            for (int i = 0; i < numExecutionNodes; i++) {
                executionNodes.add(in.readString());
            }
        }

        sortedInputOutput = in.readBoolean();
        if (sortedInputOutput) {
            int orderByIndicesLength = in.readVInt();
            orderByIndices = new int[orderByIndicesLength];
            reverseFlags = new boolean[orderByIndicesLength];
            nullsFirst = new Boolean[orderByIndicesLength];
            for (int i = 0; i < orderByIndicesLength; i++) {
                orderByIndices[i] = in.readVInt();
                reverseFlags[i] = in.readBoolean();
                nullsFirst[i] = in.readOptionalBoolean();
            }
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        out.writeVInt(downstreamNodes.size());
        for (String downstreamNode : downstreamNodes) {
            out.writeString(downstreamNode);
        }

        out.writeVInt(numUpstreams);

        int numCols = inputTypes.size();
        out.writeVInt(numCols);
        for (DataType inputType : inputTypes) {
            DataTypes.toStream(inputType, out);
        }

        if (executionNodes == null) {
            out.writeVInt(0);
        } else {
            out.writeVInt(executionNodes.size());
            for (String node : executionNodes) {
                out.writeString(node);
            }
        }

        out.writeBoolean(sortedInputOutput);
        if (sortedInputOutput) {
            out.writeVInt(orderByIndices.length);
            for (int i = 0; i < orderByIndices.length; i++) {
                out.writeVInt(orderByIndices[i]);
                out.writeBoolean(reverseFlags[i]);
                out.writeOptionalBoolean(nullsFirst[i]);
            }
        }
    }

    @Override
    public String toString() {
        MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this)
                .add("executionNodeId", executionNodeId())
                .add("name", name())
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .add("jobId", jobId())
                .add("numUpstreams", numUpstreams)
                .add("executionNodes", executionNodes)
                .add("inputTypes", inputTypes)
                .add("sortedInputOutput", sortedInputOutput);
        if (sortedInputOutput) {
            helper.add("orderByIndices", Arrays.toString(orderByIndices))
                  .add("reverseFlags", Arrays.toString(reverseFlags))
                  .add("nullsFirst", Arrays.toString(nullsFirst));
        }
        return helper.toString();
    }

    public void downstreamExecutionNodeId(int downstreamExecutionNodeId) {
        this.downstreamExecutionNodeId = downstreamExecutionNodeId;
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/NonDistributedGroupBy.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */
package io.crate.planner.node.dql;


import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class NonDistributedGroupBy extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode localMergeNode;

    public NonDistributedGroupBy(CollectNode collectNode, MergeNode localMergeNode){
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitNonDistributedGroupBy(this, context);
    }

    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    @Override
    public void addProjection(Projection projection) {
        this.resultNode().addProjection(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? collectNode : localMergeNode;
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/QueryAndFetch.java
package io.crate.planner.node.dql;


import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;

public class QueryAndFetch extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode localMergeNode;

    public QueryAndFetch(CollectNode collectNode, @Nullable MergeNode localMergeNode){
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitQueryAndFetch(this, context);
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode localMergeNode(){
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        DQLPlanNode node = resultNode();
        node.addProjection(projection);
        if (node instanceof CollectNode) {
            PlanNodeBuilder.setOutputTypes((CollectNode)node);
        } else if (node instanceof MergeNode) {
            PlanNodeBuilder.connectTypes(collectNode, node);
        }
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? collectNode : localMergeNode;
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/QueryThenFetch.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class QueryThenFetch extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode mergeNode;

    public QueryThenFetch(CollectNode collectNode, MergeNode mergeNode) {
        this.collectNode = collectNode;
        this.mergeNode = mergeNode;
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitQueryThenFetch(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.projections().add(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return mergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode == null ? collectNode : mergeNode;
    }
}


File: sql/src/main/java/io/crate/planner/node/management/KillPlan.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.management;

import io.crate.planner.Plan;
import io.crate.planner.PlanVisitor;

public class KillPlan implements Plan {

    public static final KillPlan INSTANCE = new KillPlan();

    private KillPlan() {
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitKillPlan(this, context);
    }
}


File: sql/src/test/java/io/crate/executor/transport/BaseTransportExecutorTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.WhereClause;
import io.crate.analyze.where.DocKeys;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.integrationtests.Setup;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.TableIdent;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.ESGetNode;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataTypes;
import org.junit.After;
import org.junit.Before;

import java.util.ArrayList;
import java.util.List;

public class BaseTransportExecutorTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);

    static {
        ClassLoader.getSystemClassLoader().setDefaultAssertionStatus(true);
    }

    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    TableIdent charactersIdent = new TableIdent(null, "characters");
    TableIdent booksIdent = new TableIdent(null, "books");

    Reference idRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference nameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference femaleRef = TestingHelpers.createReference(charactersIdent.name(), new ColumnIdent("female"), DataTypes.BOOLEAN);

    TableIdent partedTable = new TableIdent("doc", "parted");
    Reference partedIdRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference partedNameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference partedDateRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "date"), RowGranularity.PARTITION, DataTypes.TIMESTAMP));

    public static ESGetNode newGetNode(TableInfo tableInfo, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        QuerySpec querySpec = new QuerySpec();
        querySpec.outputs(outputs);
        List<List<Symbol>> keys = new ArrayList<>(singleStringKeys.size());
        for (String v : singleStringKeys) {
            keys.add(ImmutableList.<Symbol>of(Literal.newLiteral(v)));
        }
        WhereClause whereClause = new WhereClause(null, new DocKeys(keys, false, -1, null), null);
        querySpec.where(whereClause);
        return new ESGetNode(executionNodeId, tableInfo, querySpec);
    }

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }
}


File: sql/src/test/java/io/crate/executor/transport/ExecutionNodesTaskTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.metadata.Routing;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeGrouper;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import org.hamcrest.Matchers;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.Matchers.is;
import static org.junit.Assert.assertThat;

public class ExecutionNodesTaskTest {


    @Test
    public void testGroupByServer() throws Exception {

        Routing twoNodeRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
                .put("node1", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(1, 2)).map())
                .put("node2", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(3, 4)).map())
                .map());

        CollectNode c1 = new CollectNode(1, "c1", twoNodeRouting);

        MergeNode m1 = new MergeNode(2, "merge1", 2);
        m1.executionNodes(Sets.newHashSet("node3", "node4"));

        MergeNode m2 = new MergeNode(3, "merge2", 2);
        m2.executionNodes(Sets.newHashSet("node1", "node3"));

        Map<String, Collection<ExecutionNode>> groupByServer = ExecutionNodeGrouper.groupByServer("node1", ImmutableList.<List<ExecutionNode>>of(ImmutableList.<ExecutionNode>of(c1, m1, m2)));

        assertThat(groupByServer.containsKey("node1"), is(true));
        assertThat(groupByServer.get("node1"), Matchers.<ExecutionNode>containsInAnyOrder(c1, m2));

        assertThat(groupByServer.containsKey("node2"), is(true));
        assertThat(groupByServer.get("node2"), Matchers.<ExecutionNode>containsInAnyOrder(c1));

        assertThat(groupByServer.containsKey("node3"), is(true));
        assertThat(groupByServer.get("node3"), Matchers.<ExecutionNode>containsInAnyOrder(m1, m2));

        assertThat(groupByServer.containsKey("node4"), is(true));
        assertThat(groupByServer.get("node4"), Matchers.<ExecutionNode>containsInAnyOrder(m1));
    }


    @Test
    public void testDetectsHasDirectResponse() throws Exception {
        CollectNode c1 = new CollectNode(1, "c1");
        c1.downstreamNodes(Collections.singletonList("foo"));

        assertThat(ExecutionNodesTask.hasDirectResponse(ImmutableList.<List<ExecutionNode>>of(ImmutableList.<ExecutionNode>of(c1))), is(false));

        CollectNode c2 = new CollectNode(1, "c1");
        c2.downstreamNodes(Collections.singletonList(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE));
        assertThat(ExecutionNodesTask.hasDirectResponse(ImmutableList.<List<ExecutionNode>>of(ImmutableList.<ExecutionNode>of(c2))), is(true));
    }
}

File: sql/src/test/java/io/crate/executor/transport/TransportExecutorDDLTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.ImmutableSet;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.core.collections.Bucket;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.metadata.PartitionName;
import io.crate.metadata.TableIdent;
import io.crate.planner.IterablePlan;
import io.crate.planner.Plan;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.ddl.CreateTableNode;
import io.crate.planner.node.ddl.ESClusterUpdateSettingsNode;
import io.crate.planner.node.ddl.ESCreateTemplateNode;
import io.crate.planner.node.ddl.ESDeletePartitionNode;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.alias.Alias;
import org.elasticsearch.action.admin.indices.exists.indices.IndicesExistsRequest;
import org.elasticsearch.action.admin.indices.template.get.GetIndexTemplatesResponse;
import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
import org.elasticsearch.cluster.settings.DynamicSettings;
import org.elasticsearch.common.inject.Key;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.Arrays;
import java.util.List;
import java.util.Map;

import static io.crate.testing.TestingHelpers.isRow;
import static org.elasticsearch.common.settings.ImmutableSettings.Builder.EMPTY_SETTINGS;
import static org.hamcrest.Matchers.contains;
import static org.hamcrest.Matchers.is;

public class TransportExecutorDDLTest extends SQLTransportIntegrationTest {

    static {
        ClassLoader.getSystemClassLoader().setDefaultAssertionStatus(true);
    }

    private TransportExecutor executor;

    private final static Map<String, Object> TEST_MAPPING = ImmutableMap.<String, Object>of(
            "properties", ImmutableMap.of(
                    "id", ImmutableMap.builder()
                            .put("type", "integer")
                            .put("store", false)
                            .put("index", "not_analyzed")
                            .put("doc_values", true).build(),
                    "name", ImmutableMap.builder()
                            .put("type", "string")
                            .put("store", false)
                            .put("index", "not_analyzed")
                            .put("doc_values", true).build(),
                    "names", ImmutableMap.builder()
                            .put("type", "string")
                            .put("store", false)
                            .put("index", "not_analyzed")
                            .put("doc_values", false).build()
            ));
    private final static Map<String, Object> TEST_PARTITIONED_MAPPING = ImmutableMap.<String, Object>of(
            "_meta", ImmutableMap.of(
                    "partitioned_by", ImmutableList.of(Arrays.asList("name", "string"))
            ),
            "properties", ImmutableMap.of(
                "id", ImmutableMap.builder()
                    .put("type", "integer")
                    .put("store", false)
                    .put("index", "not_analyzed")
                    .put("doc_values", true).build(),
                "names", ImmutableMap.builder()
                    .put("type", "string")
                    .put("store", false)
                    .put("index", "not_analyzed")
                    .put("doc_values", false).build()
            ));
    private final static Settings TEST_SETTINGS = ImmutableSettings.settingsBuilder()
            .put("number_of_replicas", 0)
            .put("number_of_shards", 2).build();

    @Before
    public void transportSetup() {
        executor = internalCluster().getInstance(TransportExecutor.class);
    }

    @Override
    @After
    public void tearDown() throws Exception {
        super.tearDown();
        client().admin().cluster().prepareUpdateSettings()
                .setPersistentSettingsToRemove(ImmutableSet.of("persistent.level"))
                .setTransientSettingsToRemove(ImmutableSet.of("persistent.level", "transient.uptime"))
                .execute().actionGet();
    }

    @Test
    public void testCreateTableTask() throws Exception {
        CreateTableNode createTableNode = CreateTableNode.createTableNode(
                new TableIdent(null, "test"),
                false,
                TEST_SETTINGS,
                TEST_MAPPING
        );
        Plan plan = new IterablePlan(createTableNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket rows = listenableFuture.get().get(0).rows();
        assertThat(rows, contains(isRow(1L)));
        execute("select * from information_schema.tables where table_name = 'test' and number_of_replicas = 0 and number_of_shards = 2");
        assertThat(response.rowCount(), is(1L));

        execute("select count(*) from information_schema.columns where table_name = 'test'");
        assertThat((Long)response.rows()[0][0], is(3L));
    }

    @Test
    public void testCreateTableWithOrphanedPartitions() throws Exception {
        String partitionName = new PartitionName("test", Arrays.asList(new BytesRef("foo"))).stringValue();
        client().admin().indices().prepareCreate(partitionName)
                .addMapping(Constants.DEFAULT_MAPPING_TYPE, TEST_PARTITIONED_MAPPING)
                .setSettings(TEST_SETTINGS)
                .execute().actionGet();
        ensureGreen();
        CreateTableNode createTableNode = CreateTableNode.createTableNode(
                new TableIdent(null, "test"),
                false,
                TEST_SETTINGS,
                TEST_MAPPING
        );
        Plan plan = new IterablePlan(createTableNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(1L)));

        execute("select * from information_schema.tables where table_name = 'test' and number_of_replicas = 0 and number_of_shards = 2");
        assertThat(response.rowCount(), is(1L));

        execute("select count(*) from information_schema.columns where table_name = 'test'");
        assertThat((Long)response.rows()[0][0], is(3L));

        // check that orphaned partition has been deleted
        assertThat(client().admin().indices().exists(new IndicesExistsRequest(partitionName)).actionGet().isExists(), is(false));
    }

    @Test
    public void testCreateTableWithOrphanedAlias() throws Exception {
        String partitionName = new PartitionName("test", Arrays.asList(new BytesRef("foo"))).stringValue();
        client().admin().indices().prepareCreate(partitionName)
                .addMapping(Constants.DEFAULT_MAPPING_TYPE, TEST_PARTITIONED_MAPPING)
                .setSettings(TEST_SETTINGS)
                .addAlias(new Alias("test"))
                .execute().actionGet();
        ensureGreen();
        CreateTableNode createTableNode = CreateTableNode.createTableNode(
                new TableIdent(null, "test"),
                false,
                TEST_SETTINGS,
                TEST_MAPPING
        );
        Plan plan = new IterablePlan(createTableNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(1L)));

        execute("select * from information_schema.tables where table_name = 'test' and number_of_replicas = 0 and number_of_shards = 2");
        assertThat(response.rowCount(), is(1L));

        execute("select count(*) from information_schema.columns where table_name = 'test'");
        assertThat((Long) response.rows()[0][0], is(3L));

        // check that orphaned partition has been deleted
        assertThat(client().admin().cluster().prepareState().execute().actionGet()
                .getState().metaData().aliases().containsKey("test"), is(false));
        // check that orphaned partition has been deleted
        assertThat(client().admin().indices().exists(new IndicesExistsRequest(partitionName)).actionGet().isExists(), is(false));
    }

    @Test
    public void testDeletePartitionTask() throws Exception {
        execute("create table t (id integer primary key, name string) partitioned by (id)");
        ensureYellow();

        execute("insert into t (id, name) values (1, 'Ford')");
        assertThat(response.rowCount(), is(1L));
        ensureYellow();

        execute("select * from information_schema.table_partitions where table_name = 't'");
        assertThat(response.rowCount(), is(1L));

        String partitionName = new PartitionName("t", ImmutableList.of(new BytesRef("1"))).stringValue();
        ESDeletePartitionNode deleteIndexNode = new ESDeletePartitionNode(partitionName);
        Plan plan = new IterablePlan(deleteIndexNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(-1L)));

        execute("select * from information_schema.table_partitions where table_name = 't'");
        assertThat(response.rowCount(), is(0L));
    }

    /**
     * this case should not happen as closed indices aren't listed as TableInfo
     * but if it does maybe because of stale cluster state - validate behaviour here
     *
     * cannot prevent this task from deleting closed indices.
     */
    @Test
    public void testDeletePartitionTaskClosed() throws Exception {
        execute("create table t (id integer primary key, name string) partitioned by (id)");
        ensureYellow();

        execute("insert into t (id, name) values (1, 'Ford')");
        assertThat(response.rowCount(), is(1L));
        ensureYellow();

        String partitionName = new PartitionName("t", ImmutableList.of(new BytesRef("1"))).stringValue();
        assertTrue(client().admin().indices().prepareClose(partitionName).execute().actionGet().isAcknowledged());

        ESDeletePartitionNode deleteIndexNode = new ESDeletePartitionNode(partitionName);
        Plan plan = new IterablePlan(deleteIndexNode);

        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        Bucket objects = listenableFuture.get().get(0).rows();
        assertThat(objects, contains(isRow(-1L)));

        execute("select * from information_schema.table_partitions where table_name = 't'");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testClusterUpdateSettingsTask() throws Exception {
        final String persistentSetting = "persistent.level";
        final String transientSetting = "transient.uptime";

        // allow our settings to be updated (at all nodes)
        Key<DynamicSettings> dynamicSettingsKey = Key.get(DynamicSettings.class, ClusterDynamicSettings.class);
        for (DynamicSettings settings : internalCluster().getInstances(dynamicSettingsKey)) {
            settings.addDynamicSetting(persistentSetting);
            settings.addDynamicSetting(transientSetting);
        }

        // Update persistent only
        Settings persistentSettings = ImmutableSettings.builder()
                .put(persistentSetting, "panic")
                .build();

        ESClusterUpdateSettingsNode node = new ESClusterUpdateSettingsNode(persistentSettings);

        Bucket objects = executePlanNode(node);

        assertThat(objects, contains(isRow(1L)));
        assertEquals("panic", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().persistentSettings().get(persistentSetting));

        // Update transient only
        Settings transientSettings = ImmutableSettings.builder()
                .put(transientSetting, "123")
                .build();

        node = new ESClusterUpdateSettingsNode(EMPTY_SETTINGS, transientSettings);
        objects = executePlanNode(node);

        assertThat(objects, contains(isRow(1L)));
        assertEquals("123", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().transientSettings().get(transientSetting));

        // Update persistent & transient
        persistentSettings = ImmutableSettings.builder()
                .put(persistentSetting, "normal")
                .build();
        transientSettings = ImmutableSettings.builder()
                .put(transientSetting, "243")
                .build();

        node = new ESClusterUpdateSettingsNode(persistentSettings, transientSettings);
        objects = executePlanNode(node);

        assertThat(objects, contains(isRow(1L)));
        assertEquals("normal", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().persistentSettings().get(persistentSetting));
        assertEquals("243", client().admin().cluster().prepareState().execute().actionGet().getState().metaData().transientSettings().get(transientSetting));
    }

    private Bucket executePlanNode(PlanNode node) throws InterruptedException, java.util.concurrent.ExecutionException {
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> futures = executor.execute(job);
        ListenableFuture<List<TaskResult>> listenableFuture = Futures.allAsList(futures);
        return listenableFuture.get().get(0).rows();
    }

    @Test
    public void testCreateIndexTemplateTask() throws Exception {
        Settings indexSettings = ImmutableSettings.builder()
                .put("number_of_replicas", 0)
                .put("number_of_shards", 2)
                .build();
        Map<String, Object> mapping = ImmutableMap.<String, Object>of(
                "properties", ImmutableMap.of(
                        "id", ImmutableMap.builder()
                                .put("type", "integer")
                                .put("store", false)
                                .put("index", "not_analyzed")
                                .put("doc_values", true).build(),
                        "name", ImmutableMap.builder()
                                .put("type", "string")
                                .put("store", false)
                                .put("index", "not_analyzed")
                                .put("doc_values", true).build(),
                        "names", ImmutableMap.builder()
                                .put("type", "string")
                                .put("store", false)
                                .put("index", "not_analyzed")
                                .put("doc_values", false).build()
                ),
                "_meta", ImmutableMap.of(
                        "partitioned_by", ImmutableList.<List<String>>of(
                                ImmutableList.of("name", "string")
                        )
                )
        );
        String templateName = PartitionName.templateName(null, "partitioned");
        String templatePrefix = PartitionName.templateName(null, "partitioned") + "*";
        final String alias = "aliasName";

        ESCreateTemplateNode planNode = new ESCreateTemplateNode(
                templateName,
                templatePrefix,
                indexSettings,
                mapping,
                alias);

        Bucket objects = executePlanNode(planNode);
        assertThat(objects, contains(isRow(1L)));

        refresh();

        GetIndexTemplatesResponse response = client().admin().indices()
                .prepareGetTemplates(".partitioned.partitioned.").execute().actionGet();

        assertThat(response.getIndexTemplates().size(), is(1));
        IndexTemplateMetaData templateMeta = response.getIndexTemplates().get(0);
        assertThat(templateMeta.getName(), is(".partitioned.partitioned."));
        assertThat(templateMeta.mappings().get(Constants.DEFAULT_MAPPING_TYPE).string(),
                is("{\"default\":" +
                        "{\"properties\":{" +
                        "\"id\":{\"type\":\"integer\",\"store\":false,\"index\":\"not_analyzed\",\"doc_values\":true}," +
                        "\"name\":{\"type\":\"string\",\"store\":false,\"index\":\"not_analyzed\",\"doc_values\":true}," +
                        "\"names\":{\"type\":\"string\",\"store\":false,\"index\":\"not_analyzed\",\"doc_values\":false}" +
                        "}," +
                        "\"_meta\":{" +
                        "\"partitioned_by\":[[\"name\",\"string\"]]" +
                        "}}}"));
        assertThat(templateMeta.template(), is(".partitioned.partitioned.*"));
        assertThat(templateMeta.settings().toDelimitedString(','),
                is("index.number_of_replicas=0,index.number_of_shards=2,"));
        assertThat(templateMeta.aliases().get(alias).alias(), is(alias));
    }
}


File: sql/src/test/java/io/crate/executor/transport/TransportExecutorTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.executor.Job;
import io.crate.executor.RowCountResult;
import io.crate.executor.Task;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.ESDeleteByQueryTask;
import io.crate.executor.transport.task.elasticsearch.ESGetTask;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.doc.DocTableInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.projectors.TopN;
import io.crate.operation.scalar.DateTruncFunction;
import io.crate.planner.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.*;
import io.crate.planner.symbol.*;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.MapBuilder;
import org.elasticsearch.search.SearchHits;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import java.util.*;

import static io.crate.testing.TestingHelpers.isRow;
import static java.util.Arrays.asList;
import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

public class TransportExecutorTest extends BaseTransportExecutorTest {

    @Rule
    public ExpectedException expectedException = ExpectedException.none();

    @Before
    public void setup() {
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, String singleStringKey, int executionNodeId) {
        return newGetNode(tableName, outputs, Collections.singletonList(singleStringKey), executionNodeId);
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        return newGetNode(docSchemaInfo.getTableInfo(tableName), outputs, singleStringKeys, executionNodeId);
    }

    @Test
    public void testESGetTask() throws Exception {
        setup.setUpCharacters();

        // create plan
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);

        // validate tasks
        assertThat(job.tasks().size(), is(1));
        Task task = job.tasks().get(0);
        assertThat(task, instanceOf(ESGetTask.class));

        // execute and validate results
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    @Test
    public void testESGetTaskWithDynamicReference() throws Exception {
        setup.setUpCharacters();

        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, new DynamicReference(
                new ReferenceIdent(new TableIdent(null, "characters"), "foo"), RowGranularity.DOC));
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, null)));
    }

    @Test
    public void testESMultiGet() throws Exception {
        setup.setUpCharacters();
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, asList("1", "2"), ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects.size(), is(2));
    }

    @Test
    public void testQTFTask() throws Exception {
        // select id, name from characters;
        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection((DocTableInfo) characters, (List<Symbol>) collectSymbols, (List<Symbol>) outputSymbols, (CollectNode) collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFilter() throws Exception {
        // select id, name from characters where name = 'Ford';
        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(nameRef, Literal.newLiteral("Ford")));

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    private FetchProjection getFetchProjection(DocTableInfo characters, List<Symbol> collectSymbols, List<Symbol> outputSymbols, CollectNode collectNode, Planner.Context ctx) {
        return new FetchProjection(
                collectNode.executionNodeId(),
                new InputColumn(0, DataTypes.STRING), collectSymbols, outputSymbols,
                characters.partitionedByColumns(),
                collectNode.executionNodes(),
                5,
                false,
                ctx.jobSearchContextIdToNode(),
                ctx.jobSearchContextIdToShard()
        );
    }

    @Test
    public void testQTFTaskOrdered() throws Exception {
        // select id, name from characters order by name, female;
        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");

        OrderBy orderBy = new OrderBy(Arrays.<Symbol>asList(nameRef, femaleRef),
                new boolean[]{false, false},
                new Boolean[]{false, false});

        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        // add nameRef and femaleRef to collectSymbols because this are ordered by values
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo), nameRef, femaleRef);
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        MergeProjection mergeProjection = new MergeProjection(
                collectSymbols,
                orderBy.orderBySymbols(),
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                orderBy,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.projections(ImmutableList.<Projection>of(mergeProjection));
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                ImmutableList.<Projection>of(fetchProjection),
                orderBy,
                collectSymbols,
                null,
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFunction() throws Exception {
        // select id, date_trunc('day', date) from searchf where id = 2;
        execute("create table searchf (id int primary key, date timestamp) with (number_of_replicas=0)");
        ensureGreen();
        execute("insert into searchf (id, date) values (1, '1980-01-01'), (2, '1980-01-02')");
        refresh();

        Reference id_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "id"),
                RowGranularity.DOC,
                DataTypes.INTEGER
        ));
        Reference date_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "date"),
                RowGranularity.DOC,
                DataTypes.TIMESTAMP
        ));
        Function function = new Function(new FunctionInfo(
                new FunctionIdent(DateTruncFunction.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.TIMESTAMP)),
                DataTypes.TIMESTAMP
        ), Arrays.asList(Literal.newLiteral("month"), date_ref));
        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.INTEGER, DataTypes.INTEGER)),
                DataTypes.BOOLEAN),
                Arrays.asList(id_ref, Literal.newLiteral(2))
        );

        DocTableInfo searchf = docSchemaInfo.getTableInfo("searchf");
        ReferenceInfo docIdRefInfo = searchf.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        Planner.Context ctx = new Planner.Context(clusterService());
        List<Symbol> collectSymbols = ImmutableList.<Symbol>of(new Reference(docIdRefInfo));
        CollectNode collectNode = PlanNodeBuilder.collect(
                searchf,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        TopNProjection topN = new TopNProjection(2, TopN.NO_OFFSET);
        topN.outputs(Collections.<Symbol>singletonList(new InputColumn(0)));

        FetchProjection fetchProjection = getFetchProjection(searchf, collectSymbols, Arrays.asList(id_ref, function), collectNode, ctx);

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.of(topN, fetchProjection),
                collectNode,
                ctx);
        Plan plan = new QueryThenFetch(collectNode, mergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, 315532800000L)));
    }

    @Test
    public void testQTFTaskPartitioned() throws Exception {
        setup.setUpPartitionedTableWithName();
        DocTableInfo parted = docSchemaInfo.getTableInfo("parted");
        Planner.Context ctx = new Planner.Context(clusterService());

        ReferenceInfo docIdRefInfo = parted.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols =  Arrays.<Symbol>asList(partedIdRef, partedNameRef, partedDateRef);

        CollectNode collectNode = PlanNodeBuilder.collect(
                parted,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(parted, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(3, "Ford", 1396388720242L),
                isRow(1, "Trillian", null),
                isRow(2, null, 0L)
        ));
    }

    @Test
    public void testESDeleteByQueryTask() throws Exception {
        setup.setUpCharacters();

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(idRef, Literal.newLiteral(2)));

        ESDeleteByQueryNode node = new ESDeleteByQueryNode(
                1,
                ImmutableList.of(new String[]{"characters"}),
                ImmutableList.of(new WhereClause(whereClause)));
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        ESDeleteByQueryTask task = (ESDeleteByQueryTask) job.tasks().get(0);

        task.start();
        TaskResult taskResult = task.result().get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(-1L)));

        // verify deletion
        execute("select * from characters where id = 2");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testInsertWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();

        /* insert into characters (id, name) values (99, 'Marvin'); */
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});
        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));

        // verify insertion
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "99", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(99, "Marvin")));
    }

    @Test
    public void testInsertIntoPartitionedTableWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table parted (" +
                "  id int, " +
                "  name string, " +
                "  date timestamp" +
                ") partitioned by (date)");
        ensureGreen();

        /* insert into parted (id, name, date) values(0, 'Trillian', 13959981214861); */
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                true,
                false,
                null,
                new Reference[]{idRef, nameRef});

        PartitionName partitionName = new PartitionName("parted", Arrays.asList(new BytesRef("13959981214861")));
        updateNode.add(partitionName.stringValue(), "123", "123", null, null, new Object[]{0L, new BytesRef("Trillian")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket indexResult = taskResult.rows();
        assertThat(indexResult, contains(isRow(1L)));

        refresh();

        assertTrue(
                client().admin().indices().prepareExists(partitionName.stringValue())
                        .execute().actionGet().isExists()
        );
        assertTrue(
                client().admin().indices().prepareAliasesExist("parted")
                        .execute().actionGet().exists()
        );
        SearchHits hits = client().prepareSearch(partitionName.stringValue())
                .setTypes(Constants.DEFAULT_MAPPING_TYPE)
                .addFields("id", "name")
                .setQuery(new MapBuilder<String, Object>()
                                .put("match_all", new HashMap<String, Object>())
                                .map()
                ).execute().actionGet().getHits();
        assertThat(hits.getTotalHits(), is(1L));
        assertThat((Integer) hits.getHits()[0].field("id").getValues().get(0), is(0));
        assertThat((String) hits.getHits()[0].field("name").getValues().get(0), is("Trillian"));
    }

    @Test
    public void testInsertMultiValuesWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();

        /* insert into characters (id, name) values (99, 'Marvin'), (42, 'Deep Thought'); */
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});

        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});
        updateNode.add("characters", "42", "42", null, null, new Object[]{42, new BytesRef("Deep Thought")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(2L)));

        // verify insertion
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, Arrays.asList("99", "42"), ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(
                isRow(99, "Marvin"),
                isRow(42, "Deep Thought")
        ));
    }

    @Test
    public void testUpdateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        // update characters set name='Vogon lyric fan' where id=1
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(), false, false, new String[]{nameRef.ident().columnIdent().fqn()}, null);
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral("Vogon lyric fan")}, null);
        Plan plan = new IterablePlan(updateNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));

        // verify update
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Vogon lyric fan")));
    }

    @Test
    public void testInsertOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();
        /* insert into characters (id, name, female) values (5, 'Zaphod Beeblebrox', false)
           on duplicate key update set name = 'Zaphod Beeblebrox'; */
        Object[] missingAssignments = new Object[]{5, new BytesRef("Zaphod Beeblebrox"), false};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{nameRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});

        updateNode.add("characters", "5", "5", new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));

        // verify insert
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "5", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();
        assertThat(objects, contains(isRow(5, "Zaphod Beeblebrox", false)));

    }

    @Test
    public void testUpdateOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();
        /* insert into characters (id, name, female) values (1, 'Zaphod Beeblebrox', false)
           on duplicate key update set name = 'Zaphod Beeblebrox'; */
        Object[] missingAssignments = new Object[]{1, new BytesRef("Zaphod Beeblebrox"), true};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{femaleRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral(true)}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));

        // verify update
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Arthur", true)));
    }

    @Test
    public void testBulkUpdateByQueryTask() throws Exception {
        setup.setUpCharacters();
        /* update characters set name 'Zaphod Beeblebrox' where female = false
           update characters set name 'Zaphod Beeblebrox' where female = true
         */

        List<Plan> childNodes = new ArrayList<>();
        Planner.Context plannerContext = new Planner.Context(clusterService());

        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");
        Reference uidReference = new Reference(
                new ReferenceInfo(
                        new ReferenceIdent(tableInfo.ident(), "_uid"),
                        RowGranularity.DOC, DataTypes.STRING));

        // 1st collect and merge nodes
        Function whereClause1 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        UpdateProjection updateProjection = new UpdateProjection(
                new InputColumn(0, DataTypes.STRING),
                new String[]{"name"},
                new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")},
                null);

        CollectNode collectNode1 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause1),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode1 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode1,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode1, mergeNode1));

        // 2nd collect and merge nodes
        Function whereClause2 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        CollectNode collectNode2 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause2),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode2 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode2,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode2, mergeNode2));

        Upsert plan = new Upsert(childNodes);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        assertThat(job.tasks().get(0), instanceOf(ExecutionNodesTask.class));
        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results.size(), is(2));

        for (int i = 0; i < results.size(); i++) {
            TaskResult result = results.get(i).get();
            assertThat(result, instanceOf(RowCountResult.class));
            // each of the bulk request hits 2 records
            assertThat(((RowCountResult)result).rowCount(), is(2L));
        }
    }

    @Test
    public void testKillTask() throws Exception {
        Job job = executor.newJob(KillPlan.INSTANCE);
        assertThat(job.tasks(), hasSize(1));
        assertThat(job.tasks().get(0), instanceOf(KillTask.class));

        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results, hasSize(1));
        results.get(0).get();
    }
}


File: sql/src/test/java/io/crate/integrationtests/FetchOperationIntegrationTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.integrationtests;

import com.carrotsearch.hppc.LongArrayList;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.analyze.Analysis;
import io.crate.analyze.Analyzer;
import io.crate.analyze.ParameterContext;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import io.crate.executor.Job;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.NodeFetchRequest;
import io.crate.executor.transport.NodeFetchResponse;
import io.crate.executor.transport.TransportExecutor;
import io.crate.executor.transport.TransportFetchNodeAction;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.fetch.RowInputSymbolVisitor;
import io.crate.planner.Plan;
import io.crate.planner.Planner;
import io.crate.planner.RowGranularity;
import io.crate.planner.consumer.ConsumerContext;
import io.crate.planner.consumer.QueryThenFetchConsumer;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.QueryThenFetch;
import io.crate.planner.projection.FetchProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.sql.parser.SqlParser;
import io.crate.types.DataType;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.CountDownLatch;

import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

@ElasticsearchIntegrationTest.ClusterScope(numDataNodes = 2, numClientNodes = 0)
public class FetchOperationIntegrationTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);
    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }

    private void setUpCharacters() {
        sqlExecutor.exec("create table characters (id int primary key, name string) " +
                "clustered into 2 shards with(number_of_replicas=0)");
        sqlExecutor.ensureYellowOrGreen();
        sqlExecutor.exec("insert into characters (id, name) values (?, ?)",
                new Object[][]{
                        new Object[]{1, "Arthur"},
                        new Object[]{2, "Ford"},
                }
        );
        sqlExecutor.refresh("characters");
    }

    private Plan analyzeAndPlan(String stmt) {
        Analysis analysis = analyze(stmt);
        Planner planner = internalCluster().getInstance(Planner.class);
        return planner.plan(analysis);
    }

    private Analysis analyze(String stmt) {
        Analyzer analyzer = internalCluster().getInstance(Analyzer.class);
        return analyzer.analyze(
                SqlParser.createStatement(stmt),
                new ParameterContext(new Object[0], new Object[0][], null)
        );
    }

    private CollectNode createCollectNode(Planner.Context plannerContext, boolean keepContextForFetcher) {
        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");

        ReferenceInfo docIdRefInfo = tableInfo.getReferenceInfo(new ColumnIdent("_docid"));
        Symbol docIdRef = new Reference(docIdRefInfo);
        List<Symbol> toCollect = ImmutableList.of(docIdRef);

        List<DataType> outputTypes = new ArrayList<>(toCollect.size());
        for (Symbol symbol : toCollect) {
            outputTypes.add(symbol.valueType());
        }
        CollectNode collectNode = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "collect",
                tableInfo.getRouting(WhereClause.MATCH_ALL, null));
        collectNode.toCollect(toCollect);
        collectNode.outputTypes(outputTypes);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.keepContextForFetcher(keepContextForFetcher);
        collectNode.jobId(UUID.randomUUID());
        plannerContext.allocateJobSearchContextIds(collectNode.routing());

        return collectNode;
    }

    private List<Bucket> getBuckets(CollectNode collectNode) throws InterruptedException, java.util.concurrent.ExecutionException {
        List<Bucket> results = new ArrayList<>();
        for (String nodeName : internalCluster().getNodeNames()) {
            ContextPreparer contextPreparer = internalCluster().getInstance(ContextPreparer.class, nodeName);
            JobContextService contextService = internalCluster().getInstance(JobContextService.class, nodeName);

            JobExecutionContext.Builder builder = contextService.newBuilder(collectNode.jobId());
            ListenableFuture<Bucket> future = contextPreparer.prepare(collectNode.jobId(), collectNode, builder);
            assert future != null;

            JobExecutionContext context = contextService.createContext(builder);
            context.start();
            results.add(future.get());
        }
        return results;
    }

    @Test
    public void testCollectDocId() throws Exception {
        setUpCharacters();
        Planner.Context plannerContext = new Planner.Context(clusterService());
        CollectNode collectNode = createCollectNode(plannerContext, false);

        List<Bucket> results = getBuckets(collectNode);

        assertThat(results.size(), is(2));
        int seenJobSearchContextId = -1;
        for (Bucket rows : results) {
            assertThat(rows.size(), is(1));
            Object docIdCol = rows.iterator().next().get(0);
            assertNotNull(docIdCol);
            assertThat(docIdCol, instanceOf(Long.class));
            long docId = (long)docIdCol;
            // unpack jobSearchContextId and reader doc id from docId
            int jobSearchContextId = (int)(docId >> 32);
            int doc = (int)docId;
            assertThat(doc, is(0));
            assertThat(jobSearchContextId, greaterThan(-1));
            if (seenJobSearchContextId == -1) {
                assertThat(jobSearchContextId, anyOf(is(0), is(1)));
                seenJobSearchContextId = jobSearchContextId;
            } else {
                assertThat(jobSearchContextId, is(seenJobSearchContextId == 0 ? 1 : 0));
            }
        }
    }

    @Test
    public void testFetchAction() throws Exception {
        setUpCharacters();

        Analysis analysis = analyze("select id, name from characters");
        QueryThenFetchConsumer queryThenFetchConsumer = internalCluster().getInstance(QueryThenFetchConsumer.class);
        Planner.Context plannerContext = new Planner.Context(clusterService());
        ConsumerContext consumerContext = new ConsumerContext(analysis.rootRelation(), plannerContext);
        queryThenFetchConsumer.consume(analysis.rootRelation(), consumerContext);

        QueryThenFetch plan = ((QueryThenFetch) ((PlannedAnalyzedRelation) consumerContext.rootRelation()).plan());
        UUID jobId = UUID.randomUUID();
        plan.collectNode().jobId(jobId);

        List<Bucket> results = getBuckets(plan.collectNode());


        TransportFetchNodeAction transportFetchNodeAction = internalCluster().getInstance(TransportFetchNodeAction.class);

        // extract docIds by nodeId and jobSearchContextId
        Map<String, LongArrayList> jobSearchContextDocIds = new HashMap<>();
        for (Bucket rows : results) {
            long docId = (long)rows.iterator().next().get(0);
            // unpack jobSearchContextId and reader doc id from docId
            int jobSearchContextId = (int)(docId >> 32);
            String nodeId = plannerContext.nodeId(jobSearchContextId);
            LongArrayList docIdsPerNode = jobSearchContextDocIds.get(nodeId);
            if (docIdsPerNode == null) {
                docIdsPerNode = new LongArrayList();
                jobSearchContextDocIds.put(nodeId, docIdsPerNode);
            }
            docIdsPerNode.add(docId);
        }

        Iterable<Projection> projections = Iterables.filter(plan.mergeNode().projections(), Predicates.instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection )Iterables.getOnlyElement(projections);
        RowInputSymbolVisitor rowInputSymbolVisitor = new RowInputSymbolVisitor(internalCluster().getInstance(Functions.class));
        RowInputSymbolVisitor.Context context = rowInputSymbolVisitor.extractImplementations(fetchProjection.outputSymbols());

        final CountDownLatch latch = new CountDownLatch(jobSearchContextDocIds.size());
        final List<Row> rows = new ArrayList<>();
        for (Map.Entry<String, LongArrayList> nodeEntry : jobSearchContextDocIds.entrySet()) {
            NodeFetchRequest nodeFetchRequest = new NodeFetchRequest();
            nodeFetchRequest.jobId(plan.collectNode().jobId());
            nodeFetchRequest.executionNodeId(plan.collectNode().executionNodeId());
            nodeFetchRequest.toFetchReferences(context.references());
            nodeFetchRequest.closeContext(true);
            nodeFetchRequest.jobSearchContextDocIds(nodeEntry.getValue());

            transportFetchNodeAction.execute(nodeEntry.getKey(), nodeFetchRequest, new ActionListener<NodeFetchResponse>() {
                @Override
                public void onResponse(NodeFetchResponse nodeFetchResponse) {
                    for (Row row : nodeFetchResponse.rows()) {
                        rows.add(row);
                    }
                    latch.countDown();
                }

                @Override
                public void onFailure(Throwable e) {
                    latch.countDown();
                    fail(e.getMessage());
                }
            });
        }
        latch.await();

        assertThat(rows.size(), is(2));
        for (Row row : rows) {
            assertThat((Integer) row.get(0), anyOf(is(1), is(2)));
            assertThat((BytesRef) row.get(1), anyOf(is(new BytesRef("Arthur")), is(new BytesRef("Ford"))));
        }
    }

    @Test
    public void testFetchProjection() throws Exception {
        setUpCharacters();

        Plan plan = analyzeAndPlan("select id, name, substr(name, 2) from characters order by id");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        QueryThenFetch qtf = (QueryThenFetch) plan;

        assertThat(qtf.collectNode().keepContextForFetcher(), is(true));
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToNode(), notNullValue());
        assertThat(((FetchProjection) qtf.mergeNode().projections().get(1)).jobSearchContextIdToShard(), notNullValue());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });

        latch.await();
        assertThat(resultingRows.size(), is(2));
        assertThat(resultingRows.get(0).length, is(3));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((BytesRef) resultingRows.get(0)[1], is(new BytesRef("Arthur")));
        assertThat((BytesRef) resultingRows.get(0)[2], is(new BytesRef("rthur")));
        assertThat((Integer) resultingRows.get(1)[0], is(2));
        assertThat((BytesRef) resultingRows.get(1)[1], is(new BytesRef("Ford")));
        assertThat((BytesRef) resultingRows.get(1)[2], is(new BytesRef("ord")));
    }

    @Test
    public void testFetchProjectionWithBulkSize() throws Exception {
        /**
         * Setup scenario where more docs per node exists than the configured bulkSize,
         * so multiple request to one node must be done and merged together.
         */
        setup.setUpLocations();
        sqlExecutor.refresh("locations");
        int bulkSize = 2;

        Plan plan = analyzeAndPlan("select position, name from locations order by position");
        assertThat(plan, instanceOf(QueryThenFetch.class));

        rewriteFetchProjectionToBulkSize(bulkSize, ((QueryThenFetch) plan).mergeNode());

        Job job = executor.newJob(plan);
        ListenableFuture<List<TaskResult>> results = Futures.allAsList(executor.execute(job));

        final List<Object[]> resultingRows = new ArrayList<>();
        final CountDownLatch latch = new CountDownLatch(1);
        Futures.addCallback(results, new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(List<TaskResult> resultList) {
                for (Row row : resultList.get(0).rows()) {
                    resultingRows.add(row.materialize());
                }
                latch.countDown();
            }

            @Override
            public void onFailure(Throwable t) {
                latch.countDown();
                fail(t.getMessage());
            }
        });
        latch.await();

        assertThat(resultingRows.size(), is(13));
        assertThat(resultingRows.get(0).length, is(2));
        assertThat((Integer) resultingRows.get(0)[0], is(1));
        assertThat((Integer) resultingRows.get(12)[0], is(6));
    }

    private void rewriteFetchProjectionToBulkSize(int bulkSize, MergeNode mergeNode) {
        List<Projection> newProjections = new ArrayList<>(mergeNode.projections().size());
        for (Projection projection : mergeNode.projections()) {
            if (projection instanceof FetchProjection) {
                FetchProjection fetchProjection = (FetchProjection) projection;
                newProjections.add(new FetchProjection(
                        fetchProjection.executionNodeId(),
                        fetchProjection.docIdSymbol(),
                        fetchProjection.inputSymbols(),
                        fetchProjection.outputSymbols(),
                        fetchProjection.partitionedBy(),
                        fetchProjection.executionNodes(),
                        bulkSize,
                        fetchProjection.closeContexts(),
                        fetchProjection.jobSearchContextIdToNode(),
                        fetchProjection.jobSearchContextIdToShard()));
            } else {
                newProjections.add(projection);
            }
        }
        mergeNode.projections(newProjections);
    }
}

File: sql/src/test/java/io/crate/operation/PageDownstreamFactoryTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.operation;

import com.google.common.base.Optional;
import com.google.common.collect.Iterators;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.SettableFuture;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.ArrayBucket;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.BucketPage;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.jobs.ExecutionState;
import io.crate.metadata.*;
import io.crate.operation.aggregation.impl.AggregationImplModule;
import io.crate.operation.aggregation.impl.MinimumAggregation;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.testing.CollectingProjector;
import io.crate.operation.projectors.TopN;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.client.Client;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.breaker.NoopCircuitBreaker;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.AbstractModule;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.threadpool.ThreadPool;
import org.hamcrest.collection.IsIterableContainingInOrder;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Answers;

import java.util.Arrays;
import java.util.Collections;
import java.util.Iterator;
import java.util.concurrent.Executor;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;
import static org.mockito.Mockito.mock;

public class PageDownstreamFactoryTest extends CrateUnitTest {

    private static final RamAccountingContext ramAccountingContext =
            new RamAccountingContext("dummy", new NoopCircuitBreaker(CircuitBreaker.Name.FIELDDATA));

    private GroupProjection groupProjection;
    private Functions functions;
    private ReferenceResolver referenceResolver;
    private ThreadPool threadPool;

    @Before
    @SuppressWarnings("unchecked")
    public void prepare() {
        Injector injector = new ModulesBuilder()
                .add(new AggregationImplModule())
                .add(new AbstractModule() {
                    @Override
                    protected void configure() {
                        bind(Client.class).toInstance(mock(Client.class));
                    }
                })
                .createInjector();
        threadPool = new ThreadPool("testing");
        functions = injector.getInstance(Functions.class);
        referenceResolver = new GlobalReferenceResolver(
                Collections.<ReferenceIdent, ReferenceImplementation>emptyMap());

        FunctionIdent minAggIdent = new FunctionIdent(MinimumAggregation.NAME, Arrays.<DataType>asList(DataTypes.DOUBLE));
        FunctionInfo minAggInfo = new FunctionInfo(minAggIdent, DataTypes.DOUBLE);

        groupProjection = new GroupProjection();
        groupProjection.keys(Arrays.<Symbol>asList(new InputColumn(0, DataTypes.INTEGER)));
        groupProjection.values(Arrays.asList(
                new Aggregation(minAggInfo, Arrays.<Symbol>asList(new InputColumn(1)),
                        Aggregation.Step.PARTIAL, Aggregation.Step.FINAL)
        ));
    }

    @Override
    @After
    public void tearDown() throws Exception {
        super.tearDown();
        threadPool.shutdown();
        threadPool.awaitTermination(1, TimeUnit.SECONDS);
    }

    @Test
    public void testMergeSingleResult() throws Exception {
        TopNProjection topNProjection = new TopNProjection(3, TopN.NO_OFFSET,
                Arrays.<Symbol>asList(new InputColumn(0)), new boolean[]{false}, new Boolean[]{null});
        topNProjection.outputs(Arrays.<Symbol>asList(new InputColumn(0), new InputColumn(1)));

        MergeNode mergeNode = new MergeNode(0, "merge", 2); // no need for inputTypes here
        mergeNode.projections(Arrays.asList(
                groupProjection,
                topNProjection
        ));

        Object[][] objs = new Object[20][];
        for (int i = 0; i < objs.length; i++) {
            objs[i] = new Object[]{i % 4, i + 0.5d};
        }
        Bucket rows = new ArrayBucket(objs);
        BucketPage page = new BucketPage(Futures.immediateFuture(rows));
        final PageDownstreamFactory pageDownstreamFactory = new PageDownstreamFactory(
                mock(ClusterService.class),
                threadPool,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                mock(BulkRetryCoordinatorPool.class),
                referenceResolver,
                functions
        );
        CollectingProjector collectingProjector = new CollectingProjector();
        final PageDownstream pageDownstream = getPageDownstream(mergeNode, pageDownstreamFactory, collectingProjector);
        final SettableFuture<?> future = SettableFuture.create();
        pageDownstream.nextPage(page, new PageConsumeListener() {
            @Override
            public void needMore() {
                pageDownstream.finish();
                future.set(null);
            }

            @Override
            public void finish() {
                fail("operation should want more");
            }
        });
        future.get();
        Bucket mergeResult = collectingProjector.result().get();
        assertThat(mergeResult, IsIterableContainingInOrder.contains(
                isRow(0, 0.5d),
                isRow(1, 1.5d),
                isRow(2, 2.5d)
        ));
    }

    private PageDownstream getPageDownstream(MergeNode mergeNode, PageDownstreamFactory pageDownstreamFactory, CollectingProjector collectingProjector) {
        Tuple<PageDownstream, FlatProjectorChain> downstreamFlatProjectorChainTuple =
                pageDownstreamFactory.createMergeNodePageDownstream(
                        mergeNode, collectingProjector, ramAccountingContext, Optional.<Executor>absent());
        downstreamFlatProjectorChainTuple.v2().startProjections(mock(ExecutionState.class));
        return downstreamFlatProjectorChainTuple.v1();
    }

    @Test
    public void testMergeMultipleResults() throws Exception {
        MergeNode mergeNode = new MergeNode(0, "merge", 2); // no need for inputTypes here
        mergeNode.projections(Arrays.<Projection>asList(
                groupProjection
        ));
        final PageDownstreamFactory pageDownstreamFactory = new PageDownstreamFactory(
                mock(ClusterService.class),
                threadPool,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                mock(BulkRetryCoordinatorPool.class),
                referenceResolver,
                functions
        );
        CollectingProjector collectingProjector = new CollectingProjector();
        final PageDownstream pageDownstream = getPageDownstream(mergeNode, pageDownstreamFactory, collectingProjector);

        Bucket rows = new ArrayBucket(new Object[][]{{0, 100.0d}});
        BucketPage page1 = new BucketPage(Futures.immediateFuture(rows));
        Bucket otherRows = new ArrayBucket(new Object[][]{{0, 2.5d}});
        BucketPage page2 = new BucketPage(Futures.immediateFuture(otherRows));
        final Iterator<BucketPage> iterator = Iterators.forArray(page1, page2);

        final SettableFuture<?> future = SettableFuture.create();
        pageDownstream.nextPage(iterator.next(), new PageConsumeListener() {
            @Override
            public void needMore() {
                if (iterator.hasNext()) {
                    pageDownstream.nextPage(iterator.next(), this);
                } else {
                    pageDownstream.finish();
                    future.set(null);
                }
            }

            @Override
            public void finish() {
                fail("should still want more");
            }
        });
        future.get();
        Bucket mergeResult = collectingProjector.result().get();
        assertThat(mergeResult, contains(isRow(0, 2.5)));
    }

}


File: sql/src/test/java/io/crate/operation/collect/DocLevelCollectTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.operation.operator.EqOperator;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.routing.ShardRouting;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;
import static org.hamcrest.Matchers.containsInAnyOrder;


@ElasticsearchIntegrationTest.ClusterScope(randomDynamicTemplates = false, numDataNodes = 1)
public class DocLevelCollectTest extends SQLTransportIntegrationTest {

    private static final String TEST_TABLE_NAME = "test_table";
    private static final Reference testDocLevelReference = new Reference(
            new ReferenceInfo(
                    new ReferenceIdent(new TableIdent(null, TEST_TABLE_NAME), "doc"),
                    RowGranularity.DOC,
                    DataTypes.INTEGER
            )
    );
    private static final Reference underscoreIdReference = new Reference(
            new ReferenceInfo(
                    new ReferenceIdent(new TableIdent(null, TEST_TABLE_NAME), "_id"),
                    RowGranularity.DOC,
                    DataTypes.STRING
            )
    );
    private static final Reference underscoreRawReference = new Reference(
            new ReferenceInfo(
                    new ReferenceIdent(new TableIdent(null, TEST_TABLE_NAME), "_raw"),
                    RowGranularity.DOC,
                    DataTypes.STRING
            )
    );

    private static final String PARTITIONED_TABLE_NAME = "parted_table";

    private MapSideDataCollectOperation operation;
    private Functions functions;
    private DocSchemaInfo docSchemaInfo;

    @Before
    public void prepare() {
        operation = internalCluster().getDataNodeInstance(MapSideDataCollectOperation.class);
        functions = internalCluster().getDataNodeInstance(Functions.class);
        docSchemaInfo = internalCluster().getDataNodeInstance(DocSchemaInfo.class);

        execute(String.format(Locale.ENGLISH, "create table %s (" +
                "  id integer," +
                "  name string," +
                "  date timestamp" +
                ") clustered into 2 shards partitioned by (date) with(number_of_replicas=0)", PARTITIONED_TABLE_NAME));
        ensureGreen();
        execute(String.format("insert into %s (id, name, date) values (?, ?, ?)",
                PARTITIONED_TABLE_NAME),
                new Object[]{1, "Ford", 0L});
        execute(String.format("insert into %s (id, name, date) values (?, ?, ?)",
                PARTITIONED_TABLE_NAME),
                new Object[]{2, "Trillian", 1L});
        ensureGreen();
        refresh();

        execute(String.format(Locale.ENGLISH, "create table %s (" +
                " id integer primary key," +
                " doc integer" +
                ") clustered into 2 shards with(number_of_replicas=0)", TEST_TABLE_NAME));
        ensureGreen();
        execute(String.format("insert into %s (id, doc) values (?, ?)", TEST_TABLE_NAME), new Object[]{1, 2});
        execute(String.format("insert into %s (id, doc) values (?, ?)", TEST_TABLE_NAME), new Object[]{3, 4});
        refresh();
    }

    @After
    public void cleanUp() {
        operation = null;
        functions = null;
        docSchemaInfo = null;
    }

    private Routing routing(String table) {
        Map<String, Map<String, List<Integer>>> locations = new TreeMap<>();

        for (final ShardRouting shardRouting : clusterService().state().routingTable().allShards(table)) {
            Map<String, List<Integer>> shardIds = locations.get(shardRouting.currentNodeId());
            if (shardIds == null) {
                shardIds = new TreeMap<>();
                locations.put(shardRouting.currentNodeId(), shardIds);
            }

            List<Integer> shardIdSet = shardIds.get(shardRouting.index());
            if (shardIdSet == null) {
                shardIdSet = new ArrayList<>();
                shardIds.put(shardRouting.index(), shardIdSet);
            }
            shardIdSet.add(shardRouting.id());
        }
        return new Routing(locations);
    }

    @Test
    public void testCollectDocLevel() throws Exception {
        CollectNode collectNode = new CollectNode(0, "docCollect", routing(TEST_TABLE_NAME));
        collectNode.toCollect(Arrays.<Symbol>asList(testDocLevelReference, underscoreRawReference, underscoreIdReference));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.jobId(UUID.randomUUID());
        PlanNodeBuilder.setOutputTypes(collectNode);
        Bucket result = collect(collectNode);
        assertThat(result, containsInAnyOrder(
                isRow(2, "{\"id\":1,\"doc\":2}", "1"),
                isRow(4, "{\"id\":3,\"doc\":4}", "3")
        ));
    }

    @Test
    public void testCollectDocLevelWhereClause() throws Exception {
        EqOperator op = (EqOperator) functions.get(new FunctionIdent(EqOperator.NAME,
                ImmutableList.<DataType>of(DataTypes.INTEGER, DataTypes.INTEGER)));
        CollectNode collectNode = new CollectNode(0, "docCollect", routing(TEST_TABLE_NAME));
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testDocLevelReference));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.whereClause(new WhereClause(new Function(
                op.info(),
                Arrays.<Symbol>asList(testDocLevelReference, Literal.newLiteral(2)))
        ));
        PlanNodeBuilder.setOutputTypes(collectNode);

        Bucket result = collect(collectNode);
        assertThat(result, contains(isRow(2)));
    }


    @Test
    public void testCollectWithPartitionedColumns() throws Exception {
        Routing routing = docSchemaInfo.getTableInfo(PARTITIONED_TABLE_NAME).getRouting(WhereClause.MATCH_ALL);
        TableIdent tableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, PARTITIONED_TABLE_NAME);
        CollectNode collectNode = new CollectNode(0, "docCollect", routing);
        collectNode.toCollect(Arrays.<Symbol>asList(
                new Reference(new ReferenceInfo(
                        new ReferenceIdent(tableIdent, "id"),
                        RowGranularity.DOC, DataTypes.INTEGER)),
                new Reference(new ReferenceInfo(
                        new ReferenceIdent(tableIdent, "date"),
                        RowGranularity.SHARD, DataTypes.TIMESTAMP))
        ));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.isPartitioned(true);
        collectNode.jobId(UUID.randomUUID());
        PlanNodeBuilder.setOutputTypes(collectNode);

        Bucket result = collect(collectNode);
        for (Row row : result) {
            System.out.println("Row:" + Arrays.toString(row.materialize()));
        }

        assertThat(result, containsInAnyOrder(
                isRow(1, 0L),
                isRow(2, 1L)
        ));
    }

    private Bucket collect(CollectNode collectNode) throws Exception {
        ContextPreparer contextPreparer = internalCluster().getDataNodeInstance(ContextPreparer.class);
        JobContextService contextService = internalCluster().getDataNodeInstance(JobContextService.class);
        JobExecutionContext.Builder builder = contextService.newBuilder(collectNode.jobId());
        ListenableFuture<Bucket> future = contextPreparer.prepare(collectNode.jobId(), collectNode, builder);
        assert future != null;
        JobExecutionContext context = contextService.createContext(builder);
        context.start();
        return future.get(500, TimeUnit.MILLISECONDS);
    }
}


File: sql/src/test/java/io/crate/operation/collect/HandlerSideLevelCollectTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.jobs.ExecutionState;
import io.crate.metadata.*;
import io.crate.metadata.information.InformationSchemaInfo;
import io.crate.metadata.sys.SysClusterTableInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.operator.EqOperator;
import io.crate.testing.CollectingProjector;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.hamcrest.Matchers;
import org.junit.Before;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.core.Is.is;
import static org.mockito.Mockito.mock;

@ElasticsearchIntegrationTest.ClusterScope(numDataNodes = 1)
public class HandlerSideLevelCollectTest extends SQLTransportIntegrationTest {

    private MapSideDataCollectOperation operation;
    private Functions functions;
    private String localNodeId;


    @Before
    public void prepare() {
        operation = internalCluster().getDataNodeInstance(MapSideDataCollectOperation.class);
        functions = internalCluster().getInstance(Functions.class);
        localNodeId = internalCluster().getDataNodeInstance(ClusterService.class).state().nodes().localNodeId();
    }

    @Test
    public void testClusterLevel() throws Exception {
        Routing routing = SysClusterTableInfo.ROUTING;
        CollectNode collectNode = new CollectNode(0, "clusterCollect", routing);
        collectNode.jobId(UUID.randomUUID());
        Reference clusterNameRef = new Reference(SysClusterTableInfo.INFOS.get(new ColumnIdent("name")));
        collectNode.toCollect(Arrays.<Symbol>asList(clusterNameRef));
        collectNode.maxRowGranularity(RowGranularity.CLUSTER);
        collectNode.handlerSideCollect(localNodeId);
        Bucket result = collect(collectNode);
        assertThat(result.size(), is(1));
        assertThat(((BytesRef) result.iterator().next().get(0)).utf8ToString(), Matchers.startsWith("SUITE-"));
    }

    private Bucket collect(CollectNode collectNode) throws InterruptedException, java.util.concurrent.ExecutionException {
        CollectingProjector collectingProjector = new CollectingProjector();
        collectingProjector.startProjection(mock(ExecutionState.class));
        operation.collect(collectNode, collectingProjector, mock(JobCollectContext.class));
        return collectingProjector.result().get();
    }

    @Test
    public void testInformationSchemaTables() throws Exception {
        Routing routing = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder().put(
                TableInfo.NULL_NODE_ID, TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("information_schema.tables", null).map()
        ).map());
        CollectNode collectNode = new CollectNode(0, "tablesCollect", routing);
        collectNode.jobId(UUID.randomUUID());
        InformationSchemaInfo schemaInfo =  internalCluster().getInstance(InformationSchemaInfo.class);
        TableInfo tablesTableInfo = schemaInfo.getTableInfo("tables");
        List<Symbol> toCollect = new ArrayList<>();
        for (ReferenceInfo info : tablesTableInfo.columns()) {
            toCollect.add(new Reference(info));
        }
        Symbol tableNameRef = toCollect.get(1);

        FunctionImplementation eqImpl = functions.get(new FunctionIdent(EqOperator.NAME,
                ImmutableList.<DataType>of(DataTypes.STRING, DataTypes.STRING)));
        Function whereClause = new Function(eqImpl.info(),
                Arrays.asList(tableNameRef, Literal.newLiteral("shards")));

        collectNode.whereClause(new WhereClause(whereClause));
        collectNode.toCollect(toCollect);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.handlerSideCollect(localNodeId);
        Bucket result = collect(collectNode);
        System.out.println(TestingHelpers.printedTable(result));
        assertEquals("sys| shards| 1| 0| NULL| NULL| NULL| NULL\n", TestingHelpers.printedTable(result));
    }


    @Test
    public void testInformationSchemaColumns() throws Exception {
        Routing routing = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder().put(
                TableInfo.NULL_NODE_ID, TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("information_schema.columns", null).map()
        ).map());
        CollectNode collectNode = new CollectNode(0, "columnsCollect", routing);
        collectNode.jobId(UUID.randomUUID());
        InformationSchemaInfo schemaInfo =  internalCluster().getInstance(InformationSchemaInfo.class);
        TableInfo tableInfo = schemaInfo.getTableInfo("columns");
        List<Symbol> toCollect = new ArrayList<>();
        for (ReferenceInfo info : tableInfo.columns()) {
            toCollect.add(new Reference(info));
        }
        collectNode.toCollect(toCollect);
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.handlerSideCollect(localNodeId);
        Bucket result = collect(collectNode);


        String expected = "sys| cluster| id| 1| string\n" +
                "sys| cluster| name| 2| string\n" +
                "sys| cluster| master_node| 3| string\n" +
                "sys| cluster| settings| 4| object";


        assertTrue(TestingHelpers.printedTable(result).contains(expected));

        // second time - to check if the internal iterator resets
        System.out.println(TestingHelpers.printedTable(result));
        result = collect(collectNode);
        assertTrue(TestingHelpers.printedTable(result).contains(expected));
    }

}


File: sql/src/test/java/io/crate/operation/collect/LocalDataCollectTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.WhereClause;
import io.crate.blob.BlobEnvironment;
import io.crate.blob.v2.BlobIndices;
import io.crate.breaker.CircuitBreakerModule;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnhandledServerException;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.*;
import io.crate.metadata.shard.ShardReferenceImplementation;
import io.crate.metadata.shard.ShardReferenceResolver;
import io.crate.metadata.shard.blob.BlobShardReferenceImplementation;
import io.crate.metadata.sys.SysNodesTableInfo;
import io.crate.metadata.sys.SysShardsTableInfo;
import io.crate.operation.Input;
import io.crate.operation.operator.AndOperator;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.operator.OperatorModule;
import io.crate.operation.reference.sys.node.NodeSysExpression;
import io.crate.operation.reference.sys.node.SysNodeExpressionModule;
import io.crate.testing.CollectingProjector;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.operation.reference.sys.shard.SysShardExpression;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.create.TransportBulkCreateIndicesAction;
import org.elasticsearch.action.admin.indices.create.TransportCreateIndexAction;
import org.elasticsearch.action.admin.indices.delete.TransportDeleteIndexAction;
import org.elasticsearch.action.admin.indices.settings.put.TransportUpdateSettingsAction;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.action.bulk.BulkRetryCoordinator;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.action.bulk.TransportShardBulkAction;
import org.elasticsearch.action.support.ActionFilters;
import org.elasticsearch.client.Client;
import org.elasticsearch.cluster.ClusterInfoService;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.metadata.MetaDataCreateIndexService;
import org.elasticsearch.cluster.metadata.MetaDataDeleteIndexService;
import org.elasticsearch.cluster.metadata.MetaDataUpdateSettingsService;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.cluster.routing.allocation.AllocationService;
import org.elasticsearch.cluster.routing.allocation.decider.AllocationDecider;
import org.elasticsearch.cluster.routing.allocation.decider.DiskThresholdDecider;
import org.elasticsearch.cluster.settings.ClusterDynamicSettings;
import org.elasticsearch.cluster.settings.DynamicSettings;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.breaker.NoopCircuitBreaker;
import org.elasticsearch.common.inject.*;
import org.elasticsearch.common.inject.multibindings.MapBinder;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.discovery.Discovery;
import org.elasticsearch.discovery.DiscoveryService;
import org.elasticsearch.index.Index;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.index.mapper.MapperService;
import org.elasticsearch.index.settings.IndexSettings;
import org.elasticsearch.index.shard.IndexShard;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.indices.IndicesLifecycle;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.indices.breaker.CircuitBreakerService;
import org.elasticsearch.indices.breaker.NoneCircuitBreakerService;
import org.elasticsearch.monitor.network.NetworkService;
import org.elasticsearch.monitor.os.OsService;
import org.elasticsearch.monitor.os.OsStats;
import org.elasticsearch.node.service.NodeService;
import org.elasticsearch.node.settings.NodeSettingsService;
import org.elasticsearch.script.ScriptService;
import org.elasticsearch.search.InternalSearchService;
import org.elasticsearch.search.SearchService;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.transport.TransportService;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Answers;

import java.util.*;
import java.util.concurrent.ExecutionException;

import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.*;
import static org.mockito.Matchers.any;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class LocalDataCollectTest extends CrateUnitTest {

    static class TestFunction extends Scalar<Integer,Object> {
        public static final FunctionIdent ident = new FunctionIdent("twoTimes", Arrays.<DataType>asList(DataTypes.INTEGER));
        public static final FunctionInfo info = new FunctionInfo(ident, DataTypes.INTEGER);

        @Override
        public Integer evaluate(Input<Object>... args) {
            if (args.length == 0) {
                return 0;
            }
            Short value = (Short) args[0].value();
            return value * 2;
        }

        @Override
        public FunctionInfo info() {
            return info;
        }

        @Override
        public Symbol normalizeSymbol(Function symbol) {
            return symbol;
        }
    }

    static class ShardIdExpression extends SysShardExpression<Integer> implements ShardReferenceImplementation<Integer> {

        private final ShardId shardId;

        @Inject
        public ShardIdExpression(ShardId shardId) {
            this.shardId = shardId;
        }

        @Override
        public Integer value() {
            return shardId.id();
        }

        @Override
        public ReferenceImplementation getChildImplementation(String name) {
            return null;
        }
    }

    private DiscoveryService discoveryService;
    private Functions functions;
    private IndexService indexService = mock(IndexService.class);
    private MapSideDataCollectOperation operation;
    private Routing testRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
        .put(TEST_NODE_ID, new TreeMap<String, List<Integer>>()).map()
    );

    private JobContextService jobContextService;

    private final ThreadPool testThreadPool = new ThreadPool(getClass().getSimpleName());
    private final static String TEST_NODE_ID = "test_node";
    private final static String TEST_TABLE_NAME = "test_table";

    private static Reference testNodeReference = new Reference(
            SysNodesTableInfo.INFOS.get(new ColumnIdent("os", ImmutableList.of("cpu", "stolen")))
    );
    private static Reference testShardIdReference = new Reference(SysShardsTableInfo.INFOS.get(new ColumnIdent("id")));

    private static final RamAccountingContext RAM_ACCOUNTING_CONTEXT =
            new RamAccountingContext("dummy", new NoopCircuitBreaker(CircuitBreaker.Name.FIELDDATA));

    class TestModule extends AbstractModule {
        protected MapBinder<FunctionIdent, FunctionImplementation> functionBinder;

        @Override
        protected void configure() {
            functionBinder = MapBinder.newMapBinder(binder(), FunctionIdent.class, FunctionImplementation.class);
            functionBinder.addBinding(TestFunction.ident).toInstance(new TestFunction());
            bind(Functions.class).asEagerSingleton();
            bind(ReferenceInfos.class).toInstance(mock(ReferenceInfos.class));
            bind(ThreadPool.class).toInstance(testThreadPool);

            BulkRetryCoordinator bulkRetryCoordinator = mock(BulkRetryCoordinator.class);
            BulkRetryCoordinatorPool bulkRetryCoordinatorPool = mock(BulkRetryCoordinatorPool.class);
            when(bulkRetryCoordinatorPool.coordinator(any(ShardId.class))).thenReturn(bulkRetryCoordinator);
            bind(BulkRetryCoordinatorPool.class).toInstance(bulkRetryCoordinatorPool);

            bind(TransportBulkCreateIndicesAction.class).toInstance(mock(TransportBulkCreateIndicesAction.class));
            bind(CircuitBreakerService.class).toInstance(new NoneCircuitBreakerService());
            bind(ActionFilters.class).toInstance(mock(ActionFilters.class));
            bind(ScriptService.class).toInstance(mock(ScriptService.class));
            bind(SearchService.class).toInstance(mock(InternalSearchService.class));
            bind(AllocationService.class).toInstance(mock(AllocationService.class));
            bind(MetaDataCreateIndexService.class).toInstance(mock(MetaDataCreateIndexService.class));
            bind(DynamicSettings.class).annotatedWith(ClusterDynamicSettings.class).toInstance(mock(DynamicSettings.class));
            bind(MetaDataDeleteIndexService.class).toInstance(mock(MetaDataDeleteIndexService.class));
            bind(ClusterInfoService.class).toInstance(mock(ClusterInfoService.class));
            bind(TransportService.class).toInstance(mock(TransportService.class));
            bind(MapperService.class).toInstance(mock(MapperService.class));

            OsService osService = mock(OsService.class);
            OsStats osStats = mock(OsStats.class);
            when(osService.stats()).thenReturn(osStats);
            OsStats.Cpu osCpu = mock(OsStats.Cpu.class);
            when(osCpu.stolen()).thenReturn((short) 1);
            when(osStats.cpu()).thenReturn(osCpu);

            bind(OsService.class).toInstance(osService);
            bind(NodeService.class).toInstance(mock(NodeService.class));
            bind(Discovery.class).toInstance(mock(Discovery.class));
            bind(NetworkService.class).toInstance(mock(NetworkService.class));

            bind(TransportShardBulkAction.class).toInstance(mock(TransportShardBulkAction.class));
            bind(TransportCreateIndexAction.class).toInstance(mock(TransportCreateIndexAction.class));

            discoveryService = mock(DiscoveryService.class);
            DiscoveryNode discoveryNode = mock(DiscoveryNode.class);
            when(discoveryNode.id()).thenReturn(TEST_NODE_ID);
            when(discoveryService.localNode()).thenReturn(discoveryNode);

            ClusterService clusterService = mock(ClusterService.class);
            ClusterState state = mock(ClusterState.class);
            DiscoveryNodes discoveryNodes = mock(DiscoveryNodes.class);
            when(discoveryNodes.localNodeId()).thenReturn(TEST_NODE_ID);
            when(state.nodes()).thenReturn(discoveryNodes);
            when(clusterService.state()).thenReturn(state);
            when(clusterService.localNode()).thenReturn(discoveryNode);
            bind(ClusterService.class).toInstance(clusterService);

            IndicesService indicesService = mock(IndicesService.class);
            bind(IndicesService.class).toInstance(indicesService);
            bind(Settings.class).toInstance(ImmutableSettings.EMPTY);

            bind(MetaDataUpdateSettingsService.class).toInstance(mock(MetaDataUpdateSettingsService.class));
            bind(Client.class).toInstance(mock(Client.class));

            Provider<TransportCreateIndexAction> transportCreateIndexActionProvider = mock(Provider.class);
            when(transportCreateIndexActionProvider.get()).thenReturn(mock(TransportCreateIndexAction.class));
            Provider<TransportDeleteIndexAction> transportDeleteActionProvider = mock(Provider.class);
            when(transportDeleteActionProvider.get()).thenReturn(mock(TransportDeleteIndexAction.class));
            Provider<TransportUpdateSettingsAction> transportUpdateSettingsActionProvider = mock(Provider.class);
            when(transportUpdateSettingsActionProvider.get()).thenReturn(mock(TransportUpdateSettingsAction.class));

            BlobIndices blobIndices = new BlobIndices(
                    ImmutableSettings.EMPTY,
                    transportCreateIndexActionProvider,
                    transportDeleteActionProvider,
                    transportUpdateSettingsActionProvider,
                    indicesService,
                    mock(IndicesLifecycle.class),
                    mock(BlobEnvironment.class),
                    clusterService
            );
            bind(BlobIndices.class).toInstance(blobIndices);

            bind(ReferenceResolver.class).to(GlobalReferenceResolver.class);

            TransportPutIndexTemplateAction transportPutIndexTemplateAction = mock(TransportPutIndexTemplateAction.class);
            bind(TransportPutIndexTemplateAction.class).toInstance(transportPutIndexTemplateAction);

            bind(IndexService.class).toInstance(indexService);
        }
    }

    class TestShardModule extends AbstractModule {

        private final ShardId shardId;
        private final ShardIdExpression shardIdExpression;

        public TestShardModule(int shardId) {
            super();
            this.shardId = new ShardId(TEST_TABLE_NAME, shardId);
            this.shardIdExpression = new ShardIdExpression(this.shardId);
        }

        @Override
        protected void configure() {
            IndexShard shard = mock(IndexShard.class);
            bind(IndexShard.class).toInstance(shard);
            when(shard.shardId()).thenReturn(shardId);
            Index index = new Index(TEST_TABLE_NAME);
            bind(Index.class).toInstance(index);
            bind(ShardId.class).toInstance(shardId);
            MapBinder<ReferenceIdent, ShardReferenceImplementation> binder = MapBinder
                    .newMapBinder(binder(), ReferenceIdent.class, ShardReferenceImplementation.class);
            binder.addBinding(SysShardsTableInfo.INFOS.get(new ColumnIdent("id")).ident()).toInstance(shardIdExpression);
            bind(ShardReferenceResolver.class).asEagerSingleton();
            bind(AllocationDecider.class).to(DiskThresholdDecider.class);
            bind(ShardCollectService.class).asEagerSingleton();

            bind(DiscoveryService.class).toInstance(discoveryService);


            // blob stuff
            MapBinder<ReferenceIdent, BlobShardReferenceImplementation> blobBinder = MapBinder
                    .newMapBinder(binder(), ReferenceIdent.class, BlobShardReferenceImplementation.class);
            bind(Settings.class).annotatedWith(IndexSettings.class).toInstance(ImmutableSettings.EMPTY);
        }
    }

    @Before
    public void configure() {
        Injector injector = new ModulesBuilder().add(
                new CircuitBreakerModule(),
                new OperatorModule(),
                new TestModule(),
                new SysNodeExpressionModule()
        ).createInjector();
        Injector shard0Injector = injector.createChildInjector(
                new TestShardModule(0)
        );
        Injector shard1Injector = injector.createChildInjector(
                new TestShardModule(1)
        );
        functions = injector.getInstance(Functions.class);

        IndicesService indicesService = injector.getInstance(IndicesService.class);
        indexService = injector.getInstance(IndexService.class);

        when(indexService.shardInjectorSafe(0)).thenReturn(shard0Injector);
        when(indexService.shardInjectorSafe(1)).thenReturn(shard1Injector);
        when(indexService.shardSafe(0)).thenReturn(shard0Injector.getInstance(IndexShard.class));
        when(indexService.shardSafe(1)).thenReturn(shard1Injector.getInstance(IndexShard.class));
        when(indicesService.indexServiceSafe(TEST_TABLE_NAME)).thenReturn(indexService);

        NodeSettingsService nodeSettingsService = mock(NodeSettingsService.class);
        jobContextService = new JobContextService(ImmutableSettings.EMPTY, testThreadPool, mock(StatsTables.class));

        ClusterService clusterService = injector.getInstance(ClusterService.class);
        operation = new MapSideDataCollectOperation(
                clusterService,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                injector.getInstance(BulkRetryCoordinatorPool.class),
                functions,
                injector.getInstance(ReferenceResolver.class),
                injector.getInstance(NodeSysExpression.class),
                indicesService,
                testThreadPool,
                new CollectServiceResolver(discoveryService,
                        new SystemCollectService(
                                discoveryService,
                                functions,
                                new StatsTables(ImmutableSettings.EMPTY, nodeSettingsService))
                ),
                new ResultProviderFactory() {
                    @Override
                    public ResultProvider createDownstream(ExecutionNode node, UUID jobId) {
                        return new CollectingProjector();
                    }
                },
                mock(InformationSchemaCollectService.class),
                mock(UnassignedShardsCollectService.class)
        );
    }

    @After
    public void cleanUp() throws Exception {
        testThreadPool.shutdownNow();
    }

    private Routing shardRouting(final Integer... shardIds) {
        return new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put(TEST_NODE_ID, TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                            .put(TEST_TABLE_NAME, Arrays.asList(shardIds))
                            .map()
            )
            .map()
        );
    }

    @Test
    public void testCollectExpressions() throws Exception {
        CollectNode collectNode = new CollectNode(0, "collect", testRouting);
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.maxRowGranularity(RowGranularity.NODE);

        Bucket result = getBucket(collectNode);

        assertThat(result.size(), equalTo(1));
        assertThat(result, contains(isRow((short) 1)));
    }

    @Test
    public void testWrongRouting() throws Exception {

        expectedException.expect(UnhandledServerException.class);
        expectedException.expectMessage("unsupported routing");

        CollectNode collectNode = new CollectNode(0, "wrong", new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("bla", TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                .put("my_index", Arrays.asList(1))
                .put("my_index", Arrays.asList(1))
                .map()
            ).map()
        ));
        collectNode.maxRowGranularity(RowGranularity.DOC);
        collectNode.jobId(UUID.randomUUID());
        operation.collect(collectNode, new CollectingProjector(), null);
    }

    @Test
    public void testCollectUnknownReference() throws Throwable {
        expectedException.expect(UnhandledServerException.class);
        expectedException.expectMessage("Unknown Reference some.table.some_column");

        CollectNode collectNode = new CollectNode(0, "unknown", testRouting);
        collectNode.jobId(UUID.randomUUID());
        Reference unknownReference = new Reference(
                new ReferenceInfo(
                        new ReferenceIdent(
                                new TableIdent("some", "table"),
                                "some_column"
                        ),
                        RowGranularity.NODE,
                        DataTypes.BOOLEAN
                )
        );
        collectNode.toCollect(Arrays.<Symbol>asList(unknownReference));
        collectNode.maxRowGranularity(RowGranularity.NODE);
        try {
            getBucket(collectNode);
        } catch (ExecutionException e) {
            throw e.getCause();
        }
    }

    @Test
    public void testCollectFunction() throws Exception {
        CollectNode collectNode = new CollectNode(0, "function", testRouting);
        collectNode.jobId(UUID.randomUUID());
        Function twoTimesTruthFunction = new Function(
                TestFunction.info,
                Arrays.<Symbol>asList(testNodeReference)
        );
        collectNode.toCollect(Arrays.<Symbol>asList(twoTimesTruthFunction, testNodeReference));
        collectNode.maxRowGranularity(RowGranularity.NODE);
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), equalTo(1));
        assertThat(result, contains(isRow(2, (short) 1)));
    }


    @Test
    public void testUnknownFunction() throws Throwable {
        // will be wrapped somewhere above
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot find implementation for function unknown()");

        CollectNode collectNode = new CollectNode(0, "unknownFunction", testRouting);
        Function unknownFunction = new Function(
                new FunctionInfo(
                        new FunctionIdent("unknown", ImmutableList.<DataType>of()),
                        DataTypes.BOOLEAN
                ),
                ImmutableList.<Symbol>of()
        );
        collectNode.toCollect(Arrays.<Symbol>asList(unknownFunction));
        collectNode.jobId(UUID.randomUUID());
        try {
            getBucket(collectNode);
        } catch (ExecutionException e) {
            throw e.getCause();
        }
    }

    @Test
    public void testCollectLiterals() throws Exception {
        CollectNode collectNode = new CollectNode(0, "literals", testRouting);
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(
                Literal.newLiteral("foobar"),
                Literal.newLiteral(true),
                Literal.newLiteral(1),
                Literal.newLiteral(4.2)
        ));
        Bucket result = getBucket(collectNode);
        assertThat(result, contains(isRow(new BytesRef("foobar"), true, 1, 4.2)));
    }

    @Test
    public void testCollectWithFalseWhereClause() throws Exception {
        CollectNode collectNode = new CollectNode(0, "whereClause", testRouting);
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.whereClause(new WhereClause(new Function(
                AndOperator.INFO,
                Arrays.<Symbol>asList(Literal.newLiteral(false), Literal.newLiteral(false))
        )));
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(0));
    }

    @Test
    public void testCollectWithTrueWhereClause() throws Exception {
        CollectNode collectNode = new CollectNode(0, "whereClause", testRouting);
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.whereClause(new WhereClause(new Function(
                AndOperator.INFO,
                Arrays.<Symbol>asList(Literal.newLiteral(true), Literal.newLiteral(true))
        )));
        collectNode.jobId(UUID.randomUUID());
        collectNode.maxRowGranularity(RowGranularity.NODE);
        Bucket result = getBucket(collectNode);
        assertThat(result, contains(isRow((short) 1)));

    }

    @Test
    public void testCollectWithNullWhereClause() throws Exception {
        EqOperator op = (EqOperator) functions.get(new FunctionIdent(
                EqOperator.NAME, ImmutableList.<DataType>of(DataTypes.INTEGER, DataTypes.INTEGER)));
        CollectNode collectNode = new CollectNode(0, "whereClause", testRouting);
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testNodeReference));
        collectNode.whereClause(new WhereClause(new Function(
                op.info(),
                Arrays.<Symbol>asList(Literal.NULL, Literal.NULL)
        )));
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(0));
    }

    private Bucket getBucket(CollectNode collectNode) throws InterruptedException, ExecutionException {
        CollectingProjector cd = new CollectingProjector();
        JobExecutionContext.Builder builder = jobContextService.newBuilder(collectNode.jobId());
        JobCollectContext jobCollectContext =
                new JobCollectContext(collectNode.jobId(), collectNode, operation, RAM_ACCOUNTING_CONTEXT, cd);
        builder.addSubContext(collectNode.executionNodeId(), jobCollectContext);
        JobExecutionContext context = jobContextService.createContext(builder);
        cd.startProjection(jobCollectContext);
        operation.collect(collectNode, cd, jobCollectContext);
        return cd.result().get();
    }

    @Test
    public void testCollectShardExpressions() throws Exception {
        CollectNode collectNode = new CollectNode(0, "shardCollect", shardRouting(0, 1));
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testShardIdReference));
        collectNode.maxRowGranularity(RowGranularity.SHARD);

        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(2));
        assertThat(result, containsInAnyOrder(isRow(0), isRow(1)));
    }

    @Test
    public void testCollectShardExpressionsWhereShardIdIs0() throws Exception {
        EqOperator op = (EqOperator) functions.get(new FunctionIdent(
                EqOperator.NAME, ImmutableList.<DataType>of(DataTypes.INTEGER, DataTypes.INTEGER)));

        CollectNode collectNode = new CollectNode(0, "shardCollect", shardRouting(0, 1));
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.<Symbol>asList(testShardIdReference));
        collectNode.whereClause(new WhereClause(
                new Function(op.info(), Arrays.asList(testShardIdReference, Literal.newLiteral(0)))));
        collectNode.maxRowGranularity(RowGranularity.SHARD);
        Bucket result = getBucket(collectNode);
        assertThat(result, contains(isRow(0)));
    }

    @Test
    public void testCollectShardExpressionsLiteralsAndNodeExpressions() throws Exception {
        CollectNode collectNode = new CollectNode(0, "shardCollect", shardRouting(0, 1));
        collectNode.jobId(UUID.randomUUID());
        collectNode.toCollect(Arrays.asList(testShardIdReference, Literal.newLiteral(true), testNodeReference));
        collectNode.maxRowGranularity(RowGranularity.SHARD);
        Bucket result = getBucket(collectNode);
        assertThat(result.size(), is(2));
        assertThat(result, containsInAnyOrder(isRow(0, true, (short) 1), isRow(1, true, (short) 1)));
    }
}


File: sql/src/test/java/io/crate/operation/collect/LuceneDocCollectorTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.operation.collect;

import com.google.common.collect.ImmutableList;
import io.crate.action.sql.SQLBulkRequest;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.breaker.RamAccountingContext;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.metadata.*;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.operation.scalar.arithmetic.MultiplyFunction;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Function;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.CollectingProjector;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.bulk.BulkRequest;
import org.elasticsearch.action.bulk.BulkResponse;
import org.elasticsearch.action.index.IndexRequest;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.breaker.NoopCircuitBreaker;
import org.elasticsearch.common.xcontent.XContentFactory;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.test.ElasticsearchIntegrationTest;
import org.junit.Before;
import org.junit.Test;

import java.io.IOException;
import java.util.Arrays;
import java.util.List;
import java.util.UUID;

import static io.crate.testing.TestingHelpers.createReference;
import static org.hamcrest.Matchers.is;
import static org.hamcrest.Matchers.nullValue;
import static org.mockito.Matchers.any;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

@ElasticsearchIntegrationTest.ClusterScope(scope = ElasticsearchIntegrationTest.Scope.SUITE, numDataNodes = 1)
public class LuceneDocCollectorTest extends SQLTransportIntegrationTest {

    private final static Integer PAGE_SIZE = 20;
    private final static String INDEX_NAME = "countries";
    private final static Integer NUMBER_OF_DOCS = 25;
    private OrderBy orderBy;
    private JobContextService jobContextService;
    private ShardCollectService shardCollectService;

    private CollectingProjector collectingProjector = new CollectingProjector();

    private static final RamAccountingContext RAM_ACCOUNTING_CONTEXT =
            new RamAccountingContext("dummy", new NoopCircuitBreaker(CircuitBreaker.Name.FIELDDATA));
    private JobCollectContext jobCollectContext;

    @Before
    public void prepare() throws Exception{
        execute("create table \""+INDEX_NAME+ "\" (" +
                " continent string, " +
                " countryName string," +
                " population integer" +
                ") clustered into 1 shards with (number_of_replicas=0)");
        refresh();
        generateData();
        IndicesService instanceFromNode = internalCluster().getDataNodeInstance(IndicesService.class);
        IndexService indexService = instanceFromNode.indexServiceSafe(INDEX_NAME);

        shardCollectService = indexService.shardInjectorSafe(0).getInstance(ShardCollectService.class);
        jobContextService = indexService.shardInjectorSafe(0).getInstance(JobContextService.class);

        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{false}, new Boolean[]{false});
    }

    private byte[] generateRowSource(String continent, String countryName, Integer population) throws IOException {
        return XContentFactory.jsonBuilder()
                .startObject()
                .field("continent", continent)
                .field("countryName", countryName)
                .field("population", population)
                .endObject()
                .bytes().toBytes();
    }

    public void generateData() throws Exception {
        BulkRequest bulkRequest = new BulkRequest();
        for (int i=0; i < NUMBER_OF_DOCS; i++) {
            IndexRequest indexRequest = new IndexRequest(INDEX_NAME, "default", String.valueOf(i));
            if (i == 0) {
                indexRequest.source(generateRowSource("Europe", "Germany", i));
            } else if (i == 1) {
                indexRequest.source(generateRowSource("Europe", "Austria", i));
            } else if (i >= 2 && i <=4) {
                indexRequest.source(generateRowSource("Europe", null, i));
            } else {
                indexRequest.source(generateRowSource("America", "USA", i));
            }
            bulkRequest.add(indexRequest);
        }
        BulkResponse response = client().bulk(bulkRequest).actionGet();
        assertFalse(response.hasFailures());
        refresh();
    }

    private LuceneDocCollector createDocCollector(OrderBy orderBy, Integer limit, List<Symbol> toCollect) throws Exception{
        return createDocCollector(orderBy, limit, toCollect, WhereClause.MATCH_ALL, PAGE_SIZE);
    }

    private LuceneDocCollector createDocCollector(OrderBy orderBy, Integer limit, List<Symbol> toCollect, WhereClause whereClause, int pageSize) throws Exception{
        CollectNode node = new CollectNode(0, "collect");
        node.whereClause(whereClause);
        node.orderBy(orderBy);
        node.limit(limit);
        UUID jobId = UUID.randomUUID();
        node.jobId(jobId);
        node.toCollect(toCollect);
        node.maxRowGranularity(RowGranularity.DOC);

        ShardProjectorChain projectorChain = mock(ShardProjectorChain.class);
        when(projectorChain.newShardDownstreamProjector(any(ProjectionToProjectorVisitor.class))).thenReturn(collectingProjector);

        JobExecutionContext.Builder builder = jobContextService.newBuilder(jobId);
        jobCollectContext = new JobCollectContext(
                jobId, node, mock(CollectOperation.class), RAM_ACCOUNTING_CONTEXT, collectingProjector);
        builder.addSubContext(node.executionNodeId(), jobCollectContext);
        jobContextService.createContext(builder);
        LuceneDocCollector collector = (LuceneDocCollector)shardCollectService.getCollector(node, projectorChain, jobCollectContext, 0);
        collector.pageSize(pageSize);
        return collector;
    }

    @Test
    public void testLimitWithoutOrder() throws Exception{
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(null, 15, orderBy.orderBySymbols());
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(15));
    }

    @Test
    public void testOrderedWithLimit() throws Exception{
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(orderBy, 15, orderBy.orderBySymbols());
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(15));
        assertThat(((BytesRef)collectingProjector.rows.get(0)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(1)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(2)[0]).utf8ToString(), is("USA") );
        assertThat(((BytesRef)collectingProjector.rows.get(3)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedWithLimitHigherThanPageSize() throws Exception{
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(orderBy, PAGE_SIZE + 5, orderBy.orderBySymbols());
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(PAGE_SIZE + 5));
        assertThat(((BytesRef)collectingProjector.rows.get(0)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(1)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(2)[0]).utf8ToString(), is("USA") );
        assertThat(((BytesRef)collectingProjector.rows.get(3)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(((BytesRef)collectingProjector.rows.get(0)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(1)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(2)[0]).utf8ToString(), is("USA") );
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS -1)[0], is(nullValue()));
    }

    @Test
    public void testOrderedNullsFirstWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{false}, new Boolean[]{true});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(0)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(1)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(2)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(3)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(4)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(5)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedDescendingWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{true}, new Boolean[]{false});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 3)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 4)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 5)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 6)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderedDescendingNullsFirstWithoutLimit() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent ident = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference ref = new Reference(new ReferenceInfo(ident, RowGranularity.DOC, DataTypes.STRING));
        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)ref), new boolean[]{true}, new Boolean[]{true});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, orderBy.orderBySymbols(), WhereClause.MATCH_ALL, 1);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(0)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(1)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(2)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0]).utf8ToString(), is("Germany") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 3)[0]).utf8ToString(), is("USA") );
    }

    @Test
    public void testOrderForNonSelected() throws Exception {
        collectingProjector.rows.clear();
        ReferenceIdent countriesIdent = new ReferenceIdent(new TableIdent("doc", "countries"), "countryName");
        Reference countries = new Reference(new ReferenceInfo(countriesIdent, RowGranularity.DOC, DataTypes.STRING));

        ReferenceIdent populationIdent = new ReferenceIdent(new TableIdent("doc", "countries"), "population");
        Reference population = new Reference(new ReferenceInfo(populationIdent, RowGranularity.DOC, DataTypes.INTEGER));

        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)population), new boolean[]{true}, new Boolean[]{true});

        LuceneDocCollector docCollector = createDocCollector(orderBy, null, ImmutableList.of((Symbol)countries));
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(collectingProjector.rows.get(0).length, is(1));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 6)[0]).utf8ToString(), is("USA") );
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 5)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 4)[0], is(nullValue()));
        assertThat(collectingProjector.rows.get(NUMBER_OF_DOCS - 3)[0], is(nullValue()));
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0]).utf8ToString(), is("Austria") );
        assertThat(((BytesRef)collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0]).utf8ToString(), is("Germany") );
    }

    @Test
    public void testOrderByScalar() throws Exception {
        collectingProjector.rows.clear();
        Reference population = createReference("population", DataTypes.INTEGER);
        Function scalarFunction = new Function(
                new FunctionInfo(
                        new FunctionIdent(MultiplyFunction.NAME, Arrays.<DataType>asList(DataTypes.INTEGER, DataTypes.INTEGER)),
                        DataTypes.LONG),
                Arrays.asList(population, Literal.newLiteral(-1))
        );

        OrderBy orderBy = new OrderBy(ImmutableList.of((Symbol)scalarFunction), new boolean[]{false}, new Boolean[]{false});
        LuceneDocCollector docCollector = createDocCollector(orderBy, null, ImmutableList.of((Symbol)population));
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
        assertThat(((Integer)collectingProjector.rows.get(NUMBER_OF_DOCS - 2)[0]), is(1) );
        assertThat(((Integer)collectingProjector.rows.get(NUMBER_OF_DOCS - 1)[0]), is(0) );
    }

    @Test
    public void testMultiOrdering() throws Exception {
        execute("create table test (x integer, y integer) clustered into 1 shards with (number_of_replicas=0)");
        waitNoPendingTasksOnAll();
        SQLBulkRequest request = new SQLBulkRequest("insert into test values (?, ?)",
                new Object[][]{
                    new Object[]{2, 3},
                    new Object[]{2, 1},
                    new Object[]{2, null},
                    new Object[]{1, null},
                    new Object[]{1, 2},
                    new Object[]{1, 1},
                    new Object[]{1, 0},
                    new Object[]{1, null}
                }
        );
        sqlExecutor.exec(request);
        execute("refresh table test");
        collectingProjector.rows.clear();

        IndicesService instanceFromNode = internalCluster().getDataNodeInstance(IndicesService.class);
        IndexService indexService = instanceFromNode.indexServiceSafe("test");

        ShardCollectService shardCollectService = indexService.shardInjectorSafe(0).getInstance(ShardCollectService.class);
        JobContextService jobContextService = indexService.shardInjectorSafe(0).getInstance(JobContextService.class);

        ReferenceIdent xIdent = new ReferenceIdent(new TableIdent("doc", "test"), "x");
        Reference x = new Reference(new ReferenceInfo(xIdent, RowGranularity.DOC, DataTypes.INTEGER));

        ReferenceIdent yIdent = new ReferenceIdent(new TableIdent("doc", "test"), "y");
        Reference y = new Reference(new ReferenceInfo(yIdent, RowGranularity.DOC, DataTypes.INTEGER));

        OrderBy orderBy = new OrderBy(ImmutableList.<Symbol>of(x, y), new boolean[]{false, false}, new Boolean[]{false, false});

        CollectNode node = new CollectNode(0, "collect");
        node.whereClause(WhereClause.MATCH_ALL);
        node.orderBy(orderBy);
        node.jobId(UUID.randomUUID());
        node.toCollect(orderBy.orderBySymbols());
        node.maxRowGranularity(RowGranularity.DOC);

        JobExecutionContext.Builder builder = jobContextService.newBuilder(node.jobId());
        builder.addSubContext(node.executionNodeId(),
                new JobCollectContext(node.jobId(), node, mock(CollectOperation.class), RAM_ACCOUNTING_CONTEXT, collectingProjector));
        jobContextService.createContext(builder);

        ShardProjectorChain projectorChain = mock(ShardProjectorChain.class);
        when(projectorChain.newShardDownstreamProjector(any(ProjectionToProjectorVisitor.class))).thenReturn(collectingProjector);

        JobCollectContext jobCollectContext = jobContextService.getContext(node.jobId()).getSubContext(node.executionNodeId());
        LuceneDocCollector collector = (LuceneDocCollector)shardCollectService.getCollector(node, projectorChain, jobCollectContext, 0);
        collector.pageSize(1);
        collector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(8));

        String expected = "1| 0\n" +
                "1| 1\n" +
                "1| 2\n" +
                "1| NULL\n" +
                "1| NULL\n" +
                "2| 1\n" +
                "2| 3\n" +
                "2| NULL\n";
        assertEquals(expected, TestingHelpers.printedTable(collectingProjector.doFinish()));

        // Nulls first
        node.jobId(UUID.randomUUID());
        builder = jobContextService.newBuilder(node.jobId());
        builder.addSubContext(node.executionNodeId(),
                new JobCollectContext(node.jobId(), node, mock(CollectOperation.class), RAM_ACCOUNTING_CONTEXT, collectingProjector));
        jobContextService.createContext(builder);
        jobCollectContext = jobContextService.getContext(node.jobId()).getSubContext(node.executionNodeId());

        collectingProjector.rows.clear();
        orderBy = new OrderBy(ImmutableList.<Symbol>of(x, y), new boolean[]{false, false}, new Boolean[]{false, true});
        node.orderBy(orderBy);
        collector = (LuceneDocCollector)shardCollectService.getCollector(node, projectorChain, jobCollectContext, 0);
        collector.pageSize(1);
        collector.doCollect(jobCollectContext);

        expected = "1| NULL\n" +
                   "1| NULL\n" +
                   "1| 0\n" +
                   "1| 1\n" +
                   "1| 2\n" +
                   "2| NULL\n" +
                   "2| 1\n" +
                   "2| 3\n";
        assertEquals(expected, TestingHelpers.printedTable(collectingProjector.doFinish()));
    }

    @Test
    public void testMinScoreQuery() throws Exception {
        collectingProjector.rows.clear();
        // where _score = 1.1
        Reference minScore_ref = new Reference(
                new ReferenceInfo(new ReferenceIdent(null, "_score"), RowGranularity.DOC, DataTypes.DOUBLE));

        Function function = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.DOUBLE, DataTypes.DOUBLE)),
                DataTypes.BOOLEAN),
                Arrays.asList(minScore_ref, Literal.newLiteral(1.1))
        );
        WhereClause whereClause = new WhereClause(function);
        LuceneDocCollector docCollector = createDocCollector(null, null, orderBy.orderBySymbols(), whereClause, PAGE_SIZE);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(0));

        // where _score = 1.0
        collectingProjector.rows.clear();
        function = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.DOUBLE, DataTypes.DOUBLE)),
                DataTypes.BOOLEAN),
                Arrays.asList(minScore_ref, Literal.newLiteral(1.0))
        );
        whereClause = new WhereClause(function);
        docCollector = createDocCollector(null, null, orderBy.orderBySymbols(), whereClause, PAGE_SIZE);
        docCollector.doCollect(jobCollectContext);
        assertThat(collectingProjector.rows.size(), is(NUMBER_OF_DOCS));
    }
}


File: sql/src/test/java/io/crate/operation/collect/MapSideDataCollectOperationTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.operation.collect;

import com.google.common.collect.ImmutableMap;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.jobs.ExecutionState;
import io.crate.metadata.*;
import io.crate.operation.reference.sys.node.NodeSysExpression;
import io.crate.testing.CollectingProjector;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Symbol;
import io.crate.types.DataTypes;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.settings.ImmutableSettings;
import org.elasticsearch.discovery.DiscoveryService;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.node.settings.NodeSettingsService;
import org.elasticsearch.threadpool.ThreadPool;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.TemporaryFolder;
import org.mockito.Answers;

import java.io.File;
import java.io.FileWriter;
import java.nio.file.Paths;
import java.util.*;

import static io.crate.testing.TestingHelpers.createReference;
import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;
import static org.junit.Assert.assertThat;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class MapSideDataCollectOperationTest {

    @Rule
    public TemporaryFolder temporaryFolder = new TemporaryFolder();

    @Test
    public void testFileUriCollect() throws Exception {
        ClusterService clusterService = mock(ClusterService.class);
        DiscoveryNode discoveryNode = mock(DiscoveryNode.class);
        when(discoveryNode.id()).thenReturn("dummyNodeId");
        DiscoveryNodes discoveryNodes = mock(DiscoveryNodes.class);
        when(discoveryNodes.localNodeId()).thenReturn("dummyNodeId");
        ClusterState clusterState = mock(ClusterState.class);
        when(clusterState.nodes()).thenReturn(discoveryNodes);
        when(clusterService.state()).thenReturn(clusterState);
        DiscoveryService discoveryService = mock(DiscoveryService.class);
        when(discoveryService.localNode()).thenReturn(discoveryNode);
        IndicesService indicesService = mock(IndicesService.class);
        Functions functions = new Functions(
                ImmutableMap.<FunctionIdent, FunctionImplementation>of(),
                ImmutableMap.<String, DynamicFunctionResolver>of());
        ReferenceResolver referenceResolver = new ReferenceResolver() {
            @Override
            public ReferenceImplementation getImplementation(ReferenceIdent ident) {
                return null;
            }
        };

        NodeSettingsService nodeSettingsService = mock(NodeSettingsService.class);

        MapSideDataCollectOperation collectOperation = new MapSideDataCollectOperation(
                clusterService,
                ImmutableSettings.EMPTY,
                mock(TransportActionProvider.class, Answers.RETURNS_DEEP_STUBS.get()),
                mock(BulkRetryCoordinatorPool.class),
                functions,
                referenceResolver,
                mock(NodeSysExpression.class),
                indicesService,
                new ThreadPool(ImmutableSettings.builder().put("name", getClass().getName()).build(), null),
                new CollectServiceResolver(discoveryService,
                        new SystemCollectService(
                                discoveryService,
                                functions,
                                new StatsTables(ImmutableSettings.EMPTY, nodeSettingsService)
                        )
                ),
                new ResultProviderFactory() {
                    @Override
                    public ResultProvider createDownstream(ExecutionNode node, UUID jobId) {
                        return new CollectingProjector();
                    }
                },
                mock(InformationSchemaCollectService.class),
                mock(UnassignedShardsCollectService.class)
        );

        File tmpFile = temporaryFolder.newFile("fileUriCollectOperation.json");
        try (FileWriter writer = new FileWriter(tmpFile)) {
            writer.write("{\"name\": \"Arthur\", \"id\": 4, \"details\": {\"age\": 38}}\n");
            writer.write("{\"id\": 5, \"name\": \"Trillian\", \"details\": {\"age\": 33}}\n");
        }

        Routing routing = new Routing(
                TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
                .put("dummyNodeId", new TreeMap<String, List<Integer>>())
                .map()
        );
        FileUriCollectNode collectNode = new FileUriCollectNode(
                0,
                "test",
                routing,
                Literal.newLiteral(Paths.get(tmpFile.toURI()).toUri().toString()),
                Arrays.<Symbol>asList(
                        createReference("name", DataTypes.STRING),
                        createReference(new ColumnIdent("details", "age"), DataTypes.INTEGER)
                ),
                Arrays.<Projection>asList(),
                null,
                false
        );
        collectNode.jobId(UUID.randomUUID());
        PlanNodeBuilder.setOutputTypes(collectNode);
        CollectingProjector cd = new CollectingProjector();
        cd.startProjection(mock(ExecutionState.class));
        collectOperation.collect(collectNode, cd, mock(JobCollectContext.class));
        assertThat(cd.result().get(), contains(
                isRow("Arthur", 38),
                isRow("Trillian", 33)
        ));
    }
}


File: sql/src/test/java/io/crate/planner/PlannerTest.java
package io.crate.planner;

import com.google.common.collect.Iterables;
import io.crate.Constants;
import io.crate.analyze.Analyzer;
import io.crate.analyze.BaseAnalyzerTest;
import io.crate.analyze.ParameterContext;
import io.crate.analyze.WhereClause;
import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.exceptions.UnsupportedFeatureException;
import io.crate.exceptions.VersionInvalidException;
import io.crate.metadata.*;
import io.crate.metadata.blob.BlobSchemaInfo;
import io.crate.metadata.blob.BlobTableInfo;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.sys.SysClusterTableInfo;
import io.crate.metadata.sys.SysNodesTableInfo;
import io.crate.metadata.sys.SysSchemaInfo;
import io.crate.metadata.sys.SysShardsTableInfo;
import io.crate.metadata.table.ColumnPolicy;
import io.crate.metadata.table.SchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.metadata.table.TestingTableInfo;
import io.crate.operation.aggregation.impl.AggregationImplModule;
import io.crate.operation.operator.OperatorModule;
import io.crate.operation.predicate.PredicateModule;
import io.crate.operation.projectors.FetchProjector;
import io.crate.operation.scalar.ScalarFunctionModule;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.ddl.DropTableNode;
import io.crate.planner.node.ddl.ESClusterUpdateSettingsNode;
import io.crate.planner.node.dml.*;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.*;
import io.crate.planner.symbol.*;
import io.crate.sql.parser.SqlParser;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.admin.indices.template.put.TransportPutIndexTemplateAction;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.cluster.ClusterState;
import org.elasticsearch.cluster.metadata.IndexTemplateMetaData;
import org.elasticsearch.cluster.metadata.MetaData;
import org.elasticsearch.cluster.node.DiscoveryNode;
import org.elasticsearch.cluster.node.DiscoveryNodes;
import org.elasticsearch.common.collect.ImmutableOpenMap;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.elasticsearch.index.shard.ShardId;
import org.elasticsearch.threadpool.ThreadPool;
import org.hamcrest.Matchers;
import org.hamcrest.core.Is;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;

import java.util.*;
import java.util.concurrent.TimeUnit;

import static io.crate.testing.TestingHelpers.*;
import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;
import static org.mockito.Mockito.mock;
import static org.mockito.Mockito.when;

public class PlannerTest extends CrateUnitTest {

    private Analyzer analyzer;
    private Planner planner;
    Routing shardRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(1, 2)).map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put("t1", Arrays.asList(3, 4)).map())
            .map());

    Routing nodesRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().map())
            .map());

    final Routing partedRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put(".partitioned.parted.04232chj", Arrays.asList(1, 2)).map())
            .put("nodeTow", TreeMapBuilder.<String, List<Integer>>newMapBuilder().map())
            .map());

    final Routing clusteredPartedRouting = new Routing(TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
            .put("nodeOne", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put(".partitioned.clustered_parted.04732cpp6ks3ed1o60o30c1g",  Arrays.asList(1, 2)).map())
            .put("nodeTwo", TreeMapBuilder.<String, List<Integer>>newMapBuilder().put(".partitioned.clustered_parted.04732cpp6ksjcc9i60o30c1g",  Arrays.asList(3)).map())
            .map());

    private ClusterService clusterService;

    private final static String LOCAL_NODE_ID = "foo";
    private ThreadPool threadPool;


    @Before
    public void prepare() throws Exception {
        threadPool = TestingHelpers.newMockedThreadPool();
        Injector injector = new ModulesBuilder()
                .add(new TestModule())
                .add(new AggregationImplModule())
                .add(new ScalarFunctionModule())
                .add(new PredicateModule())
                .add(new OperatorModule())
                .createInjector();
        analyzer = injector.getInstance(Analyzer.class);
        planner = injector.getInstance(Planner.class);
    }

    @After
    public void after() throws Exception {
        threadPool.shutdown();
        threadPool.awaitTermination(1, TimeUnit.SECONDS);
    }


    class TestModule extends MetaDataModule {

        @Override
        protected void configure() {
            bind(ThreadPool.class).toInstance(threadPool);
            clusterService = mock(ClusterService.class);
            DiscoveryNode localNode = mock(DiscoveryNode.class);
            when(localNode.id()).thenReturn(LOCAL_NODE_ID);
            when(clusterService.localNode()).thenReturn(localNode);
            ClusterState clusterState = mock(ClusterState.class);
            MetaData metaData = mock(MetaData.class);
            when(metaData.concreteAllOpenIndices()).thenReturn(new String[0]);
            when(metaData.getTemplates()).thenReturn(ImmutableOpenMap.<String, IndexTemplateMetaData>of());
            when(metaData.templates()).thenReturn(ImmutableOpenMap.<String, IndexTemplateMetaData>of());
            when(clusterState.metaData()).thenReturn(metaData);
            DiscoveryNodes nodes = mock(DiscoveryNodes.class);
            DiscoveryNode node = mock(DiscoveryNode.class);
            when(clusterService.state()).thenReturn(clusterState);
            when(clusterState.nodes()).thenReturn(nodes);
            ImmutableOpenMap<String, DiscoveryNode> dataNodes =
                    ImmutableOpenMap.<String, DiscoveryNode>builder().fPut("foo", node).build();
            when(nodes.dataNodes()).thenReturn(dataNodes);
            when(nodes.localNodeId()).thenReturn(LOCAL_NODE_ID);
            FulltextAnalyzerResolver fulltextAnalyzerResolver = mock(FulltextAnalyzerResolver.class);
            bind(FulltextAnalyzerResolver.class).toInstance(fulltextAnalyzerResolver);
            bind(ClusterService.class).toInstance(clusterService);
            bind(TransportPutIndexTemplateAction.class).toInstance(mock(TransportPutIndexTemplateAction.class));
            super.configure();
        }

        @Override
        protected void bindSchemas() {
            super.bindSchemas();
            SchemaInfo schemaInfo = mock(SchemaInfo.class);
            TableIdent userTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "users");
            TableInfo userTableInfo = TestingTableInfo.builder(userTableIdent, RowGranularity.DOC, shardRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.LONG, null)
                    .add("date", DataTypes.TIMESTAMP, null)
                    .add("text", DataTypes.STRING, null, ReferenceInfo.IndexType.ANALYZED)
                    .add("no_index", DataTypes.STRING, null, ReferenceInfo.IndexType.NO)
                    .addPrimaryKey("id")
                    .clusteredBy("id")
                    .build();
            when(userTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            TableIdent charactersTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "characters");
            TableInfo charactersTableInfo = TestingTableInfo.builder(charactersTableIdent, RowGranularity.DOC, shardRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.STRING, null)
                    .addPrimaryKey("id")
                    .clusteredBy("id")
                    .build();
            when(charactersTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            TableIdent partedTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "parted");
            TableInfo partedTableInfo = TestingTableInfo.builder(partedTableIdent, RowGranularity.DOC, partedRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.STRING, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .addPartitions(
                            new PartitionName("parted", new ArrayList<BytesRef>(){{add(null);}}).stringValue(), // TODO: invalid partition: null not valid as part of primary key
                            new PartitionName("parted", Arrays.asList(new BytesRef("0"))).stringValue(),
                            new PartitionName("parted", Arrays.asList(new BytesRef("123"))).stringValue()
                    )
                    .addPrimaryKey("id")
                    .addPrimaryKey("date")
                    .clusteredBy("id")
                    .build();
            when(partedTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            TableIdent emptyPartedTableIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "empty_parted");
            TableInfo emptyPartedTableInfo = TestingTableInfo.builder(partedTableIdent, RowGranularity.DOC, shardRouting)
                    .add("name", DataTypes.STRING, null)
                    .add("id", DataTypes.STRING, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .addPrimaryKey("id")
                    .addPrimaryKey("date")
                    .clusteredBy("id")
                    .build();
            TableIdent multiplePartitionedTableIdent= new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "multi_parted");
            TableInfo multiplePartitionedTableInfo = new TestingTableInfo.Builder(
                    multiplePartitionedTableIdent, RowGranularity.DOC, new Routing())
                    .add("id", DataTypes.INTEGER, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .add("num", DataTypes.LONG, null)
                    .add("obj", DataTypes.OBJECT, null, ColumnPolicy.DYNAMIC)
                    .add("obj", DataTypes.STRING, Arrays.asList("name"), true)
                            // add 3 partitions/simulate already done inserts
                    .addPartitions(
                            new PartitionName("multi_parted", Arrays.asList(new BytesRef("1395874800000"), new BytesRef("0"))).stringValue(),
                            new PartitionName("multi_parted", Arrays.asList(new BytesRef("1395961200000"), new BytesRef("-100"))).stringValue(),
                            new PartitionName("multi_parted", Arrays.asList(null, new BytesRef("-100"))).stringValue())
                    .build();
            TableIdent clusteredByParitionedIdent = new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "clustered_parted");
            TableInfo clusteredByPartitionedTableInfo = new TestingTableInfo.Builder(
                    multiplePartitionedTableIdent, RowGranularity.DOC, clusteredPartedRouting)
                    .add("id", DataTypes.INTEGER, null)
                    .add("date", DataTypes.TIMESTAMP, null, true)
                    .add("city", DataTypes.STRING, null)
                    .clusteredBy("city")
                    .addPartitions(
                            new PartitionName("clustered_parted", Arrays.asList(new BytesRef("1395874800000"))).stringValue(),
                            new PartitionName("clustered_parted", Arrays.asList(new BytesRef("1395961200000"))).stringValue())
                    .build();
            when(emptyPartedTableInfo.schemaInfo().name()).thenReturn(ReferenceInfos.DEFAULT_SCHEMA_NAME);
            when(schemaInfo.getTableInfo(charactersTableIdent.name())).thenReturn(charactersTableInfo);
            when(schemaInfo.getTableInfo(userTableIdent.name())).thenReturn(userTableInfo);
            when(schemaInfo.getTableInfo(partedTableIdent.name())).thenReturn(partedTableInfo);
            when(schemaInfo.getTableInfo(emptyPartedTableIdent.name())).thenReturn(emptyPartedTableInfo);
            when(schemaInfo.getTableInfo(multiplePartitionedTableIdent.name())).thenReturn(multiplePartitionedTableInfo);
            when(schemaInfo.getTableInfo(clusteredByParitionedIdent.name())).thenReturn(clusteredByPartitionedTableInfo);
            when(schemaInfo.getTableInfo(BaseAnalyzerTest.IGNORED_NESTED_TABLE_IDENT.name())).thenReturn(BaseAnalyzerTest.IGNORED_NESTED_TABLE_INFO);
            schemaBinder.addBinding(ReferenceInfos.DEFAULT_SCHEMA_NAME).toInstance(schemaInfo);
            schemaBinder.addBinding(SysSchemaInfo.NAME).toInstance(mockSysSchemaInfo());
            schemaBinder.addBinding(BlobSchemaInfo.NAME).toInstance(mockBlobSchemaInfo());
        }

        private SchemaInfo mockBlobSchemaInfo(){
            BlobSchemaInfo blobSchemaInfo = mock(BlobSchemaInfo.class);
            BlobTableInfo tableInfo = mock(BlobTableInfo.class);
            when(blobSchemaInfo.getTableInfo("screenshots")).thenReturn(tableInfo);
            when(tableInfo.schemaInfo()).thenReturn(blobSchemaInfo);
            return blobSchemaInfo;
        }

        private SchemaInfo mockSysSchemaInfo() {
            SchemaInfo schemaInfo = mock(SchemaInfo.class);
            when(schemaInfo.name()).thenReturn(SysSchemaInfo.NAME);
            when(schemaInfo.systemSchema()).thenReturn(true);

            TableInfo sysClusterTableInfo = TestingTableInfo.builder(
                    SysClusterTableInfo.IDENT,
                    // granularity < DOC is already handled different
                    // here we want a table with handlerSideRouting and DOC granularity.
                    RowGranularity.DOC,
                    SysClusterTableInfo.ROUTING
            ).schemaInfo(schemaInfo).add("name", DataTypes.STRING, null).schemaInfo(schemaInfo).build();
            when(schemaInfo.getTableInfo(sysClusterTableInfo.ident().name())).thenReturn(sysClusterTableInfo);

            TableInfo sysNodesTableInfo = TestingTableInfo.builder(
                    SysNodesTableInfo.IDENT,
                    RowGranularity.NODE,
                    nodesRouting)
                    .schemaInfo(schemaInfo)
                    .add("name", DataTypes.STRING, null).schemaInfo(schemaInfo).build();

            when(schemaInfo.getTableInfo(sysNodesTableInfo.ident().name())).thenReturn(sysNodesTableInfo);

            TableInfo sysShardsTableInfo = TestingTableInfo.builder(
                    SysShardsTableInfo.IDENT,
                    RowGranularity.SHARD,
                    nodesRouting
            ).add("id", DataTypes.INTEGER, null)
             .add("table_name", DataTypes.STRING, null)
             .schemaInfo(schemaInfo).build();
            when(schemaInfo.getTableInfo(sysShardsTableInfo.ident().name())).thenReturn(sysShardsTableInfo);
            when(schemaInfo.systemSchema()).thenReturn(true);
            return schemaInfo;
        }
    }

    private Plan plan(String statement) {
        return planner.plan(analyzer.analyze(SqlParser.createStatement(statement),
                new ParameterContext(new Object[0], new Object[0][], ReferenceInfos.DEFAULT_SCHEMA_NAME)));
    }

    @Test
    public void testGroupByWithAggregationStringLiteralArguments() {
        CollectNode collectNode = ((DistributedGroupBy) plan("select count('foo'), name from users group by name")).collectNode();
        // TODO: optimize to not collect literal
        //assertThat(collectNode.toCollect().size(), is(1));
        GroupProjection groupProjection = (GroupProjection) collectNode.projections().get(0);
        Aggregation aggregation = groupProjection.values().get(0);
    }

    @Test
    public void testGroupByWithAggregationPlan() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(*), name from users group by name");

        // distributed collect
        CollectNode collectNode = distributedGroupBy.collectNode();
        assertThat(collectNode.hasDistributingDownstreams(), is(true));
        assertThat(collectNode.downstreamNodes().size(), is(2));
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.DOC));
        assertThat(collectNode.executionNodes().size(), is(2));
        assertThat(collectNode.toCollect().size(), is(1));
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(collectNode.outputTypes().size(), is(2));
        assertEquals(DataTypes.STRING, collectNode.outputTypes().get(0));
        assertEquals(DataTypes.UNDEFINED, collectNode.outputTypes().get(1));

        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();

        assertThat(mergeNode.numUpstreams(), is(2));
        assertThat(mergeNode.executionNodes().size(), is(2));
        assertEquals(mergeNode.inputTypes(), collectNode.outputTypes());
        assertThat(mergeNode.projections().size(), is(2)); // for the default limit there is always a TopNProjection
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));

        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        GroupProjection groupProjection = (GroupProjection) mergeNode.projections().get(0);
        InputColumn inputColumn = (InputColumn) groupProjection.values().get(0).inputs().get(0);
        assertThat(inputColumn.index(), is(1));

        assertThat(mergeNode.outputTypes().size(), is(2));
        assertEquals(DataTypes.LONG, mergeNode.outputTypes().get(0));
        assertEquals(DataTypes.STRING, mergeNode.outputTypes().get(1));

        MergeNode localMerge = distributedGroupBy.localMergeNode();

        assertThat(localMerge.numUpstreams(), is(2));
        assertThat(localMerge.executionNodes().size(), is(1));
        assertThat(Iterables.getOnlyElement(localMerge.executionNodes()), is(LOCAL_NODE_ID));
        assertEquals(mergeNode.outputTypes(), localMerge.inputTypes());

        assertThat(localMerge.projections().get(0), instanceOf(TopNProjection.class));
        TopNProjection topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(topN.outputs().size(), is(2));

        assertEquals(DataTypes.LONG, localMerge.outputTypes().get(0));
        assertEquals(DataTypes.STRING, localMerge.outputTypes().get(1));

    }

    @Test
    public void testGetPlan() throws Exception {
        IterablePlan plan = (IterablePlan)  plan("select name from users where id = 1");
        Iterator<PlanNode> iterator = plan.iterator();
        ESGetNode node = (ESGetNode) iterator.next();
        assertThat(node.tableInfo().ident().name(), is("users"));
        assertThat(node.docKeys().getOnlyKey(), isDocKey(1L));
        assertThat(node.outputs().size(), is(1));
    }

    @Test
    public void testGetWithVersion() throws Exception{
        expectedException.expect(VersionInvalidException.class);
        expectedException.expectMessage("\"_version\" column is not valid in the WHERE clause of a SELECT statement");
        plan("select name from users where id = 1 and _version = 1");
    }

    @Test
    public void testGetPlanStringLiteral() throws Exception {
        IterablePlan plan = (IterablePlan) plan("select name from characters where id = 'one'");
        Iterator<PlanNode> iterator = plan.iterator();
        ESGetNode node = (ESGetNode) iterator.next();
        assertThat(node.tableInfo().ident().name(), is("characters"));
        assertThat(node.docKeys().getOnlyKey(), isDocKey("one"));
        assertFalse(iterator.hasNext());
        assertThat(node.outputs().size(), is(1));
    }

    @Test
    public void testGetPlanPartitioned() throws Exception {
        IterablePlan plan = (IterablePlan) plan("select name, date from parted where id = 'one' and date = 0");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode node = iterator.next();
        assertThat(node, instanceOf(ESGetNode.class));
        ESGetNode getNode = (ESGetNode) node;
        assertThat(getNode.tableInfo().ident().name(), is("parted"));
        assertThat(getNode.docKeys().getOnlyKey(), isDocKey("one", 0L));

        //is(new PartitionName("parted", Arrays.asList(new BytesRef("0"))).stringValue()));
        assertEquals(DataTypes.STRING, getNode.outputTypes().get(0));
        assertEquals(DataTypes.TIMESTAMP, getNode.outputTypes().get(1));
    }

    @Test
    public void testMultiGetPlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("select name from users where id in (1, 2)");
        Iterator<PlanNode> iterator = plan.iterator();
        ESGetNode node = (ESGetNode) iterator.next();
        assertThat(node.docKeys().size(), is(2));
        assertThat(node.docKeys(), containsInAnyOrder(isDocKey(1L), isDocKey(2L)));
    }

    @Test
    public void testDeletePlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("delete from users where id = 1");
        Iterator<PlanNode> iterator = plan.iterator();
        ESDeleteNode node = (ESDeleteNode) iterator.next();
        assertThat(node.tableInfo().ident().name(), is("users"));
        assertThat(node.docKeys().size(), is(1));
        assertThat(node.docKeys().get(0), isDocKey(1L));
        assertFalse(iterator.hasNext());
    }

    @Test
    public void testMultiDeletePlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("delete from users where id in (1, 2)");
        Iterator<PlanNode> iterator = plan.iterator();
        assertThat(iterator.next(), instanceOf(ESDeleteByQueryNode.class));
    }

    @Test
    public void testGroupByWithAggregationAndLimit() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(*), name from users group by name limit 1 offset 1");

        // distributed merge
        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));

        // limit must include offset because the real limit can only be applied on the handler
        // after all rows have been gathered.
        TopNProjection topN = (TopNProjection) mergeNode.projections().get(1);
        assertThat(topN.limit(), is(2));
        assertThat(topN.offset(), is(0));

        // local merge
        DQLPlanNode dqlPlanNode = distributedGroupBy.localMergeNode();
        assertThat(dqlPlanNode.projections().get(0), instanceOf(TopNProjection.class));
        topN = (TopNProjection) dqlPlanNode.projections().get(0);
        assertThat(topN.limit(), is(1));
        assertThat(topN.offset(), is(1));
        assertThat(topN.outputs().get(0), instanceOf(InputColumn.class));
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(topN.outputs().get(1), instanceOf(InputColumn.class));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));
    }

    @Test
    public void testGlobalAggregationPlan() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan("select count(name) from users");
        CollectNode collectNode = globalAggregate.collectNode();

        assertEquals(DataTypes.UNDEFINED, collectNode.outputTypes().get(0));
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.DOC));
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(AggregationProjection.class));

        MergeNode mergeNode = globalAggregate.mergeNode();

        assertEquals(DataTypes.UNDEFINED, mergeNode.inputTypes().get(0));
        assertEquals(DataTypes.LONG, mergeNode.outputTypes().get(0));
    }

    @Test
    public void testGroupByOnNodeLevel() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), name from sys.nodes group by name");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertEquals(DataTypes.STRING, collectNode.outputTypes().get(0));
        assertEquals(DataTypes.UNDEFINED, collectNode.outputTypes().get(1));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.numUpstreams(), is(2));
        assertThat(mergeNode.projections().size(), is(2));

        assertEquals(DataTypes.LONG, mergeNode.outputTypes().get(0));
        assertEquals(DataTypes.STRING, mergeNode.outputTypes().get(1));

        GroupProjection groupProjection = (GroupProjection) mergeNode.projections().get(0);
        assertThat(groupProjection.keys().size(), is(1));
        assertThat(((InputColumn) groupProjection.outputs().get(0)).index(), is(0));
        assertThat(groupProjection.outputs().get(1), is(instanceOf(Aggregation.class)));
        assertThat(((Aggregation) groupProjection.outputs().get(1)).functionIdent().name(), is("count"));
        assertThat(((Aggregation) groupProjection.outputs().get(1)).fromStep(), is(Aggregation.Step.PARTIAL));
        assertThat(((Aggregation)groupProjection.outputs().get(1)).toStep(), is(Aggregation.Step.FINAL));

        TopNProjection projection = (TopNProjection) mergeNode.projections().get(1);
        assertThat(((InputColumn) projection.outputs().get(0)).index(), is(1));
        assertThat(((InputColumn) projection.outputs().get(1)).index(), is(0));

    }

    @Test
    public void testShardPlan() throws Exception {
        QueryAndFetch planNode = (QueryAndFetch) plan("select id from sys.shards order by id limit 10");
        CollectNode collectNode = planNode.collectNode();

        assertEquals(DataTypes.INTEGER, collectNode.outputTypes().get(0));
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.SHARD));

        MergeNode mergeNode = planNode.localMergeNode();

        assertThat(mergeNode.inputTypes().size(), is(1));
        assertEquals(DataTypes.INTEGER, mergeNode.inputTypes().get(0));
        assertThat(mergeNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.INTEGER, mergeNode.outputTypes().get(0));

        assertThat(mergeNode.numUpstreams(), is(2));
    }

    @Test
    public void testQueryThenFetchPlan() throws Exception {
        Plan plan = plan("select name from users where name = 'x' order by id limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();
        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        Projection lastProjection = mergeNode.finalProjection().get();
        assertThat(lastProjection, instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection) lastProjection;
        assertThat(fetchProjection.outputs().size(), is(1));
        assertThat(fetchProjection.outputs().get(0), isReference("_doc['name']"));
    }

    @Test
    public void testQueryThenFetchPlanNoFetch() throws Exception {
        // testing that a fetch projection is not added if all output symbols are included
        // at the orderBy symbols
        Plan plan = plan("select name from users where name = 'x' order by name limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();
        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        Projection lastProjection = mergeNode.finalProjection().get();
        assertThat(lastProjection, instanceOf(TopNProjection.class));
        TopNProjection topNProjection = (TopNProjection) lastProjection;
        assertThat(topNProjection.outputs().size(), is(1));
    }

    @Test
    public void testQueryThenFetchPlanDefaultLimit() throws Exception {
        QueryThenFetch plan = (QueryThenFetch)plan("select name from users");
        CollectNode collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(Constants.DEFAULT_SELECT_LIMIT));

        MergeNode mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        TopNProjection topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
        assertThat(topN.offset(), is(0));
        assertNull(topN.orderBy());

        FetchProjection fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(FetchProjector.NO_BULK_REQUESTS));

        // with offset
        plan = (QueryThenFetch)plan("select name from users offset 20");
        collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(Constants.DEFAULT_SELECT_LIMIT + 20));

        mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
        assertThat(topN.offset(), is(20));
        assertNull(topN.orderBy());

        fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(FetchProjector.NO_BULK_REQUESTS));
    }

    @Test
    public void testQueryThenFetchPlanHighLimit() throws Exception {
        QueryThenFetch plan = (QueryThenFetch)plan("select name from users limit 100000");
        CollectNode collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(100_000));

        MergeNode mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        TopNProjection topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(100_000));
        assertThat(topN.offset(), is(0));
        assertNull(topN.orderBy());

        FetchProjection fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(Constants.DEFAULT_SELECT_LIMIT));

        // with offset
        plan = (QueryThenFetch)plan("select name from users limit 100000 offset 20");
        collectNode = plan.collectNode();
        assertThat(collectNode.limit(), is(100_000 + 20));

        mergeNode = plan.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.finalProjection().get(), instanceOf(FetchProjection.class));
        topN = (TopNProjection)mergeNode.projections().get(0);
        assertThat(topN.limit(), is(100_000));
        assertThat(topN.offset(), is(20));
        assertNull(topN.orderBy());

        fetchProjection = (FetchProjection)mergeNode.projections().get(1);
        assertThat(fetchProjection.bulkSize(), is(Constants.DEFAULT_SELECT_LIMIT));
    }

    @Test
    public void testQueryThenFetchPlanPartitioned() throws Exception {
        Plan plan = plan("select id, name, date from parted where date > 0 and name = 'x' order by id limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();

        List<String> indices = new ArrayList<>();
        Map<String, Map<String, List<Integer>>> locations = collectNode.routing().locations();
        for (Map.Entry<String, Map<String, List<Integer>>> entry : locations.entrySet()) {
            indices.addAll(entry.getValue().keySet());
        }
        assertThat(indices, Matchers.contains(
                new PartitionName("parted", Arrays.asList(new BytesRef("123"))).stringValue()));

        assertTrue(collectNode.whereClause().hasQuery());
        assertTrue(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(3));
    }

    @Test
    public void testQueryThenFetchPlanFunction() throws Exception {
        Plan plan = plan("select format('Hi, my name is %s', name), name from users where name = 'x' order by id limit 10");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();

        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(2));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(1));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        Projection lastProjection = mergeNode.finalProjection().get();
        assertThat(lastProjection, instanceOf(FetchProjection.class));
        FetchProjection fetchProjection = (FetchProjection) lastProjection;
        assertThat(fetchProjection.outputs().size(), is(2));
        assertThat(fetchProjection.outputs().get(0), isFunction("format"));
        assertThat(fetchProjection.outputs().get(1), isReference("_doc['name']"));

    }

    @Test
    public void testInsertPlan() throws Exception {
        Upsert plan = (Upsert) plan("insert into users (id, name) values (42, 'Deep Thought')");

        assertThat(plan.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) plan.nodes().get(0)).iterator().next();
        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));

        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode)next;

        assertThat(updateNode.insertColumns().length, is(2));
        Reference idRef = updateNode.insertColumns()[0];
        assertThat(idRef.ident().columnIdent().fqn(), is("id"));
        Reference nameRef = updateNode.insertColumns()[1];
        assertThat(nameRef.ident().columnIdent().fqn(), is("name"));

        assertThat(updateNode.items().size(), is(1));
        SymbolBasedUpsertByIdNode.Item item = updateNode.items().get(0);
        assertThat(item.index(), is("users"));
        assertThat(item.id(), is("42"));
        assertThat(item.routing(), is("42"));

        assertThat(item.insertValues().length, is(2));
        assertThat((Long)item.insertValues()[0], is(42L));
        assertThat((BytesRef) item.insertValues()[1], is(new BytesRef("Deep Thought")));
    }

    @Test
    public void testInsertPlanMultipleValues() throws Exception {
        Upsert plan = (Upsert) plan("insert into users (id, name) values (42, 'Deep Thought'), (99, 'Marvin')");

        assertThat(plan.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) plan.nodes().get(0)).iterator().next();
        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));

        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode)next;

        assertThat(updateNode.insertColumns().length, is(2));
        Reference idRef = updateNode.insertColumns()[0];
        assertThat(idRef.ident().columnIdent().fqn(), is("id"));
        Reference nameRef = updateNode.insertColumns()[1];
        assertThat(nameRef.ident().columnIdent().fqn(), is("name"));

        assertThat(updateNode.items().size(), is(2));

        SymbolBasedUpsertByIdNode.Item item1 = updateNode.items().get(0);
        assertThat(item1.index(), is("users"));
        assertThat(item1.id(), is("42"));
        assertThat(item1.routing(), is("42"));
        assertThat(item1.insertValues().length, is(2));
        assertThat((Long)item1.insertValues()[0], is(42L));
        assertThat((BytesRef)item1.insertValues()[1], is(new BytesRef("Deep Thought")));

        SymbolBasedUpsertByIdNode.Item item2 = updateNode.items().get(1);
        assertThat(item2.index(), is("users"));
        assertThat(item2.id(), is("99"));
        assertThat(item2.routing(), is("99"));
        assertThat(item2.insertValues().length, is(2));
        assertThat((Long)item2.insertValues()[0], is(99L));
        assertThat((BytesRef) item2.insertValues()[1], is(new BytesRef("Marvin")));
    }

    @Test
    public void testCountDistinctPlan() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan("select count(distinct name) from users");

        CollectNode collectNode = globalAggregate.collectNode();
        Projection projection = collectNode.projections().get(0);
        assertThat(projection, instanceOf(AggregationProjection.class));
        AggregationProjection aggregationProjection = (AggregationProjection)projection;
        assertThat(aggregationProjection.aggregations().size(), is(1));

        Aggregation aggregation = aggregationProjection.aggregations().get(0);
        assertThat(aggregation.toStep(), is(Aggregation.Step.PARTIAL));
        Symbol aggregationInput = aggregation.inputs().get(0);
        assertThat(aggregationInput.symbolType(), is(SymbolType.INPUT_COLUMN));

        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(((Reference) collectNode.toCollect().get(0)).info().ident().columnIdent().name(), is("name"));

        MergeNode mergeNode = globalAggregate.mergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        Projection projection1 = mergeNode.projections().get(1);
        assertThat(projection1, instanceOf(TopNProjection.class));
        Symbol collection_count = projection1.outputs().get(0);
        assertThat(collection_count, instanceOf(Function.class));
    }

    @Test
    public void testNonDistributedGroupByOnClusteredColumn() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), id from users group by id limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
    }

    @Test
    public void testNonDistributedGroupByOnClusteredColumnSorted() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), id from users group by id order by 1 desc nulls last limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(((TopNProjection)collectNode.projections().get(1)).orderBy().size(), is(1));

        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        TopNProjection projection = (TopNProjection)mergeNode.projections().get(0);
        assertThat(projection.orderBy(), is(nullValue()));
        assertThat(mergeNode.sortedInputOutput(), is(true));
        assertThat(mergeNode.orderByIndices().length, is(1));
        assertThat(mergeNode.orderByIndices()[0], is(0));
        assertThat(mergeNode.reverseFlags()[0], is(true));
        assertThat(mergeNode.nullsFirst()[0], is(false));
    }

    @Test
    public void testNonDistributedGroupByOnClusteredColumnSortedScalar() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*) + 1, id from users group by id order by count(*) + 1 limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(((TopNProjection)collectNode.projections().get(1)).orderBy().size(), is(1));

        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        TopNProjection projection = (TopNProjection)mergeNode.projections().get(0);
        assertThat(projection.orderBy(), is(nullValue()));
        assertThat(mergeNode.sortedInputOutput(), is(true));
        assertThat(mergeNode.orderByIndices().length, is(1));
        assertThat(mergeNode.orderByIndices()[0], is(0));
        assertThat(mergeNode.reverseFlags()[0], is(false));
        assertThat(mergeNode.nullsFirst()[0], is(nullValue()));
    }

    @Test
    public void testNoDistributedGroupByOnAllPrimaryKeys() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*), id, date from empty_parted group by id, date limit 20");
        CollectNode collectNode = planNode.collectNode();
        assertFalse(collectNode.hasDistributingDownstreams());
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(collectNode.projections().get(0).requiredGranularity(), is(RowGranularity.SHARD));
        assertThat(collectNode.projections().get(1), instanceOf(TopNProjection.class));
        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        assertThat(mergeNode.projections().get(0), instanceOf(TopNProjection.class));
    }

    @Test
    public void testNonDistributedGroupByAggregationsWrappedInScalar() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select (count(*) + 1), id from empty_parted group by id");
        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));

        TopNProjection topNProjection = (TopNProjection) planNode.reducerMergeNode().projections().get(1);
        assertThat(topNProjection.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
        assertThat(topNProjection.offset(), is(0));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        assertThat(mergeNode.projections().get(0), instanceOf(TopNProjection.class));
    }

    @Test
    public void testGroupByWithOrderOnAggregate() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(*), name from users group by name order by count(*)");

        // sort is on handler because there is no limit/offset
        // handler
        MergeNode mergeNode = distributedGroupBy.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));

        TopNProjection topNProjection = (TopNProjection)mergeNode.projections().get(0);
        Symbol orderBy = topNProjection.orderBy().get(0);
        assertThat(orderBy, instanceOf(InputColumn.class));

        assertThat(orderBy.valueType(), Is.<DataType>is(DataTypes.LONG));
    }

    @Test
    public void testHandlerSideRouting() throws Exception {
        // just testing the dispatching here.. making sure it is not a ESSearchNode
        QueryAndFetch plan = (QueryAndFetch) plan("select * from sys.cluster");
    }

    @Test
    public void testHandlerSideRoutingGroupBy() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select count(*) from sys.cluster group by name");
        // just testing the dispatching here.. making sure it is not a ESSearchNode
        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(collectNode.toCollect().size(), is(1));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));
    }

    @Test
    public void testCountDistinctWithGroupBy() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select count(distinct id), name from users group by name order by count(distinct id)");
        CollectNode collectNode = distributedGroupBy.collectNode();

        // collect
        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(collectNode.toCollect().size(), is(2));
        assertThat(((Reference)collectNode.toCollect().get(0)).info().ident().columnIdent().name(), is("id"));
        assertThat(((Reference)collectNode.toCollect().get(1)).info().ident().columnIdent().name(), is("name"));
        Projection projection = collectNode.projections().get(0);
        assertThat(projection, instanceOf(GroupProjection.class));
        GroupProjection groupProjection = (GroupProjection)projection;
        Symbol groupKey = groupProjection.keys().get(0);
        assertThat(groupKey, instanceOf(InputColumn.class));
        assertThat(((InputColumn)groupKey).index(), is(1));
        assertThat(groupProjection.values().size(), is(1));

        Aggregation aggregation = groupProjection.values().get(0);
        assertThat(aggregation.toStep(), is(Aggregation.Step.PARTIAL));
        Symbol aggregationInput = aggregation.inputs().get(0);
        assertThat(aggregationInput.symbolType(), is(SymbolType.INPUT_COLUMN));



        // reducer
        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        Projection groupProjection1 = mergeNode.projections().get(0);
        assertThat(groupProjection1, instanceOf(GroupProjection.class));
        groupProjection = (GroupProjection)groupProjection1;
        assertThat(groupProjection.keys().get(0), instanceOf(InputColumn.class));
        assertThat(((InputColumn)groupProjection.keys().get(0)).index(), is(0));

        assertThat(groupProjection.values().get(0), instanceOf(Aggregation.class));
        Aggregation aggregationStep2 = groupProjection.values().get(0);
        assertThat(aggregationStep2.toStep(), is(Aggregation.Step.FINAL));

        TopNProjection topNProjection = (TopNProjection)mergeNode.projections().get(1);
        Symbol collection_count = topNProjection.outputs().get(0);
        assertThat(collection_count, instanceOf(Function.class));


        // handler
        MergeNode localMergeNode = distributedGroupBy.localMergeNode();
        assertThat(localMergeNode.projections().size(), is(1));
        Projection localTopN = localMergeNode.projections().get(0);
        assertThat(localTopN, instanceOf(TopNProjection.class));
    }

    @Test
    public void testUpdateByQueryPlan() throws Exception {
        Upsert plan = (Upsert) plan("update users set name='Vogon lyric fan'");
        assertThat(plan.nodes().size(), is(1));

        CollectAndMerge planNode = (CollectAndMerge) plan.nodes().get(0);

        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.routing(), is(shardRouting));
        assertFalse(collectNode.whereClause().noMatch());
        assertFalse(collectNode.whereClause().hasQuery());
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(UpdateProjection.class));
        assertThat(collectNode.toCollect().size(), is(1));
        assertThat(collectNode.toCollect().get(0), instanceOf(Reference.class));
        assertThat(((Reference)collectNode.toCollect().get(0)).info().ident().columnIdent().fqn(), is("_uid"));

        UpdateProjection updateProjection = (UpdateProjection)collectNode.projections().get(0);
        assertThat(updateProjection.uidSymbol(), instanceOf(InputColumn.class));

        assertThat(updateProjection.assignmentsColumns()[0], is("name"));
        Symbol symbol = updateProjection.assignments()[0];
        assertThat(symbol, isLiteral("Vogon lyric fan", DataTypes.STRING));

        MergeNode mergeNode = planNode.localMergeNode();
        assertThat(mergeNode.projections().size(), is(1));
        assertThat(mergeNode.projections().get(0), instanceOf(AggregationProjection.class));

        assertThat(mergeNode.outputTypes().size(), is(1));
    }

    @Test
    public void testUpdateByIdPlan() throws Exception {
        Upsert planNode = (Upsert) plan("update users set name='Vogon lyric fan' where id=1");
        assertThat(planNode.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) planNode.nodes().get(0)).iterator().next();
        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));

        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode) next;
        assertThat(updateNode.items().size(), is(1));

        assertThat(updateNode.updateColumns()[0], is("name"));

        SymbolBasedUpsertByIdNode.Item item = updateNode.items().get(0);
        assertThat(item.index(), is("users"));
        assertThat(item.id(), is("1"));

        Symbol symbol = item.updateAssignments()[0];
        assertThat(symbol, isLiteral("Vogon lyric fan", DataTypes.STRING));
    }

    @Test
    public void testUpdatePlanWithMultiplePrimaryKeyValues() throws Exception {
        Upsert planNode =  (Upsert) plan("update users set name='Vogon lyric fan' where id in (1,2,3)");
        assertThat(planNode.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) planNode.nodes().get(0)).iterator().next();

        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));
        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode) next;

        List<String> ids = new ArrayList<>(3);
        for (SymbolBasedUpsertByIdNode.Item item : updateNode.items()) {
            ids.add(item.id());
            assertThat(item.updateAssignments().length, is(1));
            assertThat(item.updateAssignments()[0], isLiteral("Vogon lyric fan", DataTypes.STRING));
        }

        assertThat(ids, containsInAnyOrder("1", "2", "3"));
    }

    @Test
    public void testUpdatePlanWithMultiplePrimaryKeyValuesPartitioned() throws Exception {
        Upsert planNode =  (Upsert) plan("update parted set name='Vogon lyric fan' where " +
                "(id=2 and date = 0) OR" +
                "(id=3 and date=123)");
        assertThat(planNode.nodes().size(), is(1));

        PlanNode next = ((IterablePlan) planNode.nodes().get(0)).iterator().next();

        assertThat(next, instanceOf(SymbolBasedUpsertByIdNode.class));
        SymbolBasedUpsertByIdNode updateNode = (SymbolBasedUpsertByIdNode) next;

        List<String> partitions = new ArrayList<>(2);
        List<String> ids = new ArrayList<>(2);
        for (SymbolBasedUpsertByIdNode.Item item : updateNode.items()) {
            partitions.add(item.index());
            ids.add(item.id());
            assertThat(item.updateAssignments().length, is(1));
            assertThat(item.updateAssignments()[0], isLiteral("Vogon lyric fan", DataTypes.STRING));
        }
        assertThat(ids, containsInAnyOrder("AgEyATA=", "AgEzAzEyMw==")); // multi primary key - values concatenated and base64'ed
        assertThat(partitions, containsInAnyOrder(".partitioned.parted.04130", ".partitioned.parted.04232chj"));
    }

    @Test
    public void testCopyFromPlan() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users from '/path/to/file.extension'");
        assertThat(plan.collectNode(), instanceOf(FileUriCollectNode.class));

        FileUriCollectNode collectNode = (FileUriCollectNode)plan.collectNode();
        assertThat((BytesRef) ((Literal) collectNode.targetUri()).value(),
                is(new BytesRef("/path/to/file.extension")));
    }

    @Test
    public void testCopyFromNumReadersSetting() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users from '/path/to/file.extension' with (num_readers=1)");
        assertThat(plan.collectNode(), instanceOf(FileUriCollectNode.class));
        FileUriCollectNode collectNode = (FileUriCollectNode) plan.collectNode();
        assertThat(collectNode.executionNodes().size(), is(1));
    }

    @Test
    public void testCopyFromPlanWithParameters() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users from '/path/to/file.ext' with (bulk_size=30, compression='gzip', shared=true)");
        assertThat(plan.collectNode(), instanceOf(FileUriCollectNode.class));
        FileUriCollectNode collectNode = (FileUriCollectNode)plan.collectNode();
        SourceIndexWriterProjection indexWriterProjection = (SourceIndexWriterProjection) collectNode.projections().get(0);
        assertThat(indexWriterProjection.bulkActions(), is(30));
        assertThat(collectNode.compression(), is("gzip"));
        assertThat(collectNode.sharedStorage(), is(true));

        // verify defaults:
        plan = (CollectAndMerge) plan("copy users from '/path/to/file.ext'");
        collectNode = (FileUriCollectNode)plan.collectNode();
        assertNull(collectNode.compression());
        assertNull(collectNode.sharedStorage());
    }

    @Test
    public void testCopyToWithColumnsReferenceRewrite() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy users (name) to '/file.ext'");
        CollectNode node = plan.collectNode();
        Reference nameRef = (Reference)node.toCollect().get(0);

        assertThat(nameRef.info().ident().columnIdent().name(), is(DocSysColumns.DOC.name()));
        assertThat(nameRef.info().ident().columnIdent().path().get(0), is("name"));
    }

    @Test
    public void testCopyToWithNonExistentPartitionClause() throws Exception {
        CollectAndMerge plan = (CollectAndMerge) plan("copy parted partition (date=0) to '/foo.txt' ");
        assertFalse(plan.collectNode().routing().hasLocations());
    }

    @Test (expected = IllegalArgumentException.class)
    public void testCopyFromPlanWithInvalidParameters() throws Exception {
        plan("copy users from '/path/to/file.ext' with (bulk_size=-28)");
    }

    @Test
    public void testShardSelect() throws Exception {
        QueryAndFetch planNode = (QueryAndFetch) plan("select id from sys.shards");
        CollectNode collectNode = planNode.collectNode();
        assertTrue(collectNode.isRouted());
        assertThat(collectNode.maxRowGranularity(), is(RowGranularity.SHARD));
    }

    @Test
    public void testDropTable() throws Exception {
        IterablePlan plan = (IterablePlan) plan("drop table users");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(DropTableNode.class));

        DropTableNode node = (DropTableNode) planNode;
        assertThat(node.tableInfo().ident().name(), is("users"));
    }

    @Test
    public void testDropTableIfExistsWithUnknownSchema() throws Exception {
        Plan plan = plan("drop table if exists unknown_schema.unknwon_table");
        assertThat(plan, instanceOf(NoopPlan.class));
    }

    @Test
    public void testDropTableIfExists() throws Exception {
        IterablePlan plan = (IterablePlan) plan("drop table if exists users");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(DropTableNode.class));

        DropTableNode node = (DropTableNode) planNode;
        assertThat(node.tableInfo().ident().name(), is("users"));
    }

    @Test
    public void testDropTableIfExistsNonExistentTableCreatesNoop() throws Exception {
        Plan plan = plan("drop table if exists groups");
        assertThat(plan, instanceOf(NoopPlan.class));
    }


    @Test
    public void testDropPartitionedTable() throws Exception {
        IterablePlan plan = (IterablePlan) plan("drop table parted");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();

        assertThat(planNode, instanceOf(DropTableNode.class));
        DropTableNode node = (DropTableNode) planNode;
        assertThat(node.tableInfo().ident().name(), is("parted"));

        assertFalse(iterator.hasNext());
    }

    @Test
    public void testDropBlobTableIfExistsCreatesIterablePlan() throws Exception {
        Plan plan = plan("drop blob table if exists screenshots");
        assertThat(plan, instanceOf(IterablePlan.class));
    }

    @Test
    public void testDropNonExistentBlobTableCreatesNoop() throws Exception {
        Plan plan = plan("drop blob table if exists unknown");
        assertThat(plan, instanceOf(NoopPlan.class));
    }

    @Test
    public void testGlobalCountPlan() throws Exception {
        CountPlan plan = (CountPlan) plan("select count(*) from users");
        assertThat(plan, instanceOf(PlannedAnalyzedRelation.class));

        assertThat(plan.countNode().whereClause(), equalTo(WhereClause.MATCH_ALL));

        assertThat(plan.mergeNode().projections().size(), is(1));
        assertThat(plan.mergeNode().projections().get(0), instanceOf(AggregationProjection.class));
    }

    @Test
    public void testSetPlan() throws Exception {
        IterablePlan plan = (IterablePlan) plan("set GLOBAL PERSISTENT stats.jobs_log_size=1024");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(ESClusterUpdateSettingsNode.class));

        ESClusterUpdateSettingsNode node = (ESClusterUpdateSettingsNode) planNode;
        // set transient settings too when setting persistent ones
        assertThat(node.transientSettings().toDelimitedString(','), is("stats.jobs_log_size=1024,"));
        assertThat(node.persistentSettings().toDelimitedString(','), is("stats.jobs_log_size=1024,"));

        plan = (IterablePlan)  plan("set GLOBAL TRANSIENT stats.enabled=false,stats.jobs_log_size=0");
        iterator = plan.iterator();
        planNode = iterator.next();
        assertThat(planNode, instanceOf(ESClusterUpdateSettingsNode.class));

        node = (ESClusterUpdateSettingsNode) planNode;
        assertThat(node.persistentSettings().getAsMap().size(), is(0));
        assertThat(node.transientSettings().toDelimitedString(','), is("stats.enabled=false,stats.jobs_log_size=0,"));
    }

    @Test
    public void testInsertFromSubQueryNonDistributedGroupBy() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, name) (select name, count(*) from sys.nodes group by name)");
        NonDistributedGroupBy nonDistributedGroupBy = (NonDistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = nonDistributedGroupBy.localMergeNode();
        assertThat(mergeNode.projections().size(), is(3));
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(mergeNode.projections().get(2), instanceOf(ColumnIndexWriterProjection.class));
        assertThat(planNode.handlerMergeNode().isPresent(), is(false));
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryDistributedGroupByWithLimit() throws Exception {
        IterablePlan plan = (IterablePlan) plan("insert into users (id, name) (select name, count(*) from users group by name order by name limit 10)");
        Iterator<PlanNode> iterator = plan.iterator();
        PlanNode planNode = iterator.next();
        assertThat(planNode, instanceOf(CollectNode.class));

        planNode = iterator.next();
        assertThat(planNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode)planNode;
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));

        planNode = iterator.next();
        assertThat(planNode, instanceOf(MergeNode.class));
        mergeNode = (MergeNode)planNode;
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(0), instanceOf(TopNProjection.class));
        assertThat(((TopNProjection)mergeNode.projections().get(0)).limit(), is(10));

        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));

        assertThat(iterator.hasNext(), is(false));
    }

    @Test
    public void testInsertFromSubQueryDistributedGroupByWithoutLimit() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, name) (select name, count(*) from users group by name)");
        DistributedGroupBy groupBy = (DistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = groupBy.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)mergeNode.projections().get(1);
        assertThat(projection.primaryKeys().size(), is(1));
        assertThat(projection.primaryKeys().get(0).fqn(), is("id"));
        assertThat(projection.columnReferences().size(), is(2));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("id"));
        assertThat(projection.columnReferences().get(1).ident().columnIdent().fqn(), is("name"));

        assertNotNull(projection.clusteredByIdent());
        assertThat(projection.clusteredByIdent().fqn(), is("id"));
        assertThat(projection.tableIdent().fqn(), is("users"));
        assertThat(projection.partitionedBySymbols().isEmpty(), is(true));

        MergeNode localMergeNode = planNode.handlerMergeNode().get();
        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.finalProjection().get().outputs().size(), is(1));

    }

    @Test
    public void testInsertFromSubQueryDistributedGroupByPartitioned() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into parted (id, date) (select id, date from users group by id, date)");
        DistributedGroupBy groupBy = (DistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = groupBy.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)mergeNode.projections().get(1);
        assertThat(projection.primaryKeys().size(), is(2));
        assertThat(projection.primaryKeys().get(0).fqn(), is("id"));
        assertThat(projection.primaryKeys().get(1).fqn(), is("date"));

        assertThat(projection.columnReferences().size(), is(1));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("id"));

        assertThat(projection.partitionedBySymbols().size(), is(1));
        assertThat(((InputColumn) projection.partitionedBySymbols().get(0)).index(), is(1));

        assertNotNull(projection.clusteredByIdent());
        assertThat(projection.clusteredByIdent().fqn(), is("id"));
        assertThat(projection.tableIdent().fqn(), is("parted"));

        MergeNode localMergeNode = planNode.handlerMergeNode().get();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.finalProjection().get().outputs().size(), is(1));

    }

    @Test
    public void testInsertFromSubQueryGlobalAggregate() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (name, id) (select arbitrary(name), count(*) from users)");
        GlobalAggregate globalAggregate = (GlobalAggregate)planNode.innerPlan();
        MergeNode mergeNode = globalAggregate.mergeNode();
        assertThat(mergeNode.projections().size(), is(3));
        assertThat(mergeNode.projections().get(1), instanceOf(TopNProjection.class));
        assertThat(mergeNode.projections().get(2), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)mergeNode.projections().get(2);

        assertThat(projection.columnReferences().size(), is(2));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("name"));
        assertThat(projection.columnReferences().get(1).ident().columnIdent().fqn(), is("id"));

        assertThat(projection.columnSymbols().size(), is(2));
        assertThat(((InputColumn)projection.columnSymbols().get(0)).index(), is(0));
        assertThat(((InputColumn)projection.columnSymbols().get(1)).index(), is(1));

        assertNotNull(projection.clusteredByIdent());
        assertThat(projection.clusteredByIdent().fqn(), is("id"));
        assertThat(projection.tableIdent().fqn(), is("users"));
        assertThat(projection.partitionedBySymbols().isEmpty(), is(true));

        assertThat(planNode.handlerMergeNode().isPresent(), is(false));
    }

    @Test
    public void testInsertFromSubQueryESGet() throws Exception {
        // doesn't use ESGetNode but CollectNode.
        // Round-trip to handler can be skipped by writing from the shards directly
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (date, id, name) (select date, id, name from users where id=1)");
        QueryAndFetch queryAndFetch = (QueryAndFetch)planNode.innerPlan();
        CollectNode collectNode = queryAndFetch.collectNode();

        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(ColumnIndexWriterProjection.class));
        ColumnIndexWriterProjection projection = (ColumnIndexWriterProjection)collectNode.projections().get(0);

        assertThat(projection.columnReferences().size(), is(3));
        assertThat(projection.columnReferences().get(0).ident().columnIdent().fqn(), is("date"));
        assertThat(projection.columnReferences().get(1).ident().columnIdent().fqn(), is("id"));
        assertThat(projection.columnReferences().get(2).ident().columnIdent().fqn(), is("name"));
        assertThat(((InputColumn) projection.ids().get(0)).index(), is(1));
        assertThat(((InputColumn)projection.clusteredBy()).index(), is(1));
        assertThat(projection.partitionedBySymbols().isEmpty(), is(true));

        assertThat(planNode.handlerMergeNode().isPresent(), is(true));
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryWithLimit() throws Exception {
        Plan plan = plan("insert into users (date, id, name) (select date, id, name from users limit 10)");
        assertThat(plan, instanceOf(QueryThenFetch.class));
        CollectNode collectNode = ((QueryThenFetch) plan).collectNode();
        assertTrue(collectNode.whereClause().hasQuery());
        assertFalse(collectNode.isPartitioned());

        DQLPlanNode resultNode = ((QueryThenFetch) plan).resultNode();
        assertThat(resultNode.outputTypes().size(), is(1));
        assertEquals(DataTypes.STRING, resultNode.outputTypes().get(0));

        assertThat(resultNode, instanceOf(MergeNode.class));
        MergeNode mergeNode = (MergeNode) resultNode;
        assertTrue(mergeNode.finalProjection().isPresent());

        assertThat(mergeNode.projections().size(), is(2));
        assertThat(mergeNode.projections().get(1), instanceOf(ColumnIndexWriterProjection.class));
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryWithOffset() throws Exception {
        plan("insert into users (date, id, name) (select date, id, name from users offset 10)");
    }

    @Test (expected = UnsupportedFeatureException.class)
    public void testInsertFromSubQueryWithOrderBy() throws Exception {
        plan("insert into users (date, id, name) (select date, id, name from users order by id)");
    }

    @Test
    public void testInsertFromSubQueryWithoutLimit() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (date, id, name) (select date, id, name from users)");
        QueryAndFetch queryAndFetch = (QueryAndFetch)planNode.innerPlan();
        CollectNode collectNode = queryAndFetch.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(ColumnIndexWriterProjection.class));
        assertNull(queryAndFetch.localMergeNode());

        MergeNode localMergeNode = planNode.handlerMergeNode().get();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
    }

    @Test
    public void testGroupByHaving() throws Exception {
        DistributedGroupBy distributedGroupBy = (DistributedGroupBy) plan(
                "select avg(date), name from users group by name having min(date) > '1970-01-01'");
        CollectNode collectNode = distributedGroupBy.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));

        MergeNode mergeNode = distributedGroupBy.reducerMergeNode();

        assertThat(mergeNode.projections().size(), is(3));

        // grouping
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        GroupProjection groupProjection = (GroupProjection)mergeNode.projections().get(0);
        assertThat(groupProjection.values().size(), is(2));

        // filter the having clause
        assertThat(mergeNode.projections().get(1), instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection)mergeNode.projections().get(1);

        // apply the default limit
        assertThat(mergeNode.projections().get(2), instanceOf(TopNProjection.class));
        TopNProjection topN = (TopNProjection)mergeNode.projections().get(2);
        assertThat(topN.outputs().get(0).valueType(), Is.<DataType>is(DataTypes.DOUBLE));
        assertThat(topN.outputs().get(1).valueType(), Is.<DataType>is(DataTypes.STRING));
        assertThat(topN.limit(), is(Constants.DEFAULT_SELECT_LIMIT));
    }

    @Test
    public void testInsertFromQueryWithPartitionedColumn() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, date) (select id, date from parted)");
        QueryAndFetch queryAndFetch = (QueryAndFetch)planNode.innerPlan();
        CollectNode collectNode = queryAndFetch.collectNode();
        List<Symbol> toCollect = collectNode.toCollect();
        assertThat(toCollect.size(), is(2));
        assertThat(toCollect.get(0), isFunction("toLong"));
        assertThat(((Function) toCollect.get(0)).arguments().get(0), isReference("_doc['id']"));
        assertThat((Reference) toCollect.get(1), equalTo(new Reference(new ReferenceInfo(
                new ReferenceIdent(new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "parted"), "date"), RowGranularity.PARTITION, DataTypes.TIMESTAMP))));
    }

    @Test
    public void testGroupByHavingInsertInto() throws Exception {
        InsertFromSubQuery planNode = (InsertFromSubQuery) plan(
                "insert into users (id, name) (select name, count(*) from users group by name having count(*) > 3)");
        DistributedGroupBy groupByNode = (DistributedGroupBy)planNode.innerPlan();
        MergeNode mergeNode = groupByNode.reducerMergeNode();
        assertThat(mergeNode.projections().size(), is(3));
        assertThat(mergeNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(mergeNode.projections().get(1), instanceOf(FilterProjection.class));
        assertThat(mergeNode.projections().get(2), instanceOf(ColumnIndexWriterProjection.class));

        FilterProjection filterProjection = (FilterProjection)mergeNode.projections().get(1);
        assertThat(filterProjection.outputs().size(), is(2));
        assertThat(filterProjection.outputs().get(0), instanceOf(InputColumn.class));
        assertThat(filterProjection.outputs().get(1), instanceOf(InputColumn.class));

        InputColumn inputColumn = (InputColumn)filterProjection.outputs().get(0);
        assertThat(inputColumn.index(), is(0));
        inputColumn = (InputColumn)filterProjection.outputs().get(1);
        assertThat(inputColumn.index(), is(1));
        MergeNode localMergeNode = planNode.handlerMergeNode().get();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.finalProjection().get().outputs().size(), is(1));

    }

    @Test
    public void testGroupByHavingNonDistributed() throws Exception {
        NonDistributedGroupBy planNode = (NonDistributedGroupBy) plan(
                "select id from users group by id having id > 0");
        CollectNode collectNode = planNode.collectNode();
        assertThat(collectNode.projections().size(), is(2));
        assertThat(collectNode.projections().get(0), instanceOf(GroupProjection.class));
        assertThat(collectNode.projections().get(1), instanceOf(FilterProjection.class));

        FilterProjection filterProjection = (FilterProjection)collectNode.projections().get(1);
        assertThat(filterProjection.requiredGranularity(), is(RowGranularity.SHARD));
        assertThat(filterProjection.outputs().size(), is(1));
        assertThat(filterProjection.outputs().get(0), instanceOf(InputColumn.class));
        InputColumn inputColumn = (InputColumn)filterProjection.outputs().get(0);
        assertThat(inputColumn.index(), is(0));

        MergeNode localMergeNode = planNode.localMergeNode();

        assertThat(localMergeNode.projections().size(), is(1));
        assertThat(localMergeNode.projections().get(0), instanceOf(TopNProjection.class));
    }

    @Test
    public void testGlobalAggregationHaving() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan(
                "select avg(date) from users having min(date) > '1970-01-01'");
        CollectNode collectNode = globalAggregate.collectNode();
        assertThat(collectNode.projections().size(), is(1));
        assertThat(collectNode.projections().get(0), instanceOf(AggregationProjection.class));

        MergeNode localMergeNode = globalAggregate.mergeNode();

        assertThat(localMergeNode.projections().size(), is(3));
        assertThat(localMergeNode.projections().get(0), instanceOf(AggregationProjection.class));
        assertThat(localMergeNode.projections().get(1), instanceOf(FilterProjection.class));
        assertThat(localMergeNode.projections().get(2), instanceOf(TopNProjection.class));

        AggregationProjection aggregationProjection = (AggregationProjection)localMergeNode.projections().get(0);
        assertThat(aggregationProjection.aggregations().size(), is(2));

        FilterProjection filterProjection = (FilterProjection)localMergeNode.projections().get(1);
        assertThat(filterProjection.outputs().size(), is(2));
        assertThat(filterProjection.outputs().get(0), instanceOf(InputColumn.class));
        InputColumn inputColumn = (InputColumn)filterProjection.outputs().get(0);
        assertThat(inputColumn.index(), is(0));

        TopNProjection topNProjection = (TopNProjection)localMergeNode.projections().get(2);
        assertThat(topNProjection.outputs().size(), is(1));
    }

    @Test
    public void testCountOnPartitionedTable() throws Exception {
        CountPlan plan = (CountPlan) plan("select count(*) from parted where date = 123");
        assertThat(plan, instanceOf(PlannedAnalyzedRelation.class));
        assertThat(plan.countNode().whereClause().partitions(), containsInAnyOrder(".partitioned.parted.04232chj"));
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectPartitionedTableOrderByPartitionedColumn() throws Exception {
        plan("select name from parted order by date");
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectPartitionedTableOrderByPartitionedColumnInFunction() throws Exception {
        plan("select name from parted order by year(date)");
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectOrderByPartitionedNestedColumn() throws Exception {
        plan("select id from multi_parted order by obj['name']");
    }

    @Test(expected = UnsupportedOperationException.class)
    public void testSelectOrderByPartitionedNestedColumnInFunction() throws Exception {
        plan("select id from multi_parted order by format('abc %s', obj['name'])");
    }

    @Test(expected = UnsupportedFeatureException.class)
    public void testQueryRequiresScalar() throws Exception {
        // only scalar functions are allowed on system tables because we have no lucene queries
        plan("select * from sys.shards where match(table_name, 'characters')");
    }

    @Test
    public void testGroupByWithHavingAndLimit() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select count(*), name from users group by name having count(*) > 1 limit 100");;

        MergeNode mergeNode = planNode.reducerMergeNode(); // reducer

        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  // pointing to second output from group projection

        // outputs: count(*), name
        TopNProjection topN = (TopNProjection) mergeNode.projections().get(2);
        assertThat(topN.outputs().get(0).valueType(), Is.<DataType>is(DataTypes.LONG));
        assertThat(topN.outputs().get(1).valueType(), Is.<DataType>is(DataTypes.STRING));


        MergeNode localMerge = planNode.localMergeNode();
        // topN projection
        //      outputs: count(*), name
        topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(topN.outputs().get(0).valueType(), Is.<DataType>is(DataTypes.LONG));
        assertThat(topN.outputs().get(1).valueType(), Is.<DataType>is(DataTypes.STRING));
    }

    @Test
    public void testGroupByWithHavingAndNoLimit() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select count(*), name from users group by name having count(*) > 1");

        MergeNode mergeNode = planNode.reducerMergeNode(); // reducer

        // group projection
        //      outputs: name, count(*)

        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  // pointing to second output from group projection

        assertThat(mergeNode.outputTypes().get(0), equalTo((DataType) DataTypes.LONG));
        assertThat(mergeNode.outputTypes().get(1), equalTo((DataType) DataTypes.STRING));

        mergeNode = planNode.localMergeNode();

        assertThat(mergeNode.outputTypes().get(0), equalTo((DataType) DataTypes.LONG));
        assertThat(mergeNode.outputTypes().get(1), equalTo((DataType) DataTypes.STRING));
    }

    @Test
    public void testGroupByWithHavingAndNoSelectListReordering() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select name, count(*) from users group by name having count(*) > 1");

        MergeNode mergeNode = planNode.reducerMergeNode(); // reducer

        // group projection
        //      outputs: name, count(*)
        // filter projection
        //      outputs: name, count(*)

        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  // pointing to second output from group projection

        // outputs: name, count(*)
        assertThat(((InputColumn) filterProjection.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) filterProjection.outputs().get(1)).index(), is(1));

        MergeNode localMerge = planNode.localMergeNode();
        // topN projection
        //      outputs: name, count(*)
        TopNProjection topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));
    }

    @Test
    public void testGroupByHavingAndNoSelectListReOrderingWithLimit() throws Exception {
        DistributedGroupBy planNode = (DistributedGroupBy) plan(
                "select name, count(*) from users group by name having count(*) > 1 limit 100");

        MergeNode mergeNode = planNode.reducerMergeNode(); // reducer

        // group projection
        //      outputs: name, count(*)
        // filter projection
        //      outputs: name, count(*)
        // topN projection
        //      outputs: name, count(*)

        Projection projection = mergeNode.projections().get(1);
        assertThat(projection, instanceOf(FilterProjection.class));
        FilterProjection filterProjection = (FilterProjection) projection;

        Symbol countArgument = ((Function) filterProjection.query()).arguments().get(0);
        assertThat(countArgument, instanceOf(InputColumn.class));
        assertThat(((InputColumn) countArgument).index(), is(1));  // pointing to second output from group projection

        // outputs: name, count(*)
        assertThat(((InputColumn) filterProjection.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) filterProjection.outputs().get(1)).index(), is(1));

        // outputs: name, count(*)
        TopNProjection topN = (TopNProjection) mergeNode.projections().get(2);
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));


        MergeNode localMerge = planNode.localMergeNode();

        // topN projection
        //      outputs: name, count(*)
        topN = (TopNProjection) localMerge.projections().get(0);
        assertThat(((InputColumn) topN.outputs().get(0)).index(), is(0));
        assertThat(((InputColumn) topN.outputs().get(1)).index(), is(1));
    }

    @Test
    public void testOrderByOnAnalyzed() throws Exception {
        expectedException.expect(UnsupportedOperationException.class);
        expectedException.expectMessage("Cannot ORDER BY 'users.text': sorting on analyzed/fulltext columns is not possible");
        plan("select text from users u order by 1");
    }

    @Test
    public void testSortOnUnknownColumn() throws Exception {
        expectedException.expect(UnsupportedOperationException.class);
        expectedException.expectMessage("Cannot ORDER BY 'details['unknown_column']': invalid data type 'null'.");
        plan("select details from ignored_nested order by details['unknown_column']");
    }

    @Test
    public void testOrderByOnIndexOff() throws Exception {
        expectedException.expect(UnsupportedOperationException.class);
        expectedException.expectMessage("Cannot ORDER BY 'users.no_index': sorting on non-indexed columns is not possible");
        plan("select no_index from users u order by 1");
    }

    @Test
    public void testGroupByOnAnalyzed() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.text': grouping on analyzed/fulltext columns is not possible");
        plan("select text from users u group by 1");
    }

    @Test
    public void testGroupByOnIndexOff() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.no_index': grouping on non-indexed columns is not possible");
        plan("select no_index from users u group by 1");
    }

    @Test
    public void testSelectAnalyzedReferenceInFunctionGroupBy() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.text': grouping on analyzed/fulltext columns is not possible");
        plan("select substr(text, 0, 2) from users u group by 1");
    }

    @Test
    public void testSelectAnalyzedReferenceInFunctionAggregation() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot select analyzed column 'users.text' within grouping or aggregations");
        plan("select min(substr(text, 0, 2)) from users");
    }

    @Test
    public void testSelectNonIndexedReferenceInFunctionGroupBy() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot GROUP BY 'users.no_index': grouping on non-indexed columns is not possible");
        plan("select substr(no_index, 0, 2) from users u group by 1");
    }

    @Test
    public void testSelectNonIndexedReferenceInFunctionAggregation() throws Exception {
        expectedException.expect(IllegalArgumentException.class);
        expectedException.expectMessage("Cannot select non-indexed column 'users.no_index' within grouping or aggregations");
        plan("select min(substr(no_index, 0, 2)) from users");
    }

    @Test
    public void testGlobalAggregateWithWhereOnPartitionColumn() throws Exception {
        GlobalAggregate globalAggregate = (GlobalAggregate) plan(
                "select min(name) from parted where date > 0");

        WhereClause whereClause = globalAggregate.collectNode().whereClause();
        assertThat(whereClause.partitions().size(), is(1));
        assertThat(whereClause.noMatch(), is(false));
    }

    private void assertNoop(Plan plan){
        assertThat(plan, instanceOf(NoopPlan.class));
    }

    @Test
    public void testHasNoResultFromHaving() throws Exception {
        assertNoop(plan("select min(name) from users having 1 = 2"));
    }

    @Test
    public void testHasNoResultFromLimit() {
        assertNoop(plan("select count(*) from users limit 1 offset 1"));
        assertNoop(plan("select count(*) from users limit 5 offset 1"));
        assertNoop(plan("select count(*) from users limit 0"));
    }

    @Test
    public void testHasNoResultFromQuery() {
        assertNoop(plan("select name from users where false"));
    }

    @Test
    public void testInsertFromValuesWithOnDuplicateKey() throws Exception {
        Upsert plan = (Upsert) plan("insert into users (id, name) values (1, null) on duplicate key update name = values(name)");
        PlanNode planNode = ((IterablePlan) plan.nodes().get(0)).iterator().next();
        assertThat(planNode, instanceOf(SymbolBasedUpsertByIdNode.class));
        SymbolBasedUpsertByIdNode node = (SymbolBasedUpsertByIdNode) planNode;

        assertThat(node.updateColumns(), is(new String[]{ "name" }));

        assertThat(node.insertColumns().length, is(2));
        Reference idRef = node.insertColumns()[0];
        assertThat(idRef.ident().columnIdent().fqn(), is("id"));
        Reference nameRef = node.insertColumns()[1];
        assertThat(nameRef.ident().columnIdent().fqn(), is("name"));

        assertThat(node.items().size(), is(1));
        SymbolBasedUpsertByIdNode.Item item = node.items().get(0);
        assertThat(item.index(), is("users"));
        assertThat(item.id(), is("1"));
        assertThat(item.routing(), is("1"));

        assertThat(item.insertValues().length, is(2));
        assertThat((Long)item.insertValues()[0], is(1L));
        assertNull(item.insertValues()[1]);

        assertThat(item.updateAssignments().length, is(1));
        assertThat(item.updateAssignments()[0], isLiteral(null, DataTypes.STRING));
    }

    @Test
    public void testGroupByOnClusteredByColumnPartitionedOnePartition() throws Exception {
        // only one partition hit
        Plan optimizedPlan = plan("select count(*), city from clustered_parted where date=1395874800000 group by city");
        assertThat(optimizedPlan, instanceOf(NonDistributedGroupBy.class));
        NonDistributedGroupBy optimizedGroupBy = (NonDistributedGroupBy) optimizedPlan;

        assertThat(optimizedGroupBy.collectNode().isPartitioned(), is(true));
        assertThat(optimizedGroupBy.collectNode().projections().size(), is(1));
        assertThat(optimizedGroupBy.collectNode().projections().get(0), instanceOf(GroupProjection.class));

        assertThat(optimizedGroupBy.localMergeNode().projections().size(), is(1));
        assertThat(optimizedGroupBy.localMergeNode().projections().get(0), instanceOf(TopNProjection.class));

        // > 1 partition hit
        Plan plan = plan("select count(*), city from clustered_parted where date=1395874800000 or date=1395961200000 group by city");
        assertThat(plan, instanceOf(DistributedGroupBy.class));
    }

    @Test
    public void testIndices() throws Exception {
        TableIdent custom = new TableIdent("custom", "table");
        String[] indices = Planner.indices(TestingTableInfo.builder(custom, RowGranularity.DOC, shardRouting).add("id", DataTypes.INTEGER, null).build(), WhereClause.MATCH_ALL);
        assertThat(indices, arrayContainingInAnyOrder("custom.table"));

        indices = Planner.indices(TestingTableInfo.builder(new TableIdent(null, "table"), RowGranularity.DOC, shardRouting).add("id", DataTypes.INTEGER, null).build(), WhereClause.MATCH_ALL);
        assertThat(indices, arrayContainingInAnyOrder("table"));

        indices = Planner.indices(TestingTableInfo.builder(custom, RowGranularity.DOC, shardRouting)
                .add("id", DataTypes.INTEGER, null)
                .add("date", DataTypes.TIMESTAMP, null, true)
                .addPartitions(new PartitionName(custom, Arrays.asList(new BytesRef("0"))).stringValue())
                .addPartitions(new PartitionName(custom, Arrays.asList(new BytesRef("12345"))).stringValue())
                .build(), WhereClause.MATCH_ALL);
        assertThat(indices, arrayContainingInAnyOrder("custom..partitioned.table.04130", "custom..partitioned.table.04332chj6gqg"));
    }

    @Test
    public void testAllocatedJobSearchContextIds() throws Exception {
        Planner.Context plannerContext = new Planner.Context(clusterService);
        CollectNode collectNode = new CollectNode(
                plannerContext.nextExecutionNodeId(), "collect", shardRouting);
        int shardNum = collectNode.routing().numShards();

        plannerContext.allocateJobSearchContextIds(collectNode.routing());

        java.lang.reflect.Field f = plannerContext.getClass().getDeclaredField("jobSearchContextIdBaseSeq");
        f.setAccessible(true);
        int jobSearchContextIdBaseSeq = (Integer)f.get(plannerContext);

        assertThat(jobSearchContextIdBaseSeq, is(shardNum));
        assertThat(collectNode.routing().jobSearchContextIdBase(), is(jobSearchContextIdBaseSeq-shardNum));

        int idx = 0;
        for (Map.Entry<String, Map<String, List<Integer>>> locations : collectNode.routing().locations().entrySet()) {
            String nodeId = locations.getKey();
            for (Map.Entry<String, List<Integer>> entry : locations.getValue().entrySet()) {
                for (Integer shardId : entry.getValue()) {
                    assertThat(plannerContext.shardId(idx), is(new ShardId(entry.getKey(), shardId)));
                    assertThat(plannerContext.nodeId(idx), is(nodeId));
                    idx++;
                }
            }
        }

        // jobSearchContextIdBase must only set once on a Routing instance
        int jobSearchContextIdBase = collectNode.routing().jobSearchContextIdBase();
        plannerContext.allocateJobSearchContextIds(collectNode.routing());
        assertThat(collectNode.routing().jobSearchContextIdBase(), is(jobSearchContextIdBase));
    }

    @Test
    public void testExecutionNodeIdSequence() throws Exception {
        Planner.Context plannerContext = new Planner.Context(clusterService);
        CollectNode collectNode1 = new CollectNode(
                plannerContext.nextExecutionNodeId(), "collect1", shardRouting);
        CollectNode collectNode2 = new CollectNode(
                plannerContext.nextExecutionNodeId(), "collect2", shardRouting);

        assertThat(collectNode1.executionNodeId(), is(0));
        assertThat(collectNode2.executionNodeId(), is(1));
    }

    @SuppressWarnings("ConstantConditions")
    @Test
    public void testLimitThatIsBiggerThanPageSizeCausesQTFPUshPlan() throws Exception {
        QueryThenFetch plan = (QueryThenFetch) plan("select * from users limit 2147483647 ");
        assertThat(plan.collectNode().downstreamNodes().size(), is(1));
        assertThat(plan.collectNode().downstreamNodes().get(0), is(LOCAL_NODE_ID));
        assertThat(plan.collectNode().hasDistributingDownstreams(), is(true));
    }

    @Test
    public void testKillPlan() throws Exception {
        Plan killPlan = plan("kill all");
        assertThat(killPlan, Matchers.<Plan>is(KillPlan.INSTANCE));
    }
}


File: sql/src/test/java/io/crate/planner/node/CollectNodeTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Value;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.Arrays;
import java.util.UUID;

import static org.hamcrest.Matchers.equalTo;
import static org.hamcrest.Matchers.is;

public class CollectNodeTest extends CrateUnitTest {

    @Test
    public void testStreaming() throws Exception {
        CollectNode cn = new CollectNode(0, "cn");
        cn.maxRowGranularity(RowGranularity.DOC);
        cn.downstreamNodes(Arrays.asList("n1", "n2"));
        cn.toCollect(ImmutableList.<Symbol>of(new Value(DataTypes.STRING)));
        cn.jobId(UUID.randomUUID());

        BytesStreamOutput out = new BytesStreamOutput();
        cn.writeTo(out);

        BytesStreamInput in = new BytesStreamInput(out.bytes());
        CollectNode cn2 = new CollectNode(1, "collect");
        cn2.readFrom(in);
        assertThat(cn, equalTo(cn2));

        assertThat(cn.toCollect(), is(cn2.toCollect()));
        assertThat(cn.downstreamNodes(), is(cn2.downstreamNodes()));
        assertThat(cn.executionNodes(), is(cn2.executionNodes()));
        assertThat(cn.jobId(), is(cn2.jobId()));
        assertThat(cn.executionNodeId(), is(cn2.executionNodeId()));
        assertThat(cn.maxRowGranularity(), is(cn2.maxRowGranularity()));
    }
}


File: sql/src/test/java/io/crate/planner/node/MergeNodeTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.FunctionInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.Arrays;
import java.util.UUID;

import static org.hamcrest.core.Is.is;

public class MergeNodeTest extends CrateUnitTest {


    @Test
    public void testSerialization() throws Exception {
        MergeNode node = new MergeNode(0, "merge", 2);
        node.jobId(UUID.randomUUID());
        node.executionNodes(Sets.newHashSet("node1", "node2"));
        node.inputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED, DataTypes.STRING));
        node.downstreamNodes(Sets.newHashSet("node3", "node4"));

        Reference nameRef = TestingHelpers.createReference("name", DataTypes.STRING);
        GroupProjection groupProjection = new GroupProjection();
        groupProjection.keys(Arrays.<Symbol>asList(nameRef));
        groupProjection.values(Arrays.asList(
                new Aggregation(
                        new FunctionInfo(new FunctionIdent(CountAggregation.NAME, ImmutableList.<DataType>of()), DataTypes.LONG),
                        ImmutableList.<Symbol>of(),
                        Aggregation.Step.PARTIAL,
                        Aggregation.Step.FINAL
                )
        ));
        TopNProjection topNProjection = new TopNProjection(10, 0);

        node.projections(Arrays.asList(groupProjection, topNProjection));

        BytesStreamOutput output = new BytesStreamOutput();
        node.writeTo(output);


        BytesStreamInput input = new BytesStreamInput(output.bytes());
        MergeNode node2 = new MergeNode();
        node2.readFrom(input);

        assertThat(node.downstreamNodes(), is(node2.downstreamNodes()));
        assertThat(node.numUpstreams(), is(node2.numUpstreams()));
        assertThat(node.executionNodes(), is(node2.executionNodes()));
        assertThat(node.jobId(), is(node2.jobId()));
        assertThat(node.inputTypes(), is(node2.inputTypes()));
        assertThat(node.executionNodeId(), is(node2.executionNodeId()));
    }
}


File: sql/src/test/java/io/crate/planner/node/StreamerVisitorTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import io.crate.Streamer;
import io.crate.metadata.FunctionIdent;
import io.crate.metadata.FunctionInfo;
import io.crate.metadata.Functions;
import io.crate.metadata.Routing;
import io.crate.operation.aggregation.impl.AggregationImplModule;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.operation.aggregation.impl.MaximumAggregation;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Symbol;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.ModulesBuilder;
import org.junit.Before;
import org.junit.Test;

import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.TreeMap;

import static org.hamcrest.Matchers.instanceOf;
import static org.hamcrest.Matchers.is;

public class StreamerVisitorTest extends CrateUnitTest {

    private StreamerVisitor visitor;
    private FunctionInfo maxInfo;
    private FunctionInfo countInfo;

    final static Routing EMPTY_ROUTING = new Routing(new TreeMap<String, Map<String, List<Integer>>>());

    @Before
    public void prepare() {
        Injector injector = new ModulesBuilder()
                .add(new AggregationImplModule())
                .createInjector();
        Functions functions = injector.getInstance(Functions.class);
        visitor = new StreamerVisitor(functions);
        maxInfo = new FunctionInfo(new FunctionIdent(MaximumAggregation.NAME, Arrays.<DataType>asList(DataTypes.INTEGER)), DataTypes.INTEGER);
        countInfo = new FunctionInfo(new FunctionIdent(CountAggregation.NAME, ImmutableList.<DataType>of()), DataTypes.LONG);
    }

    @Test
    public void testGetOutputStreamersFromCollectNode() throws Exception {
        CollectNode collectNode = new CollectNode(0, "bla", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.FLOAT, DataTypes.OBJECT));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        Streamer<?>[] streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(3));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.FLOAT.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.OBJECT.streamer().getClass()));

        // as executionNode
        ctx = visitor.processExecutionNode(collectNode);
        streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(3));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.FLOAT.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.OBJECT.streamer().getClass()));
    }

    @Test
    public void testGetOutputStreamersFromCollectNodeWithWrongNull() throws Exception {
        // null means we expect an aggstate here
        CollectNode collectNode = new CollectNode(0, "bla", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, null, DataTypes.OBJECT));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        // assume an unknown column
        assertEquals(DataTypes.UNDEFINED.streamer(), ctx.outputStreamers()[1]);

        // as executionNode
        ctx = visitor.processExecutionNode(collectNode);
        // assume an unknown column
        assertEquals(DataTypes.UNDEFINED.streamer(), ctx.outputStreamers()[1]);
    }

    @Test
    public void testGetOutputStreamersFromCollectNodeWithAggregations() throws Exception {
        CollectNode collectNode = new CollectNode(0, "bla", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, null, null, DataTypes.DOUBLE));
        AggregationProjection aggregationProjection = new AggregationProjection();
        aggregationProjection.aggregations(Arrays.asList( // not a real use case, only for test convenience, sorry
                new Aggregation(maxInfo, Arrays.<Symbol>asList(new InputColumn(0)), Aggregation.Step.ITER, Aggregation.Step.FINAL),
                new Aggregation(maxInfo, Arrays.<Symbol>asList(new InputColumn(1)), Aggregation.Step.ITER, Aggregation.Step.PARTIAL)
        ));
        collectNode.projections(Arrays.<Projection>asList(aggregationProjection));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        Streamer<?>[] streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(4));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.INTEGER.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.INTEGER.streamer().getClass()));
        assertThat(streamers[3], instanceOf(DataTypes.DOUBLE.streamer().getClass()));
    }

    @Test
    public void testGetOutputStreamersFromCollectNodeWithGroupAndTopNProjection() throws Exception {
        CollectNode collectNode = new CollectNode(0, "mynode", EMPTY_ROUTING);
        collectNode.outputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED));
        GroupProjection groupProjection = new GroupProjection(
                Arrays.<Symbol>asList(Literal.newLiteral("key")),
                Arrays.asList(new Aggregation(
                        countInfo,
                        ImmutableList.<Symbol>of(),
                        Aggregation.Step.PARTIAL, Aggregation.Step.FINAL))
        );
        collectNode.projections(Arrays.<Projection>asList(groupProjection, new TopNProjection(10,0)));
        StreamerVisitor.Context ctx = visitor.processPlanNode(collectNode);
        Streamer<?>[] streamers = ctx.outputStreamers();
        assertThat(streamers.length, is(1));
        assertThat(streamers[0], instanceOf(DataTypes.LONG.streamer().getClass()));
    }

    @Test
    public void testGetInputStreamersForMergeNode() throws Exception {
        MergeNode mergeNode = new MergeNode(0, "mörtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.SHORT, DataTypes.TIMESTAMP));
        StreamerVisitor.Context ctx = visitor.processPlanNode(mergeNode);
        Streamer<?>[] streamers = ctx.inputStreamers();
        assertThat(streamers.length, is(3));
        assertThat(streamers[0], instanceOf(DataTypes.BOOLEAN.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.SHORT.streamer().getClass()));
        assertThat(streamers[2], instanceOf(DataTypes.TIMESTAMP.streamer().getClass()));
    }

    @Test(expected= IllegalStateException.class)
    public void testGetInputStreamersForMergeNodeWithWrongNull() throws Exception {
        MergeNode mergeNode = new MergeNode(0, "mörtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, null, DataTypes.TIMESTAMP));
        visitor.processPlanNode(mergeNode);
    }

    @Test
    public void testGetInputStreamersForMergeNodeWithAggregations() throws Exception {
        MergeNode mergeNode = new MergeNode(0, "mörtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED, DataTypes.TIMESTAMP));
        AggregationProjection aggregationProjection = new AggregationProjection();
        aggregationProjection.aggregations(Arrays.asList(
                new Aggregation(maxInfo, Arrays.<Symbol>asList(new InputColumn(0)), Aggregation.Step.PARTIAL, Aggregation.Step.FINAL)
        ));
        mergeNode.projections(Arrays.<Projection>asList(aggregationProjection));
        StreamerVisitor.Context ctx = visitor.processPlanNode(mergeNode);
        Streamer<?>[] streamers = ctx.inputStreamers();
        assertThat(streamers.length, is(2));
        assertThat(streamers[0], instanceOf(DataTypes.INTEGER.streamer().getClass()));
        assertThat(streamers[1], instanceOf(DataTypes.TIMESTAMP.streamer().getClass()));
    }

    @Test
    public void testOutputStreamerFromGroupByMergeNode() throws Exception {
        /**
         * select count(*), name ... group by name limit 2
         *
         * the groupProjection has  the outputs
         *      name, count(*)
         *
         * the topN projection swaps the outputs to
         *
         *      count(*), name
         *
         * so the streamers have to be
         *
         *      longStreamer,  stringStreamer
         */

        MergeNode mergeNode = new MergeNode(0, "mörtsch", 2);
        mergeNode.inputTypes(Arrays.<DataType>asList(DataTypes.STRING, DataTypes.UNDEFINED));
        GroupProjection groupProjection = new GroupProjection(
                Arrays.<Symbol>asList(Literal.newLiteral("key")),
                Arrays.asList(new Aggregation(
                        countInfo,
                        ImmutableList.<Symbol>of(),
                        Aggregation.Step.PARTIAL, Aggregation.Step.FINAL))
        );

        TopNProjection topNProjection = new TopNProjection(2, 0);
        topNProjection.outputs(Arrays.<Symbol>asList(
                new InputColumn(1),
                new InputColumn(0)
        ));

        mergeNode.projections(Arrays.asList(groupProjection, topNProjection));
        mergeNode.outputTypes(Arrays.<DataType>asList(DataTypes.LONG, DataTypes.STRING));
        StreamerVisitor.Context context = visitor.processPlanNode(mergeNode);
        assertSame(DataTypes.STRING.streamer(), context.outputStreamers()[1]);
        assertSame(DataTypes.LONG.streamer(), context.outputStreamers()[0]);
    }
}


File: sql/src/test/java/io/crate/planner/node/dql/CountNodeTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import io.crate.analyze.WhereClause;
import io.crate.core.collections.TreeMapBuilder;
import io.crate.metadata.Routing;
import io.crate.planner.node.ExecutionNode;
import io.crate.test.integration.CrateUnitTest;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.*;

import static org.hamcrest.Matchers.*;

public class CountNodeTest extends CrateUnitTest {

    @Test
    public void testStreaming() throws Exception {
        Routing routing = new Routing(
                TreeMapBuilder.<String, Map<String, List<Integer>>>newMapBuilder()
                        .put("n1", TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                                .put("i1", Arrays.asList(1, 2))
                                .put("i2", Arrays.asList(1, 2)).map())
                        .put("n2", TreeMapBuilder.<String, List<Integer>>newMapBuilder()
                                .put("i1", Collections.singletonList(3)).map()).map());
        CountNode countNode = new CountNode(1, routing, WhereClause.MATCH_ALL);
        UUID jobId = UUID.randomUUID();
        countNode.jobId(jobId);

        BytesStreamOutput out = new BytesStreamOutput(10);
        countNode.writeTo(out);

        BytesStreamInput in = new BytesStreamInput(out.bytes());

        CountNode streamedNode = CountNode.FACTORY.create();
        streamedNode.readFrom(in);

        assertThat(streamedNode.jobId(), is(jobId));
        assertThat(streamedNode.executionNodeId(), is(1));
        assertThat(streamedNode.downstreamNodes(), contains(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE));
        assertThat(streamedNode.executionNodes(), containsInAnyOrder("n1", "n2"));
        assertThat(streamedNode.routing(), equalTo(routing));
    }
}