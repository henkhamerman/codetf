Refactoring Types: ['Pull Up Attribute', 'Move Attribute', 'Pull Up Method']
nsed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.action.job;

import com.google.common.base.Optional;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Streamer;
import io.crate.breaker.CrateCircuitBreakerService;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.executor.transport.distributed.SingleBucketBuilder;
import io.crate.jobs.CountContext;
import io.crate.jobs.JobExecutionContext;
import io.crate.jobs.PageDownstreamContext;
import io.crate.operation.PageDownstream;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.collect.JobCollectContext;
import io.crate.operation.collect.MapSideDataCollectOperation;
import io.crate.operation.count.CountOperation;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.ExecutionNodes;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.types.DataTypes;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Map;
import java.util.UUID;

@Singleton
public class ContextPreparer {

    private static final ESLogger LOGGER = Loggers.getLogger(ContextPreparer.class);

    private final MapSideDataCollectOperation collectOperation;
    private ClusterService clusterService;
    private CountOperation countOperation;
    private final CircuitBreaker circuitBreaker;
    private final ThreadPool threadPool;
    private final PageDownstreamFactory pageDownstreamFactory;
    private final ResultProviderFactory resultProviderFactory;
    private final StreamerVisitor streamerVisitor;
    private final InnerPreparer innerPreparer;

    @Inject
    public ContextPreparer(MapSideDataCollectOperation collectOperation,
                           ClusterService clusterService,
                           CrateCircuitBreakerService breakerService,
                           ThreadPool threadPool,
                           CountOperation countOperation,
                           PageDownstreamFactory pageDownstreamFactory,
                           ResultProviderFactory resultProviderFactory,
                           StreamerVisitor streamerVisitor) {
        this.collectOperation = collectOperation;
        this.clusterService = clusterService;
        this.countOperation = countOperation;
        circuitBreaker = breakerService.getBreaker(CrateCircuitBreakerService.QUERY_BREAKER);
        this.threadPool = threadPool;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.resultProviderFactory = resultProviderFactory;
        this.streamerVisitor = streamerVisitor;
        innerPreparer = new InnerPreparer();
    }

    @Nullable
    public ListenableFuture<Bucket> prepare(UUID jobId,
                                            ExecutionNode executionNode,
                                            JobExecutionContext.Builder contextBuilder) {
        PreparerContext preparerContext = new PreparerContext(jobId, contextBuilder);
        innerPreparer.process(executionNode, preparerContext);
        return preparerContext.directResultFuture;
    }

    private static class PreparerContext {

        private final UUID jobId;
        private final JobExecutionContext.Builder contextBuilder;
        private ListenableFuture<Bucket> directResultFuture;

        private PreparerContext(UUID jobId,
                                JobExecutionContext.Builder contextBuilder) {
            this.contextBuilder = contextBuilder;
            this.jobId = jobId;
        }
    }

    private class InnerPreparer extends ExecutionNodeVisitor<PreparerContext, Void> {

        @Override
        public Void visitCountNode(CountNode countNode, PreparerContext context) {
            Map<String, Map<String, List<Integer>>> locations = countNode.routing().locations();
            if (locations == null) {
                throw new IllegalArgumentException("locations are empty. Can't start count operation");
            }
            String localNodeId = clusterService.localNode().id();
            Map<String, List<Integer>> indexShardMap = locations.get(localNodeId);
            if (indexShardMap == null) {
                throw new IllegalArgumentException("The routing of the countNode doesn't contain the current nodeId");
            }

            final SingleBucketBuilder singleBucketBuilder = new SingleBucketBuilder(new Streamer[]{DataTypes.LONG});
            CountContext countContext = new CountContext(
                    countOperation,
                    singleBucketBuilder,
                    indexShardMap,
                    countNode.whereClause()
            );
            context.directResultFuture = singleBucketBuilder.result();
            context.contextBuilder.addSubContext(countNode.executionNodeId(), countContext);
            return null;
        }

        @Override
        public Void visitMergeNode(final MergeNode node, final PreparerContext context) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(circuitBreaker, node);
            ResultProvider downstream = resultProviderFactory.createDownstream(node, node.jobId());
            Tuple<PageDownstream, FlatProjectorChain> pageDownstreamProjectorChain =
                    pageDownstreamFactory.createMergeNodePageDownstream(
                            node,
                            downstream,
                            ramAccountingContext,
                            Optional.of(threadPool.executor(ThreadPool.Names.SEARCH)));
            StreamerVisitor.Context streamerContext = streamerVisitor.processPlanNode(node);
            PageDownstreamContext pageDownstreamContext = new PageDownstreamContext(
                    node.name(),
                    pageDownstreamProjectorChain.v1(),
                    streamerContext.inputStreamers(),
                    ramAccountingContext,
                    node.numUpstreams());

            context.contextBuilder.addSubContext(node.executionNodeId(), pageDownstreamContext);

            FlatProjectorChain flatProjectorChain = pageDownstreamProjectorChain.v2();
            if (flatProjectorChain != null) {
                flatProjectorChain.startProjections(pageDownstreamContext);
            }
            return null;
        }

        @Override
        public Void visitCollectNode(final CollectNode node, final PreparerContext context) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(circuitBreaker, node);
            ResultProvider downstream = collectOperation.createDownstream(node);

            if (ExecutionNodes.hasDirectResponseDownstream(node.downstreamNodes())) {
                context.directResultFuture = downstream.result();
            }
            final JobCollectContext jobCollectContext = new JobCollectContext(
                    context.jobId,
                    node,
                    collectOperation,
                    ramAccountingContext,
                    downstream
            );
            context.contextBuilder.addSubContext(node.executionNodeId(), jobCollectContext);
            return null;
        }
    }
}


File: sql/src/main/java/io/crate/executor/Job.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class Job {

    private final UUID id;
    private List<Task> tasks = new ArrayList<>();

    public Job() {
        this(UUID.randomUUID());
    }

    public Job(UUID id) {
        this.id = id;
    }

    public UUID id() {
        return id;
    }

    public void addTasks(Collection<Task> tasks) {
        this.tasks.addAll(tasks);
    }

    public List<Task> tasks() {
        return tasks;
    }
}


File: sql/src/main/java/io/crate/executor/transport/ExecutionNodesTask.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport;

import com.google.common.base.Optional;
import com.google.common.util.concurrent.FutureCallback;
import com.google.common.util.concurrent.Futures;
import com.google.common.util.concurrent.ListenableFuture;
import com.google.common.util.concurrent.SettableFuture;
import io.crate.Streamer;
import io.crate.action.job.ContextPreparer;
import io.crate.action.job.JobRequest;
import io.crate.action.job.JobResponse;
import io.crate.action.job.TransportJobAction;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.executor.JobTask;
import io.crate.executor.TaskResult;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.jobs.PageDownstreamContext;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.*;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.planner.node.*;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.collect.Tuple;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nonnull;
import javax.annotation.Nullable;
import java.util.*;


public class ExecutionNodesTask extends JobTask {

    private static final ESLogger LOGGER = Loggers.getLogger(ExecutionNodesTask.class);

    private final TransportJobAction transportJobAction;
    private final List<List<ExecutionNode>> groupedExecutionNodes;
    private final List<SettableFuture<TaskResult>> results;
    private final boolean hasDirectResponse;
    private final ClusterService clusterService;
    private ContextPreparer contextPreparer;
    private final JobContextService jobContextService;
    private final PageDownstreamFactory pageDownstreamFactory;
    private final ThreadPool threadPool;
    private TransportCloseContextNodeAction transportCloseContextNodeAction;
    private final StreamerVisitor streamerVisitor;
    private final CircuitBreaker circuitBreaker;
    private List<MergeNode> mergeNodes;
    private boolean rowCountResult = false;

    /**
     * @param mergeNodes list of mergeNodes for the final merge operation on the handler.
     *                  This may be null in the constructor but then it must be set using the
     *                  {@link #mergeNodes(List)} setter before {@link #start()} is called.
     *                   Multiple merge nodes are only occurring on bulk operations.
     */
    protected ExecutionNodesTask(UUID jobId,
                                 ClusterService clusterService,
                                 ContextPreparer contextPreparer,
                                 JobContextService jobContextService,
                                 PageDownstreamFactory pageDownstreamFactory,
                                 ThreadPool threadPool,
                                 TransportJobAction transportJobAction,
                                 TransportCloseContextNodeAction transportCloseContextNodeAction,
                                 StreamerVisitor streamerVisitor,
                                 CircuitBreaker circuitBreaker,
                                 @Nullable List<MergeNode> mergeNodes,
                                 List<List<ExecutionNode>> groupedExecutionNodes) {
        super(jobId);
        this.clusterService = clusterService;
        this.contextPreparer = contextPreparer;
        this.jobContextService = jobContextService;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.transportCloseContextNodeAction = transportCloseContextNodeAction;
        this.streamerVisitor = streamerVisitor;
        this.circuitBreaker = circuitBreaker;
        this.mergeNodes = mergeNodes;
        this.transportJobAction = transportJobAction;
        this.groupedExecutionNodes = groupedExecutionNodes;
        hasDirectResponse = hasDirectResponse(groupedExecutionNodes);

        List<SettableFuture<TaskResult>> results = new ArrayList<>(groupedExecutionNodes.size());
        for (int i = 0; i < groupedExecutionNodes.size(); i++) {
            results.add(SettableFuture.<TaskResult>create());
        }
        this.results = results;
    }

    public void mergeNodes(List<MergeNode> mergeNodes) {
        assert this.mergeNodes == null : "can only overwrite mergeNodes if it was null";
        this.mergeNodes = mergeNodes;
    }

    public void rowCountResult(boolean rowCountResult) {
        this.rowCountResult = rowCountResult;
    }

    @Override
    public void start() {
        assert mergeNodes != null : "mergeNodes must not be null";

        Map<String, Collection<ExecutionNode>> nodesByServer = ExecutionNodeGrouper.groupByServer(clusterService.state().nodes().localNodeId(), groupedExecutionNodes);
        RowDownstream rowDownstream;
        if (rowCountResult) {
            rowDownstream = new RowCountResultRowDownstream(results);
        } else {
            rowDownstream = new QueryResultRowDownstream(results);
        }
        Streamer<?>[] streamers = streamerVisitor.processExecutionNode(mergeNodes.get(0)).inputStreamers();
        List<PageDownstreamContext> pageDownstreamContexts = new ArrayList<>(groupedExecutionNodes.size());

        for (int i = 0; i < groupedExecutionNodes.size(); i++) {
            RamAccountingContext ramAccountingContext = RamAccountingContext.forExecutionNode(
                    circuitBreaker, mergeNodes.get(i));

            PageDownstreamContext pageDownstreamContext = createPageDownstreamContext(ramAccountingContext, streamers,
                    mergeNodes.get(i), groupedExecutionNodes.get(i), rowDownstream);
            if (nodesByServer.size() == 0) {
                pageDownstreamContext.finish();
                continue;
            }
            if (!hasDirectResponse) {
                createLocalContextAndStartOperation(pageDownstreamContext, nodesByServer, mergeNodes.get(i).executionNodeId());
            }
            pageDownstreamContexts.add(pageDownstreamContext);
        }
        if (nodesByServer.size() == 0) {
            return;
        }
        addCloseContextCallback(transportCloseContextNodeAction, groupedExecutionNodes, nodesByServer.keySet());
        sendJobRequests(streamers, pageDownstreamContexts, nodesByServer);
    }

    private PageDownstreamContext createPageDownstreamContext(
            RamAccountingContext ramAccountingContext,
            Streamer<?>[] streamers,
            MergeNode mergeNode,
            List<ExecutionNode> executionNodes,
            RowDownstream rowDownstream) {
        Tuple<PageDownstream, FlatProjectorChain> pageDownstreamProjectorChain = pageDownstreamFactory.createMergeNodePageDownstream(
                mergeNode,
                rowDownstream,
                ramAccountingContext,
                Optional.of(threadPool.executor(ThreadPool.Names.SEARCH))
        );
        PageDownstreamContext pageDownstreamContext = new PageDownstreamContext(
                mergeNode.name(),
                pageDownstreamProjectorChain.v1(),
                streamers,
                ramAccountingContext,
                executionNodes.get(executionNodes.size() - 1).executionNodes().size()
        );
        FlatProjectorChain flatProjectorChain = pageDownstreamProjectorChain.v2();
        if (flatProjectorChain != null) {
            flatProjectorChain.startProjections(pageDownstreamContext);
        }
        return pageDownstreamContext;
    }

    private void sendJobRequests(Streamer<?>[] streamers,
                                 List<PageDownstreamContext> pageDownstreamContexts,
                                 Map<String, Collection<ExecutionNode>> nodesByServer) {
        int idx = 0;
        for (Map.Entry<String, Collection<ExecutionNode>> entry : nodesByServer.entrySet()) {
            String serverNodeId = entry.getKey();
            if (TableInfo.NULL_NODE_ID.equals(serverNodeId)) {
                continue; // handled by local node
            }
            Collection<ExecutionNode> executionNodes = entry.getValue();

            JobRequest request = new JobRequest(jobId(), executionNodes);
            if (hasDirectResponse) {
                transportJobAction.execute(serverNodeId, request,
                        new DirectResponseListener(idx, streamers, pageDownstreamContexts));
            } else {
                transportJobAction.execute(serverNodeId, request,
                        new FailureOnlyResponseListener(results));
            }
            idx++;
        }
    }

    /**
     * removes the localNodeId entry from the nodesByServer map and initializes the context and starts the operation.
     *
     * This is done in order to be able to create the JobExecutionContext with the localMerge PageDownstreamContext
     */
    private void createLocalContextAndStartOperation(PageDownstreamContext finalLocalMerge,
                                                     Map<String, Collection<ExecutionNode>> nodesByServer,
                                                     int localMergeExecutionNodeId) {
        String localNodeId = clusterService.localNode().id();
        Collection<ExecutionNode> localExecutionNodes = nodesByServer.remove(localNodeId);

        JobExecutionContext.Builder builder = jobContextService.newBuilder(jobId());
        builder.addSubContext(localMergeExecutionNodeId, finalLocalMerge);

        if (localExecutionNodes == null || localExecutionNodes.isEmpty()) {
            // only the local merge happens locally so it is enough to just create that context.
            jobContextService.createContext(builder);
        } else {
            for (ExecutionNode executionNode : localExecutionNodes) {
                contextPreparer.prepare(jobId(), executionNode, builder);
            }
            JobExecutionContext context = jobContextService.createContext(builder);
            context.start();
        }
    }

    private void addCloseContextCallback(TransportCloseContextNodeAction transportCloseContextNodeAction,
                                         final List<List<ExecutionNode>> groupedExecutionNodes,
                                         final Set<String> server) {
        if (server.isEmpty()) {
            return;
        }
        final ContextCloser contextCloser = new ContextCloser(transportCloseContextNodeAction);
        Futures.addCallback(Futures.allAsList(results), new FutureCallback<List<TaskResult>>() {
            @Override
            public void onSuccess(@Nullable List<TaskResult> result) {
                // do nothing, contexts will be closed through fetch projection
            }

            @Override
            public void onFailure(@Nonnull Throwable t) {
                // if a failure happens, no fetch projection will be called, clean up contexts
                for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
                    for (ExecutionNode executionNode : executionNodeGroup) {
                        contextCloser.process(executionNode, server);
                    }
                }
            }
        });
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> result() {
        return results;
    }

    @Override
    public void upstreamResult(List<? extends ListenableFuture<TaskResult>> result) {
        throw new UnsupportedOperationException("ExecutionNodesTask doesn't support upstreamResult");
    }

    static boolean hasDirectResponse(List<List<ExecutionNode>> groupedExecutionNodes) {
        for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
            for (ExecutionNode executionNode : executionNodeGroup) {
                if (ExecutionNodes.hasDirectResponseDownstream(executionNode.downstreamNodes())) {
                    return true;
                }
            }
        }
        return false;
    }

    private static class DirectResponseListener implements ActionListener<JobResponse> {

        private final int bucketIdx;
        private final Streamer<?>[] streamer;
        private final List<PageDownstreamContext> pageDownstreamContexts;

        public DirectResponseListener(int bucketIdx, Streamer<?>[] streamer, List<PageDownstreamContext> pageDownstreamContexts) {
            this.bucketIdx = bucketIdx;
            this.streamer = streamer;
            this.pageDownstreamContexts = pageDownstreamContexts;
        }

        @Override
        public void onResponse(JobResponse jobResponse) {
            jobResponse.streamers(streamer);
            for (int i = 0; i < pageDownstreamContexts.size(); i++) {
                PageDownstreamContext pageDownstreamContext = pageDownstreamContexts.get(i);
                Bucket bucket = jobResponse.directResponse().get(i);
                if (bucket == null) {
                    pageDownstreamContext.failure(bucketIdx, new IllegalStateException("expected directResponse but didn't get one"));
                }
                pageDownstreamContext.setBucket(bucketIdx, bucket, true, new PageResultListener() {
                    @Override
                    public void needMore(boolean needMore) {
                        // can't page with directResult
                    }

                    @Override
                    public int buckedIdx() {
                        return bucketIdx;
                    }
                });
            }
        }

        @Override
        public void onFailure(Throwable e) {
            for (PageDownstreamContext pageDownstreamContext : pageDownstreamContexts) {
                pageDownstreamContext.failure(bucketIdx, e);
            }
        }
    }

    private static class FailureOnlyResponseListener implements ActionListener<JobResponse> {

        private final List<SettableFuture<TaskResult>> results;

        public FailureOnlyResponseListener(List<SettableFuture<TaskResult>> results) {
            this.results = results;
        }

        @Override
        public void onResponse(JobResponse jobResponse) {
            if (jobResponse.directResponse().size() > 0) {
                for (SettableFuture<TaskResult> result : results) {
                    result.setException(new IllegalStateException("Got a directResponse but didn't expect one"));
                }
            }
        }

        @Override
        public void onFailure(Throwable e) {
            // in the non-direct-response case the failure is pushed to downStreams
            LOGGER.warn(e.getMessage(), e);
        }
    }

    private static class ContextCloser extends ExecutionNodeVisitor<Set<String>, Void> {

        private final TransportCloseContextNodeAction transportCloseContextNodeAction;

        public ContextCloser(TransportCloseContextNodeAction transportCloseContextNodeAction) {
            this.transportCloseContextNodeAction = transportCloseContextNodeAction;
        }

        @Override
        public Void visitCollectNode(final CollectNode node, Set<String> nodeIds) {
            if (!node.keepContextForFetcher()) {
                return null;
            }

            LOGGER.trace("closing job context {} on {} nodes", node.jobId(), nodeIds.size());
            for (final String nodeId : nodeIds) {
                transportCloseContextNodeAction.execute(
                        nodeId,
                        new NodeCloseContextRequest(node.jobId(), node.executionNodeId()),
                        new ActionListener<NodeCloseContextResponse>() {

                    @Override
                    public void onResponse(NodeCloseContextResponse nodeCloseContextResponse) {
                    }

                    @Override
                    public void onFailure(Throwable e) {
                        LOGGER.warn("Closing job context {} failed on node {} with: {}", node.jobId(), nodeId, e.getMessage());
                    }
                });
            }
            return null;
        }
    }
}


File: sql/src/main/java/io/crate/executor/transport/TransportExecutor.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Iterables;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.action.job.ContextPreparer;
import io.crate.action.sql.DDLStatementDispatcher;
import io.crate.breaker.CrateCircuitBreakerService;
import io.crate.executor.*;
import io.crate.executor.task.DDLTask;
import io.crate.executor.task.NoopTask;
import io.crate.executor.transport.task.CreateTableTask;
import io.crate.executor.transport.task.DropTableTask;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.*;
import io.crate.jobs.JobContextService;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceResolver;
import io.crate.operation.ImplementationSymbolVisitor;
import io.crate.operation.PageDownstreamFactory;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.planner.*;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.PlanNode;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.node.StreamerVisitor;
import io.crate.planner.node.ddl.*;
import io.crate.planner.node.dml.*;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.Nullable;
import org.elasticsearch.common.breaker.CircuitBreaker;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Provider;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.threadpool.ThreadPool;

import java.util.ArrayList;
import java.util.Collection;
import java.util.List;
import java.util.UUID;

public class TransportExecutor implements Executor, TaskExecutor {

    private final Functions functions;
    private final TaskCollectingVisitor planVisitor;
    private Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider;
    private final NodeVisitor nodeVisitor;
    private final ThreadPool threadPool;

    private final ClusterService clusterService;
    private final JobContextService jobContextService;
    private final ContextPreparer contextPreparer;
    private final TransportActionProvider transportActionProvider;
    private final BulkRetryCoordinatorPool bulkRetryCoordinatorPool;

    private final ProjectionToProjectorVisitor globalProjectionToProjectionVisitor;

    // operation for handler side collecting
    private final CircuitBreaker circuitBreaker;

    private final PageDownstreamFactory pageDownstreamFactory;

    private final StreamerVisitor streamerVisitor;

    @Inject
    public TransportExecutor(Settings settings,
                             JobContextService jobContextService,
                             ContextPreparer contextPreparer,
                             TransportActionProvider transportActionProvider,
                             ThreadPool threadPool,
                             Functions functions,
                             ReferenceResolver referenceResolver,
                             PageDownstreamFactory pageDownstreamFactory,
                             Provider<DDLStatementDispatcher> ddlAnalysisDispatcherProvider,
                             ClusterService clusterService,
                             CrateCircuitBreakerService breakerService,
                             BulkRetryCoordinatorPool bulkRetryCoordinatorPool,
                             StreamerVisitor streamerVisitor) {
        this.jobContextService = jobContextService;
        this.contextPreparer = contextPreparer;
        this.transportActionProvider = transportActionProvider;
        this.pageDownstreamFactory = pageDownstreamFactory;
        this.threadPool = threadPool;
        this.functions = functions;
        this.ddlAnalysisDispatcherProvider = ddlAnalysisDispatcherProvider;
        this.clusterService = clusterService;
        this.bulkRetryCoordinatorPool = bulkRetryCoordinatorPool;
        this.streamerVisitor = streamerVisitor;
        this.nodeVisitor = new NodeVisitor();
        this.planVisitor = new TaskCollectingVisitor();
        this.circuitBreaker = breakerService.getBreaker(CrateCircuitBreakerService.QUERY_BREAKER);
        ImplementationSymbolVisitor globalImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver, functions, RowGranularity.CLUSTER);
        this.globalProjectionToProjectionVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                globalImplementationSymbolVisitor);
    }

    @Override
    public Job newJob(Plan plan) {
        final Job job = new Job();
        List<Task> tasks = planVisitor.process(plan, job);
        job.addTasks(tasks);
        return job;
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Job job) {
        assert job.tasks().size() > 0;
        return execute(job.tasks());

    }

    @Override
    public List<Task> newTasks(PlanNode planNode, UUID jobId) {
        return planNode.accept(nodeVisitor, jobId);
    }

    @Override
    public List<? extends ListenableFuture<TaskResult>> execute(Collection<Task> tasks) {
        Task lastTask = null;
        assert tasks.size() > 0 : "need at least one task to execute";
        for (Task task : tasks) {
            // chaining tasks
            if (lastTask != null) {
                task.upstreamResult(lastTask.result());
            }
            task.start();
            lastTask = task;
        }
        assert lastTask != null;
        return lastTask.result();
    }

    class TaskCollectingVisitor extends PlanVisitor<Job, List<Task>> {

        @Override
        public List<Task> visitIterablePlan(IterablePlan plan, Job job) {
            List<Task> tasks = new ArrayList<>();
            for (PlanNode planNode : plan) {
                tasks.addAll(planNode.accept(nodeVisitor, job.id()));
            }
            return tasks;
        }

        @Override
        public List<Task> visitNoopPlan(NoopPlan plan, Job job) {
            return ImmutableList.<Task>of(NoopTask.INSTANCE);
        }

        @Override
        public List<Task> visitGlobalAggregate(GlobalAggregate plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitCollectAndMerge(CollectAndMerge plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitQueryAndFetch(QueryAndFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitCountPlan(CountPlan countPlan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, countPlan.countNode(), countPlan.mergeNode()));
        }

        private Task createExecutableNodesTask(Job job, ExecutionNode executionNode, @Nullable MergeNode localMergeNode) {
            return createExecutableNodesTask(job,
                    ImmutableList.<List<ExecutionNode>>of(ImmutableList.of(executionNode)),
                    localMergeNode == null ? null : ImmutableList.of(localMergeNode));
        }

        private ExecutionNodesTask createExecutableNodesTask(Job job, List<List<ExecutionNode>> groupedExecutionNodes, @Nullable List<MergeNode> localMergeNodes) {
            for (List<ExecutionNode> executionNodeGroup : groupedExecutionNodes) {
                for (ExecutionNode executionNode : executionNodeGroup) {
                    executionNode.jobId(job.id());
                }
            }
            if (localMergeNodes != null) {
                for (MergeNode localMergeNode : localMergeNodes) {
                    localMergeNode.jobId(job.id());
                }
            }
            return new ExecutionNodesTask(
                    job.id(),
                    clusterService,
                    contextPreparer,
                    jobContextService,
                    pageDownstreamFactory,
                    threadPool,
                    transportActionProvider.transportJobInitAction(),
                    transportActionProvider.transportCloseContextNodeAction(),
                    streamerVisitor,
                    circuitBreaker,
                    localMergeNodes,
                    groupedExecutionNodes
            );
        }

        @Override
        public List<Task> visitNonDistributedGroupBy(NonDistributedGroupBy plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.localMergeNode()));
        }

        @Override
        public List<Task> visitUpsert(Upsert plan, Job job) {
            if (plan.nodes().size() == 1 && plan.nodes().get(0) instanceof IterablePlan) {
                return process(plan.nodes().get(0), job);
            }

            List<List<ExecutionNode>> groupedExecutionNodes = new ArrayList<>(plan.nodes().size());
            List<MergeNode> mergeNodes = new ArrayList<>(plan.nodes().size());
            for (Plan subPlan : plan.nodes()) {
                assert subPlan instanceof CollectAndMerge;
                groupedExecutionNodes.add(ImmutableList.<ExecutionNode>of(((CollectAndMerge) subPlan).collectNode()));
                mergeNodes.add(((CollectAndMerge) subPlan).localMergeNode());
            }
            ExecutionNodesTask task = createExecutableNodesTask(job, groupedExecutionNodes, mergeNodes);
            task.rowCountResult(true);
            return ImmutableList.<Task>of(task);
        }

        @Override
        public List<Task> visitDistributedGroupBy(DistributedGroupBy plan, Job job) {
            plan.collectNode().jobId(job.id());
            plan.reducerMergeNode().jobId(job.id());
            MergeNode localMergeNode = plan.localMergeNode();
            List<MergeNode> mergeNodes = null;
            if (localMergeNode != null) {
                localMergeNode.jobId(job.id());
                mergeNodes = ImmutableList.of(localMergeNode);
            }
            return ImmutableList.<Task>of(
                    createExecutableNodesTask(job,
                            ImmutableList.<List<ExecutionNode>>of(
                                    ImmutableList.<ExecutionNode>of(
                                            plan.collectNode(),
                                            plan.reducerMergeNode())),
                            mergeNodes));
        }

        @Override
        public List<Task> visitInsertByQuery(InsertFromSubQuery node, Job job) {
            List<Task> tasks = process(node.innerPlan(), job);
            if(node.handlerMergeNode().isPresent()) {
                // TODO: remove this hack
                Task previousTask = Iterables.getLast(tasks);
                if (previousTask instanceof ExecutionNodesTask) {
                    ((ExecutionNodesTask) previousTask).mergeNodes(ImmutableList.of(node.handlerMergeNode().get()));
                } else {
                    ArrayList<Task> tasks2 = new ArrayList<>(tasks);
                    tasks2.addAll(nodeVisitor.visitMergeNode(node.handlerMergeNode().get(), job.id()));
                    return tasks2;
                }
            }
            return tasks;
        }

        @Override
        public List<Task> visitQueryThenFetch(QueryThenFetch plan, Job job) {
            return ImmutableList.of(createExecutableNodesTask(job, plan.collectNode(), plan.mergeNode()));
        }

        @Override
        public List<Task> visitKillPlan(KillPlan killPlan, Job job) {
            return ImmutableList.<Task>of(new KillTask(
                    clusterService,
                    transportActionProvider.transportKillAllNodeAction(),
                    job.id()));
        }
    }

    class NodeVisitor extends PlanNodeVisitor<UUID, ImmutableList<Task>> {

        private ImmutableList<Task> singleTask(Task task) {
            return ImmutableList.of(task);
        }

        @Override
        public ImmutableList<Task> visitGenericDDLNode(GenericDDLNode node, UUID jobId) {
            return singleTask(new DDLTask(jobId, ddlAnalysisDispatcherProvider.get(), node));
        }

        @Override
        public ImmutableList<Task> visitESGetNode(ESGetNode node, UUID jobId) {
            return singleTask(new ESGetTask(
                    jobId,
                    functions,
                    globalProjectionToProjectionVisitor,
                    transportActionProvider.transportMultiGetAction(),
                    transportActionProvider.transportGetAction(),
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteByQueryNode(ESDeleteByQueryNode node, UUID jobId) {
            return singleTask(new ESDeleteByQueryTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteByQueryAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitESDeleteNode(ESDeleteNode node, UUID jobId) {
            return singleTask(new ESDeleteTask(
                    jobId,
                    node,
                    transportActionProvider.transportDeleteAction(),
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitCreateTableNode(CreateTableNode node, UUID jobId) {
            return singleTask(new CreateTableTask(
                            jobId,
                            clusterService,
                            transportActionProvider.transportCreateIndexAction(),
                            transportActionProvider.transportDeleteIndexAction(),
                            transportActionProvider.transportPutIndexTemplateAction(),
                            node)
            );
        }

        @Override
        public ImmutableList<Task> visitESCreateTemplateNode(ESCreateTemplateNode node, UUID jobId) {
            return singleTask(new ESCreateTemplateTask(jobId,
                    node,
                    transportActionProvider.transportPutIndexTemplateAction()));
        }

        @Override
        public ImmutableList<Task> visitSymbolBasedUpsertByIdNode(SymbolBasedUpsertByIdNode node, UUID jobId) {
            return singleTask(new SymbolBasedUpsertByIdTask(jobId,
                    clusterService,
                    clusterService.state().metaData().settings(),
                    transportActionProvider.symbolBasedTransportShardUpsertActionDelegate(),
                    transportActionProvider.transportCreateIndexAction(),
                    transportActionProvider.transportBulkCreateIndicesAction(),
                    bulkRetryCoordinatorPool,
                    node,
                    jobContextService));
        }

        @Override
        public ImmutableList<Task> visitDropTableNode(DropTableNode node, UUID jobId) {
            return singleTask(new DropTableTask(jobId,
                    transportActionProvider.transportDeleteIndexTemplateAction(),
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESDeletePartitionNode(ESDeletePartitionNode node, UUID jobId) {
            return singleTask(new ESDeletePartitionTask(jobId,
                    transportActionProvider.transportDeleteIndexAction(),
                    node));
        }

        @Override
        public ImmutableList<Task> visitESClusterUpdateSettingsNode(ESClusterUpdateSettingsNode node, UUID jobId) {
            return singleTask(new ESClusterUpdateSettingsTask(
                    jobId,
                    transportActionProvider.transportClusterUpdateSettingsAction(),
                    node));
        }

        @Override
        protected ImmutableList<Task> visitPlanNode(PlanNode node, UUID jobId) {
            throw new UnsupportedOperationException(
                    String.format("Can't generate job/task for planNode %s", node));
        }
    }
}


File: sql/src/main/java/io/crate/executor/transport/distributed/DistributedResultRequest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport.distributed;

import io.crate.Streamer;
import io.crate.core.collections.Bucket;
import io.crate.exceptions.UnknownUpstreamFailure;
import io.crate.executor.transport.StreamBucket;
import org.elasticsearch.common.io.ThrowableObjectInputStream;
import org.elasticsearch.common.io.ThrowableObjectOutputStream;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.transport.TransportRequest;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.UUID;

public class DistributedResultRequest extends TransportRequest {

    private int executionNodeId;
    private int bucketIdx;

    private Streamer<?>[] streamers;
    private Bucket rows;
    private UUID jobId;
    private boolean isLast = true;

    private Throwable throwable = null;

    public DistributedResultRequest() {
    }

    public DistributedResultRequest(UUID jobId, int executionNodeId, int bucketIdx, Streamer<?>[] streamers) {
        this.jobId = jobId;
        this.executionNodeId = executionNodeId;
        this.bucketIdx = bucketIdx;
        this.streamers = streamers;
    }

    public UUID jobId() {
        return jobId;
    }

    public int executionNodeId() {
        return executionNodeId;
    }

    public int bucketIdx() {
        return bucketIdx;
    }

    public void streamers(Streamer<?>[] streamers) {
        if (rows instanceof StreamBucket) {
            assert streamers != null;
            ((StreamBucket) rows).streamers(streamers);
        }
        this.streamers = streamers;
    }

    public boolean rowsCanBeRead(){
        if (rows instanceof StreamBucket){
            return streamers != null;
        }
        return true;
    }

    public Bucket rows() {
        return rows;
    }

    public void rows(Bucket rows) {
        this.rows = rows;
    }

    public boolean isLast() {
        return isLast;
    }

    public void isLast(boolean isLast) {
        this.isLast = isLast;
    }

    public void throwable(Throwable throwable) {
        this.throwable = throwable;
    }

    @Nullable
    public Throwable throwable() {
        return throwable;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        bucketIdx = in.readVInt();
        isLast = in.readBoolean();

        boolean failure = in.readBoolean();
        if (failure) {
            ThrowableObjectInputStream tis = new ThrowableObjectInputStream(in);
            try {
                throwable = (Throwable) tis.readObject();
            } catch (ClassNotFoundException e) {
                throwable = new UnknownUpstreamFailure();
            }
        } else {
            StreamBucket bucket = new StreamBucket(streamers);
            bucket.readFrom(in);
            rows = bucket;
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);
        out.writeVInt(bucketIdx);
        out.writeBoolean(isLast);

        boolean failure = throwable != null;
        out.writeBoolean(failure);
        if (failure) {
            ThrowableObjectOutputStream too = new ThrowableObjectOutputStream(out);
            too.writeObject(throwable);
        } else {
            // TODO: we should not rely on another bucket in this class and instead write to the stream directly
            StreamBucket.writeBucket(out, streamers, rows);
        }
    }
}


File: sql/src/main/java/io/crate/executor/transport/distributed/DistributingDownstream.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport.distributed;

import io.crate.Constants;
import io.crate.Streamer;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.Row;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;

import java.io.IOException;
import java.util.Collection;
import java.util.Deque;
import java.util.UUID;
import java.util.concurrent.CancellationException;
import java.util.concurrent.ConcurrentLinkedDeque;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

public class DistributingDownstream extends ResultProviderBase {

    private static final ESLogger LOGGER = Loggers.getLogger(DistributingDownstream.class);

    private final UUID jobId;
    private final TransportDistributedResultAction transportDistributedResultAction;
    private final MultiBucketBuilder bucketBuilder;
    private Downstream[] downstreams;
    private final AtomicInteger finishedDownstreams = new AtomicInteger(0);

    public DistributingDownstream(UUID jobId,
                                  int targetExecutionNodeId,
                                  int bucketIdx,
                                  Collection<String> downstreamNodeIds,
                                  TransportDistributedResultAction transportDistributedResultAction,
                                  Streamer<?>[] streamers) {
        this.jobId = jobId;
        this.transportDistributedResultAction = transportDistributedResultAction;

        downstreams = new Downstream[downstreamNodeIds.size()];
        bucketBuilder = new MultiBucketBuilder(streamers, downstreams.length);

        int idx = 0;
        for (String downstreamNodeId : downstreamNodeIds) {
            downstreams[idx] = new Downstream(downstreamNodeId, jobId, targetExecutionNodeId, bucketIdx, streamers);
            idx++;
        }
    }

    @Override
    public boolean setNextRow(Row row) {
        if (allDownstreamsFinished()) {
            return false;
        }
        try {
            int downstreamIdx = bucketBuilder.getBucket(row);
            // only collect if downstream want more rows, otherwise just ignore the row
            if (downstreams[downstreamIdx].wantMore.get()) {
                bucketBuilder.setNextRow(downstreamIdx, row);
                sendRequestIfNeeded(downstreamIdx);
            }
        } catch (IOException e) {
            fail(e);
            return false;
        }
        return true;
    }

    protected void sendRequestIfNeeded(int downstreamIdx) {
        int size = bucketBuilder.size(downstreamIdx);
        if (size >= Constants.PAGE_SIZE || remainingUpstreams.get() <= 0) {
            Downstream downstream = downstreams[downstreamIdx];
            downstream.bucketQueue.add(bucketBuilder.build(downstreamIdx));
            sendRequest(downstream);
        }
    }

    protected void onAllUpstreamsFinished() {
        for (int i = 0; i < downstreams.length; i++) {
            sendRequestIfNeeded(i);
        }
    }

    private void forwardFailures(Throwable throwable) {
        for (Downstream downstream : downstreams) {
            downstream.request.throwable(throwable);
            sendRequest(downstream.request, downstream);
        }
    }

    private boolean allDownstreamsFinished() {
        return finishedDownstreams.get() == downstreams.length;
    }

    private void sendRequest(Downstream downstream) {
        if (downstream.requestPending.compareAndSet(false, true)) {
            DistributedResultRequest request = downstream.request;
            Deque<Bucket> queue = downstream.bucketQueue;
            int size = queue.size();
            if (size > 0) {
                request.rows(queue.poll());
            } else {
                request.rows(Bucket.EMPTY);
            }
            request.isLast(!(size > 1 || remainingUpstreams.get() > 0));
            sendRequest(request, downstream);
        }
    }

    private void sendRequest(final DistributedResultRequest request, final Downstream downstream) {
        if (LOGGER.isTraceEnabled()) {
            LOGGER.trace("[{}] sending distributing collect request to {}, isLast? {} ...",
                    jobId.toString(),
                    downstream.node, request.isLast());
        }
        try {
            transportDistributedResultAction.pushResult(
                    downstream.node,
                    request,
                    new DistributedResultResponseActionListener(downstream)
            );
        } catch (IllegalArgumentException e) {
            LOGGER.error(e.getMessage(), e);
            downstream.wantMore.set(false);
        }
    }

    @Override
    public Bucket doFinish() {
        onAllUpstreamsFinished();
        return null;
    }

    @Override
    public Throwable doFail(Throwable t) {
        if (t instanceof CancellationException) {
            // fail without sending anything
            LOGGER.debug("{} killed", getClass().getSimpleName());
        } else {
            forwardFailures(t);
        }
        return t;
    }

    static class Downstream {

        final AtomicBoolean wantMore = new AtomicBoolean(true);
        final AtomicBoolean requestPending = new AtomicBoolean(false);
        final Deque<Bucket> bucketQueue = new ConcurrentLinkedDeque<>();
        final DistributedResultRequest request;
        final String node;

        public Downstream(String node,
                          UUID jobId,
                          int targetExecutionNodeId,
                          int bucketIdx,
                          Streamer<?>[] streamers) {
            this.node = node;
            this.request = new DistributedResultRequest(jobId, targetExecutionNodeId, bucketIdx, streamers);
        }
    }

    private class DistributedResultResponseActionListener implements ActionListener<DistributedResultResponse> {
        private final Downstream downstream;

        public DistributedResultResponseActionListener(Downstream downstream) {
            this.downstream = downstream;
        }

        @Override
        public void onResponse(DistributedResultResponse response) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("[{}] successfully sent distributing collect request to {}, needMore? {}",
                        jobId,
                        downstream.node,
                        response.needMore());
            }

            downstream.wantMore.set(response.needMore());
            if (!response.needMore()) {
                finishedDownstreams.incrementAndGet();
                // clean-up queue because no more rows are wanted
                downstream.bucketQueue.clear();
            } else {
                // send next request or final empty closing one
                downstream.requestPending.set(false);
                sendRequest(downstream);
            }
        }

        @Override
        public void onFailure(Throwable exp) {
            LOGGER.error("[{}] Exception sending distributing collect request to {}", exp, jobId, downstream.node);
            downstream.wantMore.set(false);
            downstream.bucketQueue.clear();
            finishedDownstreams.incrementAndGet();
        }
    }
}


File: sql/src/main/java/io/crate/executor/transport/distributed/TransportDistributedResultAction.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport.distributed;

import io.crate.executor.transport.DefaultTransportResponseHandler;
import io.crate.executor.transport.NodeAction;
import io.crate.executor.transport.NodeActionRequestHandler;
import io.crate.executor.transport.Transports;
import io.crate.jobs.JobContextService;
import io.crate.jobs.JobExecutionContext;
import io.crate.jobs.PageDownstreamContext;
import io.crate.operation.PageResultListener;
import io.crate.planner.node.ExecutionNode;
import org.elasticsearch.action.ActionListener;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.threadpool.ThreadPool;
import org.elasticsearch.transport.TransportService;

import java.util.Locale;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;


public class TransportDistributedResultAction implements NodeAction<DistributedResultRequest, DistributedResultResponse> {

    private static final ESLogger LOGGER = Loggers.getLogger(TransportDistributedResultAction.class);

    public final static String DISTRIBUTED_RESULT_ACTION = "crate/sql/node/merge/add_rows";
    private final static String EXECUTOR_NAME = ThreadPool.Names.SEARCH;

    private final Transports transports;
    private final JobContextService jobContextService;
    private final ScheduledExecutorService scheduler;

    @Inject
    public TransportDistributedResultAction(Transports transports,
                                            JobContextService jobContextService,
                                            ThreadPool threadPool,
                                            TransportService transportService) {
        this.transports = transports;
        this.jobContextService = jobContextService;
        scheduler = threadPool.scheduler();
        transportService.registerHandler(DISTRIBUTED_RESULT_ACTION, new NodeActionRequestHandler<DistributedResultRequest, DistributedResultResponse>(this) {
            @Override
            public DistributedResultRequest newInstance() {
                return new DistributedResultRequest();
            }
        });
    }

    public void pushResult(String node, DistributedResultRequest request, ActionListener<DistributedResultResponse> listener) {
        transports.executeLocalOrWithTransport(this, node, request, listener,
                new DefaultTransportResponseHandler<DistributedResultResponse>(listener, EXECUTOR_NAME) {
                    @Override
                    public DistributedResultResponse newInstance() {
                        return new DistributedResultResponse();
                    }
                });
    }

    @Override
    public String actionName() {
        return DISTRIBUTED_RESULT_ACTION;
    }

    @Override
    public String executorName() {
        return EXECUTOR_NAME;
    }

    @Override
    public void nodeOperation(DistributedResultRequest request,
                              ActionListener<DistributedResultResponse> listener) {
        nodeOperation(request, listener, 0);
    }

    private void nodeOperation(final DistributedResultRequest request,
                               final ActionListener<DistributedResultResponse> listener,
                               final int retry) {
        if (request.executionNodeId() == ExecutionNode.NO_EXECUTION_NODE) {
            listener.onFailure(new IllegalStateException("request must contain a valid executionNodeId"));
            return;
        }
        JobExecutionContext context = jobContextService.getContextOrNull(request.jobId());
        if (context == null) {
            retryOrFailureResponse(request, listener, retry);
            return;
        }

        PageDownstreamContext pageDownstreamContext = context.getSubContextOrNull(request.executionNodeId());
        if (pageDownstreamContext == null) {
            // this is currently sometimes the case when upstreams send failures more than once
            listener.onFailure(new IllegalStateException(String.format(Locale.ENGLISH,
                    "Couldn't find pageDownstreamContext for %d", request.executionNodeId())));
            return;
        }

        Throwable throwable = request.throwable();
        if (throwable == null) {
            request.streamers(pageDownstreamContext.streamer());
            pageDownstreamContext.setBucket(
                    request.bucketIdx(),
                    request.rows(),
                    request.isLast(),
                    new SendResponsePageResultListener(listener, request));
        } else {
            pageDownstreamContext.failure(request.bucketIdx(), throwable);
            listener.onResponse(new DistributedResultResponse(false));
        }
    }

    private void retryOrFailureResponse(DistributedResultRequest request,
                                        ActionListener<DistributedResultResponse> listener,
                                        int retry) {
        if (retry > 20) {
            listener.onFailure(new IllegalStateException(
                    String.format("Couldn't find JobExecutionContext for %s", request.jobId())));
        } else {
            scheduler.schedule(new NodeOperationRunnable(request, listener, retry), (retry + 1) * 2, TimeUnit.MILLISECONDS);
        }
    }

    private static class SendResponsePageResultListener implements PageResultListener {
        private final ActionListener<DistributedResultResponse> listener;
        private final DistributedResultRequest request;

        public SendResponsePageResultListener(ActionListener<DistributedResultResponse> listener, DistributedResultRequest request) {
            this.listener = listener;
            this.request = request;
        }

        @Override
        public void needMore(boolean needMore) {
            LOGGER.trace("sending needMore response, need more? {}", needMore);
            listener.onResponse(new DistributedResultResponse(needMore));
        }

        @Override
        public int buckedIdx() {
            return request.bucketIdx();
        }
    }

    private class NodeOperationRunnable implements Runnable {
        private final DistributedResultRequest request;
        private final ActionListener<DistributedResultResponse> listener;
        private final int retry;

        public NodeOperationRunnable(DistributedResultRequest request, ActionListener<DistributedResultResponse> listener, int retry) {
            this.request = request;
            this.listener = listener;
            this.retry = retry;
        }

        @Override
        public void run() {
            nodeOperation(request, listener, retry + 1);
        }
    }
}


File: sql/src/main/java/io/crate/jobs/PageDownstreamContext.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.jobs;

import com.google.common.util.concurrent.SettableFuture;
import io.crate.Streamer;
import io.crate.breaker.RamAccountingContext;
import io.crate.core.collections.Bucket;
import io.crate.core.collections.BucketPage;
import io.crate.operation.PageConsumeListener;
import io.crate.operation.PageDownstream;
import io.crate.operation.PageResultListener;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;

import java.util.ArrayList;
import java.util.BitSet;
import java.util.concurrent.CancellationException;
import java.util.concurrent.atomic.AtomicBoolean;

public class PageDownstreamContext implements ExecutionSubContext, ExecutionState {

    private static final ESLogger LOGGER = Loggers.getLogger(PageDownstreamContext.class);

    private final Object lock = new Object();
    private String name;
    private final PageDownstream pageDownstream;
    private final Streamer<?>[] streamer;
    private final RamAccountingContext ramAccountingContext;
    private final int numBuckets;
    private final ArrayList<SettableFuture<Bucket>> bucketFutures;
    private final BitSet allFuturesSet;
    private final BitSet exhausted;
    private final ArrayList<PageResultListener> listeners = new ArrayList<>();
    private final ArrayList<ContextCallback> callbacks = new ArrayList<>(1);
    private final AtomicBoolean closed = new AtomicBoolean(false);
    private volatile boolean isKilled = false;


    public PageDownstreamContext(String name,
                                 PageDownstream pageDownstream,
                                 Streamer<?>[] streamer,
                                 RamAccountingContext ramAccountingContext,
                                 int numBuckets) {
        this.name = name;
        this.pageDownstream = pageDownstream;
        this.streamer = streamer;
        this.ramAccountingContext = ramAccountingContext;
        this.numBuckets = numBuckets;
        bucketFutures = new ArrayList<>(numBuckets);
        allFuturesSet = new BitSet(numBuckets);
        exhausted = new BitSet(numBuckets);
        initBucketFutures();
    }

    private void initBucketFutures() {
        bucketFutures.clear();
        for (int i = 0; i < numBuckets; i++) {
            bucketFutures.add(SettableFuture.<Bucket>create());
        }
    }

    private boolean pageEmpty() {
        return allFuturesSet.cardinality() == 0;
    }

    private boolean allExhausted() {
        return exhausted.cardinality() == numBuckets;
    }

    private boolean isExhausted(int bucketIdx) {
        return exhausted.get(bucketIdx);
    }

    public void setBucket(int bucketIdx, Bucket rows, boolean isLast, PageResultListener pageResultListener) {
        synchronized (listeners) {
            listeners.add(pageResultListener);
        }
        synchronized (lock) {
            LOGGER.trace("setBucket: {}", bucketIdx);
            if (allFuturesSet.get(bucketIdx)) {
                pageDownstream.fail(new IllegalStateException("May not set the same bucket of a page more than once"));
                return;
            }

            if (pageEmpty()) {
                LOGGER.trace("calling nextPage");
                pageDownstream.nextPage(new BucketPage(bucketFutures), new ResultListenerBridgingConsumeListener());
            }
            setExhaustedUpstreams();

            if (isLast) {
                exhausted.set(bucketIdx);
            }
            bucketFutures.get(bucketIdx).set(rows);
            allFuturesSet.set(bucketIdx);

            clearPageIfFull();
        }
    }

    public synchronized void failure(int bucketIdx, Throwable throwable) {
        // can't trigger failure on pageDownstream immediately as it would remove the context which the other
        // upstreams still require
        synchronized (lock) {
            LOGGER.trace("failure: bucket: {} {}", bucketIdx, throwable);
            if (allFuturesSet.get(bucketIdx)) {
                pageDownstream.fail(new IllegalStateException("May not set the same bucket %d of a page more than once"));
                return;
            }
            if (pageEmpty()) {
                LOGGER.trace("calling nextPage");
                pageDownstream.nextPage(new BucketPage(bucketFutures), new ResultListenerBridgingConsumeListener());
            }
            setExhaustedUpstreams();

            LOGGER.trace("failure: {}", bucketIdx);
            exhausted.set(bucketIdx);
            bucketFutures.get(bucketIdx).setException(throwable);
            allFuturesSet.set(bucketIdx);
            clearPageIfFull();
        }
    }

    private void clearPageIfFull() {
        if (allFuturesSet.cardinality() == numBuckets) {
            allFuturesSet.clear();
            initBucketFutures();
        }
    }

    /**
     * need to set the futures of all upstreams that are exhausted as there won't come any more buckets from those upstreams
     */
    private void setExhaustedUpstreams() {
        for (int i = 0; i < exhausted.size(); i++) {
            if (exhausted.get(i)) {
                bucketFutures.get(i).set(Bucket.EMPTY);
                allFuturesSet.set(i);
            }
        }
    }

    public Streamer<?>[] streamer() {
        return streamer;
    }

    public void finish() {
        LOGGER.trace("calling finish on pageDownstream {}", pageDownstream);
        if (!closed.getAndSet(true)) {
            for (ContextCallback contextCallback : callbacks) {
                contextCallback.onClose(null, -1L);
            }
            pageDownstream.finish();
            ramAccountingContext.close();
        } else {
            LOGGER.warn("called finish on an already closed PageDownstreamContext");
        }
    }

    public void addCallback(ContextCallback contextCallback) {
        assert !closed.get() : "may not add a callback on a closed context";
        callbacks.add(contextCallback);
    }

    @Override
    public void start() {
        // no-op
    }

    @Override
    public void close() {
        finish();
    }

    @Override
    public void kill() {
        isKilled = true;
        if (!closed.getAndSet(true)) {
            CancellationException cancellationException = new CancellationException();
            for (ContextCallback contextCallback : callbacks) {
                contextCallback.onClose(cancellationException, -1L);
            }
            pageDownstream.fail(cancellationException);
            ramAccountingContext.close();
        } else {
            LOGGER.warn("called kill on an already closed PageDownstreamContext");
        }
    }

    @Override
    public String name() {
        return name;
    }

    @Override
    public boolean isKilled() {
        return isKilled;
    }

    private class ResultListenerBridgingConsumeListener implements PageConsumeListener {

        @Override
        public void needMore() {
            boolean allExhausted = allExhausted();
            LOGGER.trace("allExhausted: {}", allExhausted);
            synchronized (listeners) {
                LOGGER.trace("calling needMore on all listeners({})", listeners.size());
                for (PageResultListener listener : listeners) {
                    if (allExhausted) {
                        listener.needMore(false);
                    } else {
                        listener.needMore(!isExhausted(listener.buckedIdx()));
                    }
                }
                listeners.clear();
            }
            if (allExhausted) {
                PageDownstreamContext.this.finish();
            }
        }

        @Override
        public void finish() {
            synchronized (listeners) {
                LOGGER.trace("calling finish() on all listeners({})", listeners.size());
                for (PageResultListener listener : listeners) {
                    listener.needMore(false);
                }
                listeners.clear();
                PageDownstreamContext.this.finish();
            }
        }
    }
}


File: sql/src/main/java/io/crate/operation/collect/JobCollectContext.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.operation.collect;

import com.carrotsearch.hppc.IntObjectOpenHashMap;
import com.carrotsearch.hppc.cursors.IntObjectCursor;
import io.crate.breaker.RamAccountingContext;
import io.crate.jobs.ContextCallback;
import io.crate.jobs.ExecutionState;
import io.crate.jobs.ExecutionSubContext;
import io.crate.operation.RowDownstream;
import io.crate.operation.RowDownstreamHandle;
import io.crate.operation.RowUpstream;
import io.crate.planner.node.dql.CollectNode;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.index.engine.Engine;
import org.elasticsearch.index.shard.IndexShard;
import org.elasticsearch.index.shard.ShardId;

import javax.annotation.Nullable;
import java.util.ArrayList;
import java.util.Iterator;
import java.util.Locale;
import java.util.UUID;
import java.util.concurrent.CancellationException;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentMap;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicInteger;

public class JobCollectContext implements ExecutionSubContext, RowUpstream, ExecutionState {

    private final UUID id;
    private final CollectNode collectNode;
    private final CollectOperation collectOperation;
    private final RamAccountingContext ramAccountingContext;
    private final RowDownstream downstream;

    private final IntObjectOpenHashMap<JobQueryShardContext> queryContexts = new IntObjectOpenHashMap<>();
    private final IntObjectOpenHashMap<JobFetchShardContext> fetchContexts = new IntObjectOpenHashMap<>();
    private final ConcurrentMap<ShardId, EngineSearcherDelegate> shardsSearcherMap = new ConcurrentHashMap<>();
    private final AtomicInteger activeQueryContexts = new AtomicInteger(0);
    private final AtomicInteger activeFetchContexts = new AtomicInteger(0);
    private final Object subContextLock = new Object();

    private final AtomicBoolean closed = new AtomicBoolean(false);
    private final ArrayList<ContextCallback> contextCallbacks = new ArrayList<>(1);

    private volatile boolean isKilled = false;
    private long usedBytesOfQueryPhase = 0L;

    private static final ESLogger LOGGER = Loggers.getLogger(JobCollectContext.class);

    public JobCollectContext(UUID jobId,
                             CollectNode collectNode,
                             CollectOperation collectOperation,
                             RamAccountingContext ramAccountingContext,
                             RowDownstream downstream) {
        id = jobId;
        this.collectNode = collectNode;
        this.collectOperation = collectOperation;
        this.ramAccountingContext = ramAccountingContext;
        this.downstream = downstream;
    }

    @Override
    public void addCallback(ContextCallback contextCallback) {
        assert !closed.get() : "may not add a callback on a closed context";
        contextCallbacks.add(contextCallback);
    }

    public void addContext(int jobSearchContextId, JobQueryShardContext shardQueryContext) {
        interruptIfKilled();
        if (closed.get()) {
            throw new IllegalStateException("context already closed");
        }
        synchronized (subContextLock) {
            if (queryContexts.put(jobSearchContextId, shardQueryContext) != null) {
                throw new IllegalArgumentException(String.format(Locale.ENGLISH,
                        "ExecutionSubContext for %d already added", jobSearchContextId));
            }
        }
        int numActive = activeQueryContexts.incrementAndGet();
        LOGGER.trace("adding query subContext {}, now there are {} query subContexts", jobSearchContextId, numActive);

        shardQueryContext.addCallback(new RemoveQueryContextCallback(jobSearchContextId));
        EngineSearcherDelegate searcherDelegate;
        try {
            searcherDelegate = acquireSearcher(shardQueryContext.indexShard());
            shardQueryContext.searcher(searcherDelegate);
        } catch (Exception e) {
            // all resources (specially engine searcher) must be closed
            shardQueryContext.close();
            throw e;
        }

        if (collectNode.keepContextForFetcher()) {
            JobFetchShardContext shardFetchContext = new JobFetchShardContext(
                    searcherDelegate,
                    shardQueryContext.searchContext());
            synchronized (subContextLock) {
                fetchContexts.put(jobSearchContextId, shardFetchContext);
            }
            shardFetchContext.addCallback(new RemoveFetchContextCallback(jobSearchContextId));

            int numActiveFetch = activeFetchContexts.incrementAndGet();
            LOGGER.trace("adding fetch subContext {}, now there are {} fetch subContexts", jobSearchContextId, numActiveFetch);
        }
    }


    @Nullable
    public JobFetchShardContext getFetchContext(int jobSearchContextId) {
        synchronized (subContextLock) {
            return fetchContexts.get(jobSearchContextId);
        }
    }

    @Override
    public void close() {
        if (closed.compareAndSet(false, true)) { // prevent double release
            synchronized (subContextLock) {
                if (queryContexts.size() != 0 || fetchContexts.size() != 0) {
                    LOGGER.trace("closing query subContexts {}", id);
                    Iterator<IntObjectCursor<JobQueryShardContext>> queryIterator = queryContexts.iterator();
                    while (queryIterator.hasNext()) {
                        queryIterator.next().value.close();
                    }
                    LOGGER.trace("closing fetch subContexts {}", id);
                    Iterator<IntObjectCursor<JobFetchShardContext>> fetchIterator = fetchContexts.iterator();
                    while (fetchIterator.hasNext()) {
                        fetchIterator.next().value.close();
                    }
                } else {
                    callContextCallback();
                }
            }
            ramAccountingContext.close();
        } else {
            LOGGER.trace("close called on an already closed JobCollectContext: {}", id);
        }
    }

    @Override
    public void kill() {
        isKilled = true;
        if (closed.compareAndSet(false, true)) { // prevent double release
            synchronized (subContextLock) {
                if (queryContexts.size() != 0 || fetchContexts.size() != 0) {
                    LOGGER.trace("killing query subContexts {}", id);
                    Iterator<IntObjectCursor<JobQueryShardContext>> queryIterator = queryContexts.iterator();
                    while (queryIterator.hasNext()) {
                        IntObjectCursor<JobQueryShardContext> cursor = queryIterator.next();
                        cursor.value.kill();
                    }
                    LOGGER.trace("killing fetch subContexts {}", id);
                    Iterator<IntObjectCursor<JobFetchShardContext>> fetchIterator = fetchContexts.iterator();
                    while (fetchIterator.hasNext()) {
                        fetchIterator.next().value.kill();
                    }
                } else {
                    callContextCallback();
                }
            }
            ramAccountingContext.close();
        } else {
            LOGGER.trace("killed called on an already closed JobCollectContext: {}", id);
        }
    }

    @Override
    public String name() {
        return collectNode.name();
    }

    @Override
        public void start() {
        startQueryPhase();
    }

    protected void startQueryPhase() {
        try {
            collectOperation.collect(collectNode, downstream, this);
        } catch (Throwable t) {
            RowDownstreamHandle rowDownstreamHandle = downstream.registerUpstream(this);
            rowDownstreamHandle.fail(t);
            close();
        }
    }

    @Override
    public boolean isKilled() {
        return isKilled;
    }

    public void interruptIfKilled() {
        if (isKilled) {
            throw new CancellationException();
        }
    }

    public RamAccountingContext ramAccountingContext() {
        return ramAccountingContext;
    }

    private void callContextCallback() {
        if (contextCallbacks.isEmpty()) {
            return;
        }
        if (activeQueryContexts.get() == 0 && activeFetchContexts.get() == 0) {
            for (ContextCallback contextCallback : contextCallbacks) {
                contextCallback.onClose(null, usedBytesOfQueryPhase);
            }
        }
    }

    /**
     * Try to find a {@link EngineSearcherDelegate} for the same shard.
     * If none is found, create new one with new acquired {@link Engine.Searcher}.
     */
    protected EngineSearcherDelegate acquireSearcher(IndexShard indexShard) {
        EngineSearcherDelegate engineSearcherDelegate;
        for (;;) {
            engineSearcherDelegate = shardsSearcherMap.get(indexShard.shardId());
            if (engineSearcherDelegate == null) {
                engineSearcherDelegate = new EngineSearcherDelegate(acquireNewSearcher(indexShard));
                if (shardsSearcherMap.putIfAbsent(indexShard.shardId(), engineSearcherDelegate) == null) {
                    return engineSearcherDelegate;
                }
            } else {
                return engineSearcherDelegate;
            }
        }
    }

    /**
     * Acquire a new searcher, wrapper method needed for simplified testing
     */
    protected Engine.Searcher acquireNewSearcher(IndexShard indexShard) {
        return EngineSearcher.getSearcherWithRetry(indexShard, "search", null);
    }


    class RemoveQueryContextCallback implements ContextCallback {

        private final int jobSearchContextId;

        public RemoveQueryContextCallback(int jobSearchContextId) {
            this.jobSearchContextId = jobSearchContextId;
        }

        @Override
        public void onClose(@Nullable Throwable error, long bytesUsed) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("[{}] Closing query subContext {}",
                        System.identityHashCode(queryContexts), jobSearchContextId);
            }

            JobQueryShardContext remove;
            synchronized (subContextLock) {
                remove = queryContexts.remove(jobSearchContextId);
            }
            int remaining;
            if (remove == null) {
                LOGGER.trace("Closed query context {} which was already closed.", jobSearchContextId);
                remaining = activeQueryContexts.get();
            } else {
                remaining = activeQueryContexts.decrementAndGet();

                if (collectNode.keepContextForFetcher()
                        && (remove.collector() == null || !remove.collector().producedRows() || remove.collector().failed())) {
                    // close fetch context on error or if query produced no rows

                    JobFetchShardContext fetchShardContext;
                    synchronized (subContextLock) {
                        fetchShardContext = fetchContexts.get(jobSearchContextId);
                    }
                    if (fetchShardContext != null) {
                        fetchShardContext.close();
                    }
                }
            }


            if (remaining == 0) {
                usedBytesOfQueryPhase = ramAccountingContext.totalBytes();
                ramAccountingContext.close();
                callContextCallback();
            }
        }
    }

    class RemoveFetchContextCallback implements ContextCallback {

        private final int jobSearchContextId;

        public RemoveFetchContextCallback(int jobSearchContextId) {
            this.jobSearchContextId = jobSearchContextId;
        }

        @Override
        public void onClose(@Nullable Throwable error, long bytesUsed) {
            if (LOGGER.isTraceEnabled()) {
                LOGGER.trace("[{}] Closing fetch subContext {}",
                        System.identityHashCode(fetchContexts), jobSearchContextId);
            }

            JobFetchShardContext remove;
            synchronized (subContextLock) {
                remove = fetchContexts.remove(jobSearchContextId);
            }
            int remaining;
            if (remove == null) {
                LOGGER.trace("Closed fetch context {} which was already closed.", jobSearchContextId);
                remaining = activeFetchContexts.get();
            } else {
                remaining = activeFetchContexts.decrementAndGet();
            }
            if (remaining == 0) {
                callContextCallback();
            }
        }
    }

}


File: sql/src/main/java/io/crate/operation/collect/MapSideDataCollectOperation.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.operation.collect;

import com.google.common.base.Function;
import com.google.common.base.Throwables;
import com.google.common.collect.ImmutableMap;
import com.google.common.collect.Lists;
import com.google.common.util.concurrent.*;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.exceptions.TableUnknownException;
import io.crate.exceptions.UnhandledServerException;
import io.crate.executor.transport.TransportActionProvider;
import io.crate.metadata.Functions;
import io.crate.metadata.ReferenceResolver;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.*;
import io.crate.operation.collect.files.FileCollectInputSymbolVisitor;
import io.crate.operation.collect.files.FileInputFactory;
import io.crate.operation.collect.files.FileReadingCollector;
import io.crate.operation.projectors.FlatProjectorChain;
import io.crate.operation.projectors.ProjectionToProjectorVisitor;
import io.crate.operation.projectors.ResultProvider;
import io.crate.operation.projectors.ResultProviderFactory;
import io.crate.operation.reference.file.FileLineReferenceResolver;
import io.crate.operation.reference.sys.node.NodeSysExpression;
import io.crate.operation.reference.sys.node.NodeSysReferenceResolver;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.symbol.ValueSymbolVisitor;
import org.elasticsearch.action.bulk.BulkRetryCoordinatorPool;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Injector;
import org.elasticsearch.common.inject.Singleton;
import org.elasticsearch.common.logging.ESLogger;
import org.elasticsearch.common.logging.Loggers;
import org.elasticsearch.common.settings.Settings;
import org.elasticsearch.index.IndexService;
import org.elasticsearch.index.IndexShardMissingException;
import org.elasticsearch.indices.IndexMissingException;
import org.elasticsearch.indices.IndicesService;
import org.elasticsearch.threadpool.ThreadPool;

import javax.annotation.Nullable;
import java.util.*;
import java.util.concurrent.Callable;
import java.util.concurrent.CancellationException;
import java.util.concurrent.RejectedExecutionException;
import java.util.concurrent.ThreadPoolExecutor;

/**
 * collect local data from node/shards/docs on nodes where the data resides (aka Mapper nodes)
 */
@Singleton
public class MapSideDataCollectOperation implements CollectOperation, RowUpstream {

    private static final ESLogger LOGGER = Loggers.getLogger(MapSideDataCollectOperation.class);

    private static class VoidFunction<Arg> implements Function<Arg, Void> {
        @Nullable
        @Override
        public Void apply(@Nullable Arg input) {
            return null;
        }
    }

    private final IndicesService indicesService;
    protected final EvaluatingNormalizer nodeNormalizer;
    protected final ClusterService clusterService;
    private final FileCollectInputSymbolVisitor fileInputSymbolVisitor;
    private final CollectServiceResolver collectServiceResolver;
    private final ProjectionToProjectorVisitor projectorVisitor;
    private final ThreadPoolExecutor executor;
    private final ListeningExecutorService listeningExecutorService;
    private final int poolSize;
    private final ResultProviderFactory resultProviderFactory;

    private final InformationSchemaCollectService informationSchemaCollectService;
    private final UnassignedShardsCollectService unassignedShardsCollectService;

    private final OneRowCollectService clusterCollectService;
    private final CollectService nodeCollectService;

    private final Functions functions;
    private final NodeSysExpression nodeSysExpression;
    private ThreadPool threadPool;
    private final BulkRetryCoordinatorPool bulkRetryCoordinatorPool;
    private final TransportActionProvider transportActionProvider;
    private final Settings settings;

    @Inject
    public MapSideDataCollectOperation(ClusterService clusterService,
                                       Settings settings,
                                       TransportActionProvider transportActionProvider,
                                       BulkRetryCoordinatorPool bulkRetryCoordinatorPool,
                                       Functions functions,
                                       ReferenceResolver referenceResolver,
                                       NodeSysExpression nodeSysExpression,
                                       IndicesService indicesService,
                                       ThreadPool threadPool,
                                       CollectServiceResolver collectServiceResolver,
                                       ResultProviderFactory resultProviderFactory,
                                       InformationSchemaCollectService informationSchemaCollectService,
                                       UnassignedShardsCollectService unassignedShardsCollectService) {
        this.resultProviderFactory = resultProviderFactory;
        this.informationSchemaCollectService = informationSchemaCollectService;
        this.unassignedShardsCollectService = unassignedShardsCollectService;
        this.executor = (ThreadPoolExecutor)threadPool.executor(ThreadPool.Names.SEARCH);
        this.poolSize = executor.getCorePoolSize();
        this.listeningExecutorService = MoreExecutors.listeningDecorator(executor);

        this.clusterService = clusterService;
        this.indicesService = indicesService;
        this.nodeNormalizer = new EvaluatingNormalizer(functions, RowGranularity.NODE, referenceResolver);

        this.collectServiceResolver = collectServiceResolver;

        this.settings = settings;
        this.functions = functions;
        this.nodeSysExpression = nodeSysExpression;

        this.clusterCollectService = new OneRowCollectService(new ImplementationSymbolVisitor(
                referenceResolver,
                functions,
                RowGranularity.CLUSTER
        ));
        this.nodeCollectService = new CollectService() {
            @Override
            public CrateCollector getCollector(CollectNode node, RowDownstream downstream) {
                return getNodeLevelCollector(node, downstream);
            }
        };
        this.fileInputSymbolVisitor =
                new FileCollectInputSymbolVisitor(functions, FileLineReferenceResolver.INSTANCE);

        this.threadPool = threadPool;
        this.bulkRetryCoordinatorPool = bulkRetryCoordinatorPool;
        this.transportActionProvider = transportActionProvider;

        ImplementationSymbolVisitor nodeImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver,
                functions,
                RowGranularity.NODE
        );
        this.projectorVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                nodeImplementationSymbolVisitor
        );
    }


    public ResultProvider createDownstream(CollectNode collectNode) {
        return resultProviderFactory.createDownstream(collectNode, collectNode.jobId());
    }

    /**
     * dispatch by the following criteria:
     * <p>
     * * if local node id is contained in routing:<br>
     * * if no shards are given:<br>
     * &nbsp; -&gt; run row granularity level collect<br>
     * &nbsp; except for doc level:
     * &nbsp; &nbsp; if table if partitioned:
     * &nbsp; &nbsp; -&gt; edge case for empty partitioned table
     * &nbsp; &nbsp; else:
     * &nbsp; &nbsp; -&gt; collect from information schema
     * * if shards are given:<br>
     * &nbsp; -&gt; run shard or doc level collect<br>
     * * else if we got cluster RowGranularity:<br>
     * &nbsp; -&gt; run node level collect (cluster level)<br>
     * </p>
     */
    @Override
    public ListenableFuture<List<Void>> collect(CollectNode collectNode,
                                                RowDownstream downstream,
                                                final JobCollectContext jobCollectContext) {
        assert collectNode.isRouted(); // not routed collect is not handled here
        assert collectNode.jobId() != null : "no jobId present for collect operation";
        String localNodeId = clusterService.state().nodes().localNodeId();
        Set<String> routingNodes = collectNode.routing().nodes();
        if (routingNodes.contains(localNodeId) || localNodeId.equals(collectNode.handlerSideCollect())) {
            if (collectNode.routing().containsShards(localNodeId)) {
                // shard or doc level (incl. unassigned shards)
                return handleShardCollect(collectNode, downstream, jobCollectContext);
            } else {
                ListenableFuture<List<Void>> results;

                if (collectNode instanceof FileUriCollectNode) {
                    results = handleWithService(nodeCollectService, collectNode, downstream, jobCollectContext);
                } else if (collectNode.isPartitioned() && collectNode.maxRowGranularity() == RowGranularity.DOC) {
                    // edge case: partitioned table without actual indices
                    // no results
                    downstream.registerUpstream(this).finish();
                    results = IMMEDIATE_LIST;
                } else {
                    CollectService collectService = getCollectService(collectNode, localNodeId);
                    results = handleWithService(collectService, collectNode, downstream, jobCollectContext);
                }

                // close JobCollectContext after non doc collectors are finished
                Futures.addCallback(results, new FutureCallback<List<Void>>() {
                    @Override
                    public void onSuccess(@Nullable List<Void> result) {
                        jobCollectContext.close();
                    }

                    @Override
                    public void onFailure(Throwable t) {
                        jobCollectContext.close();
                    }
                });

                return results;
            }
        }
        throw new UnhandledServerException("unsupported routing");
    }

    private CollectService getCollectService(CollectNode collectNode, String localNodeId) {
        switch (collectNode.maxRowGranularity()) {
            case CLUSTER:
                // sys.cluster
                return clusterCollectService;
            case NODE:
                // sys.nodes collect
                return nodeCollectService;
            case SHARD:
                // unassigned shards
                return unassignedShardsCollectService;
            case DOC:
                if (localNodeId.equals(collectNode.handlerSideCollect())) {
                    // information schema select
                    return informationSchemaCollectService;
                } else {
                    // sys.operations, sys.jobs, sys.*log
                    return nodeCollectService;
                }
            default:
                throw new UnsupportedOperationException("Unsupported rowGranularity " + collectNode.maxRowGranularity());
        }
    }

    private ListenableFuture<List<Void>> handleWithService(final CollectService collectService,
                                                           final CollectNode node,
                                                           final RowDownstream rowDownstream,
                                                           final JobCollectContext jobCollectContext) {
        return listeningExecutorService.submit(new Callable<List<Void>>() {
            @Override
            public List<Void> call() throws Exception {
                try {
                    EvaluatingNormalizer nodeNormalizer = MapSideDataCollectOperation.this.nodeNormalizer;
                    if (node.maxRowGranularity().finerThan(RowGranularity.CLUSTER)) {
                        nodeNormalizer = new EvaluatingNormalizer(functions,
                                RowGranularity.NODE,
                                new NodeSysReferenceResolver(nodeSysExpression));
                    }
                    CollectNode localCollectNode = node.normalize(nodeNormalizer);
                    RowDownstream localRowDownStream = rowDownstream;
                    if (localCollectNode.whereClause().noMatch()) {
                        localRowDownStream.registerUpstream(MapSideDataCollectOperation.this).finish();
                    } else {
                        if (!localCollectNode.projections().isEmpty()) {
                            FlatProjectorChain projectorChain = FlatProjectorChain.withAttachedDownstream(
                                    projectorVisitor,
                                    jobCollectContext.ramAccountingContext(),
                                    localCollectNode.projections(),
                                    localRowDownStream,
                                    node.jobId()
                            );
                            projectorChain.startProjections(jobCollectContext);
                            localRowDownStream = projectorChain.firstProjector();
                        }
                        CrateCollector collector = collectService.getCollector(localCollectNode, localRowDownStream); // calls projector.registerUpstream()
                        collector.doCollect(jobCollectContext);
                    }
                } catch (Throwable t) {
                    LOGGER.error("error during collect", t);
                    rowDownstream.registerUpstream(MapSideDataCollectOperation.this).fail(t);
                    Throwables.propagate(t);
                }
                return ONE_LIST;
            }
        });
    }

    private CrateCollector getNodeLevelCollector(CollectNode collectNode,
                                                 RowDownstream downstream) {
        if (collectNode instanceof FileUriCollectNode) {
            FileCollectInputSymbolVisitor.Context context = fileInputSymbolVisitor.extractImplementations(collectNode);
            FileUriCollectNode fileUriCollectNode = (FileUriCollectNode) collectNode;

            String[] readers = fileUriCollectNode.executionNodes().toArray(
                    new String[fileUriCollectNode.executionNodes().size()]);
            Arrays.sort(readers);
            return new FileReadingCollector(
                    ValueSymbolVisitor.STRING.process(fileUriCollectNode.targetUri()),
                    context.topLevelInputs(),
                    context.expressions(),
                    downstream,
                    fileUriCollectNode.fileFormat(),
                    fileUriCollectNode.compression(),
                    ImmutableMap.<String, FileInputFactory>of(),
                    fileUriCollectNode.sharedStorage(),
                    readers.length,
                    Arrays.binarySearch(readers, clusterService.state().nodes().localNodeId())
            );
        } else {
            CollectService service = collectServiceResolver.getService(collectNode.routing());
            if (service != null) {
                return service.getCollector(collectNode, downstream);
            }
            ImplementationSymbolVisitor nodeImplementationSymbolVisitor = new ImplementationSymbolVisitor(
                    new NodeSysReferenceResolver(nodeSysExpression),
                    functions,
                    RowGranularity.NODE
            );
            ImplementationSymbolVisitor.Context ctx = nodeImplementationSymbolVisitor.extractImplementations(collectNode);
            assert ctx.maxGranularity().ordinal() <= RowGranularity.NODE.ordinal() : "wrong RowGranularity";
            return new SimpleOneRowCollector(
                    ctx.topLevelInputs(), ctx.collectExpressions(), downstream);
        }
    }

    private int numShards(CollectNode collectNode, String localNodeId) {
        int numShards = collectNode.routing().numShards(localNodeId);
        if (localNodeId.equals(collectNode.handlerSideCollect()) && collectNode.routing().nodes().contains(TableInfo.NULL_NODE_ID)) {
            // add 1 for unassigned shards - treated as one shard
            // as it is handled with one collector
            numShards += 1;
        }
        return numShards;
    }

    /**
     * collect data on shard or doc level
     * <p>
     * collects data from each shard in a separate thread,
     * collecting the data into a single state through an {@link java.util.concurrent.ArrayBlockingQueue}.
     * </p>
     *
     * @param collectNode {@link CollectNode} containing routing information and symbols to collect
     */
    protected ListenableFuture<List<Void>> handleShardCollect(CollectNode collectNode,
                                                              RowDownstream downstream,
                                                              JobCollectContext jobCollectContext) {
        String localNodeId = clusterService.state().nodes().localNodeId();

        final int numShards = numShards(collectNode, localNodeId);

        NodeSysReferenceResolver referenceResolver = new NodeSysReferenceResolver(nodeSysExpression);
        EvaluatingNormalizer nodeNormalizer = new EvaluatingNormalizer(functions,
                RowGranularity.NODE,
                referenceResolver);
        CollectNode normalizedCollectNode = collectNode.normalize(nodeNormalizer);

        if (normalizedCollectNode.whereClause().noMatch()) {
            downstream.registerUpstream(this).finish();
            return IMMEDIATE_LIST;
        }

        assert normalizedCollectNode.jobId() != null : "jobId must be set on CollectNode";

        ImplementationSymbolVisitor implementationSymbolVisitor = new ImplementationSymbolVisitor(
                referenceResolver,
                functions,
                RowGranularity.NODE
        );
        ProjectionToProjectorVisitor projectorVisitor = new ProjectionToProjectorVisitor(
                clusterService,
                threadPool,
                settings,
                transportActionProvider,
                bulkRetryCoordinatorPool,
                implementationSymbolVisitor
        );

        ShardProjectorChain projectorChain = new ShardProjectorChain(
                collectNode.jobId(),
                numShards,
                normalizedCollectNode.projections(),
                downstream,
                projectorVisitor,
                jobCollectContext.ramAccountingContext()
        );
        TableUnknownException lastException = null;
        int jobSearchContextId = normalizedCollectNode.routing().jobSearchContextIdBase();
        // get shardCollectors from single shards
        final List<CrateCollector> shardCollectors = new ArrayList<>(numShards);
        for (Map.Entry<String, Map<String, List<Integer>>> nodeEntry : normalizedCollectNode.routing().locations().entrySet()) {
            if (nodeEntry.getKey().equals(localNodeId)) {
                Map<String, List<Integer>> shardIdMap = nodeEntry.getValue();
                for (Map.Entry<String, List<Integer>> entry : shardIdMap.entrySet()) {
                    String indexName = entry.getKey();
                    IndexService indexService;
                    try {
                        indexService = indicesService.indexServiceSafe(indexName);
                    } catch (IndexMissingException e) {
                        lastException = new TableUnknownException(entry.getKey(), e);
                        continue;
                    }

                    for (Integer shardId : entry.getValue()) {
                        Injector shardInjector;
                        try {
                            shardInjector = indexService.shardInjectorSafe(shardId);
                            ShardCollectService shardCollectService = shardInjector.getInstance(ShardCollectService.class);
                            CrateCollector collector = shardCollectService.getCollector(
                                    normalizedCollectNode,
                                    projectorChain,
                                    jobCollectContext,
                                    jobSearchContextId
                            );
                            shardCollectors.add(collector);
                        } catch (IndexShardMissingException e) {
                            throw new UnhandledServerException(
                                    String.format(Locale.ENGLISH, "unknown shard id %d on index '%s'",
                                            shardId, entry.getKey()), e);
                        } catch (CancellationException e) {
                            throw e;
                        } catch (Exception e) {
                            LOGGER.error("Error while getting collector", e);
                            throw new UnhandledServerException(e);
                        }
                        jobSearchContextId++;
                    }
                }
            } else if (TableInfo.NULL_NODE_ID.equals(nodeEntry.getKey()) && localNodeId.equals(collectNode.handlerSideCollect())) {
                // collect unassigned shards
                LOGGER.trace("collecting unassigned shards on node {}", localNodeId);
                EvaluatingNormalizer clusterNormalizer = new EvaluatingNormalizer(functions,
                        RowGranularity.CLUSTER,
                        referenceResolver);
                CollectNode clusterNormalizedCollectNode = collectNode.normalize(clusterNormalizer);

                RowDownstream projectorChainDownstream = projectorChain.newShardDownstreamProjector(projectorVisitor);
                CrateCollector collector = unassignedShardsCollectService.getCollector(
                        clusterNormalizedCollectNode,
                        projectorChainDownstream
                );
                shardCollectors.add(collector);
            } else if (jobSearchContextId > -1) {
                // just increase jobSearchContextId by shard size of foreign node(s) indices
                for (List<Integer> shardIdMap : nodeEntry.getValue().values()) {
                    jobSearchContextId += shardIdMap.size();
                }
            }
        }
        assert shardCollectors.size() == numShards : "invalid number of shardcollectors";

        if (lastException != null
                && jobSearchContextId == collectNode.routing().jobSearchContextIdBase()) {
            // all collectors threw an table unknown exception, re-throw it
            throw lastException;
        }

        // start the projection
        projectorChain.startProjections(jobCollectContext);
        try {
            LOGGER.trace("starting {} shardCollectors...", numShards);
            return runCollectThreaded(collectNode, shardCollectors, jobCollectContext);
        } catch (RejectedExecutionException e) {
            // on distributing collects the merge nodes need to be informed about the failure
            // so they can clean up their context
            // in order to fire the failure we need to add the operation directly as an upstream to get a handle
            downstream.registerUpstream(this).fail(e);
            return Futures.immediateFailedFuture(e);
        }

    }

    private ListenableFuture<List<Void>> runCollectThreaded(CollectNode collectNode,
                                                            final List<CrateCollector> shardCollectors,
                                                            final JobCollectContext jobCollectContext) throws RejectedExecutionException {
        if (collectNode.maxRowGranularity() == RowGranularity.SHARD) {
            // run sequential to prevent sys.shards queries from using too many threads
            // and overflowing the threadpool queues
            return listeningExecutorService.submit(new Callable<List<Void>>() {
                @Override
                public List<Void> call() throws Exception {
                    for (CrateCollector collector : shardCollectors) {
                        collector.doCollect(jobCollectContext);
                    }
                    return ONE_LIST;
                }
            });
        } else {
            return ThreadPools.runWithAvailableThreads(
                    executor,
                    poolSize,
                    collectors2Callables(shardCollectors, jobCollectContext),
                    new VoidFunction<List<Void>>());
        }
    }

    private Collection<Callable<Void>> collectors2Callables(List<CrateCollector> collectors,
                                                            final JobCollectContext jobCollectContext) {
        return Lists.transform(collectors, new Function<CrateCollector, Callable<Void>>() {

            @Override
            public Callable<Void> apply(final CrateCollector collector) {
                return new Callable<Void>() {
                    @Override
                    public Void call() throws Exception {
                        collector.doCollect(jobCollectContext);
                        return null;
                    }
                };
            }
        });
    }

    private static class OneRowCollectService implements CollectService {

        private final ImplementationSymbolVisitor clusterImplementationSymbolVisitor;

        private OneRowCollectService(ImplementationSymbolVisitor clusterImplementationSymbolVisitor) {
            this.clusterImplementationSymbolVisitor = clusterImplementationSymbolVisitor;
        }

        @Override
        public CrateCollector getCollector(CollectNode node, RowDownstream downstream) {
            // resolve Implementations
            ImplementationSymbolVisitor.Context ctx = clusterImplementationSymbolVisitor.extractImplementations(node);
            List<Input<?>> inputs = ctx.topLevelInputs();
            Set<CollectExpression<?>> collectExpressions = ctx.collectExpressions();
            return new SimpleOneRowCollector(inputs, collectExpressions, downstream);
        }
    }
}


File: sql/src/main/java/io/crate/operation/projectors/InternalResultProviderFactory.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.operation.projectors;

import com.google.common.collect.Lists;
import io.crate.Streamer;
import io.crate.executor.transport.distributed.DistributingDownstream;
import io.crate.executor.transport.distributed.SingleBucketBuilder;
import io.crate.executor.transport.distributed.TransportDistributedResultAction;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodes;
import io.crate.planner.node.StreamerVisitor;
import org.elasticsearch.cluster.ClusterService;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Collections;
import java.util.UUID;

@Singleton
public class InternalResultProviderFactory implements ResultProviderFactory {

    private final ClusterService clusterService;
    private final TransportDistributedResultAction transportDistributedResultAction;
    private final StreamerVisitor streamerVisitor;

    @Inject
    public InternalResultProviderFactory(ClusterService clusterService,
                                         TransportDistributedResultAction transportDistributedResultAction,
                                         StreamerVisitor streamerVisitor) {
        this.clusterService = clusterService;
        this.transportDistributedResultAction = transportDistributedResultAction;
        this.streamerVisitor = streamerVisitor;
    }

    public ResultProvider createDownstream(ExecutionNode node, UUID jobId) {
        Streamer<?>[] streamers = getStreamers(node);

        if (ExecutionNodes.hasDirectResponseDownstream(node.downstreamNodes())) {
            return new SingleBucketBuilder(streamers);
        } else {
            assert node.downstreamNodes().size() > 0 : "must have at least one downstream";

            // TODO: set bucketIdx properly
            ArrayList<String> server = Lists.newArrayList(node.executionNodes());
            Collections.sort(server);
            int bucketIdx = server.indexOf(clusterService.localNode().id());

            return new DistributingDownstream(
                    jobId,
                    node.downstreamExecutionNodeId(),
                    bucketIdx,
                    node.downstreamNodes(),
                    transportDistributedResultAction,
                    streamers
            );
        }
    }

    protected Streamer<?>[] getStreamers(ExecutionNode node) {
        return streamerVisitor.processExecutionNode(node).outputStreamers();
    }
}


File: sql/src/main/java/io/crate/planner/PlanNodeBuilder.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner;

import com.google.common.base.MoreObjects;
import com.google.common.base.Predicates;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Iterables;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.PartitionName;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.consumer.OrderByPositionVisitor;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.Symbols;

import javax.annotation.Nullable;
import java.util.List;
import java.util.Map;
import java.util.TreeMap;

public class PlanNodeBuilder {

    public static CollectNode distributingCollect(TableInfo tableInfo,
                                                  Planner.Context plannerContext,
                                                  WhereClause whereClause,
                                                  List<Symbol> toCollect,
                                                  List<String> downstreamNodes,
                                                  ImmutableList<Projection> projections) {
        Routing routing = tableInfo.getRouting(whereClause, null);
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "distributing collect",
                routing);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.downstreamNodes(downstreamNodes);
        node.toCollect(toCollect);
        node.projections(projections);

        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        return node;
    }

    public static MergeNode distributedMerge(CollectNode collectNode,
                                             Planner.Context plannerContext,
                                             List<Projection> projections) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "distributed merge",
                collectNode.executionNodes().size());
        node.projections(projections);

        assert collectNode.hasDistributingDownstreams();
        node.executionNodes(ImmutableSet.copyOf(collectNode.downstreamNodes()));
        connectTypes(collectNode, node);
        return node;
    }

    public static MergeNode localMerge(List<Projection> projections,
                                       DQLPlanNode previousNode,
                                       Planner.Context plannerContext) {
        MergeNode node = new MergeNode(
                plannerContext.nextExecutionNodeId(),
                "localMerge",
                previousNode.executionNodes().size());
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }

    /**
     * Create a MergeNode which uses a {@link io.crate.operation.merge.SortingBucketMerger}
     * as it expects sorted input and produces sorted output.
     *
     * @param projections the projections to include in the resulting MergeNode
     * @param orderBy {@linkplain io.crate.analyze.OrderBy} containing sorting parameters
     * @param sourceSymbols the input symbols for this mergeNode
     * @param orderBySymbols the symbols to sort on. If this is null,
     *                       {@linkplain io.crate.analyze.OrderBy#orderBySymbols()}
     *                       will be used
     * @param previousNode the previous planNode to derive inputtypes from
     */
    public static MergeNode sortedLocalMerge(List<Projection> projections,
                                             OrderBy orderBy,
                                             List<Symbol> sourceSymbols,
                                             @Nullable List<Symbol> orderBySymbols,
                                             DQLPlanNode previousNode,
                                             Planner.Context plannerContext) {
        int[] orderByIndices = OrderByPositionVisitor.orderByPositions(
                MoreObjects.firstNonNull(orderBySymbols, orderBy.orderBySymbols()),
                sourceSymbols
        );
        MergeNode node = MergeNode.sortedMergeNode(
                plannerContext.nextExecutionNodeId(),
                "sortedLocalMerge",
                previousNode.executionNodes().size(),
                orderByIndices,
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        node.projections(projections);
        connectTypes(previousNode, node);
        return node;
    }

    /**
     * calculates the outputTypes using the projections and input types.
     * must be called after projections have been set.
     */
    public static void setOutputTypes(CollectNode node) {
        if (node.projections().isEmpty()) {
            node.outputTypes(Symbols.extractTypes(node.toCollect()));
        } else {
            node.outputTypes(Planner.extractDataTypes(node.projections(), Symbols.extractTypes(node.toCollect())));
        }
    }

    /**
     * sets the inputTypes from the previousNode's outputTypes
     * and calculates the outputTypes using the projections and input types.
     * <p>
     * must be called after projections have been set
     * </p>
     */
    public static void connectTypes(DQLPlanNode previousNode, DQLPlanNode nextNode) {
        nextNode.inputTypes(previousNode.outputTypes());
        nextNode.outputTypes(Planner.extractDataTypes(nextNode.projections(), nextNode.inputTypes()));
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        assert !Iterables.any(toCollect, Predicates.instanceOf(InputColumn.class)) : "cannot collect inputcolumns";
        Routing routing = tableInfo.getRouting(whereClause, routingPreference);
        if (partitionIdent != null && routing.hasLocations()) {
            routing = filterRouting(routing, PartitionName.fromPartitionIdent(
                    tableInfo.ident().schema(), tableInfo.ident().name(), partitionIdent).stringValue());
        }
        plannerContext.allocateJobSearchContextIds(routing);
        CollectNode node = new CollectNode(
                plannerContext.nextExecutionNodeId(),
                "collect",
                routing,
                toCollect,
                projections);
        node.whereClause(whereClause);
        node.maxRowGranularity(tableInfo.rowGranularity());
        node.isPartitioned(tableInfo.isPartitioned());
        setOutputTypes(node);
        node.orderBy(orderBy);
        node.limit(limit);
        return node;
    }

    private static Routing filterRouting(Routing routing, String includeTableName) {
        assert routing.hasLocations();
        assert includeTableName != null;
        Map<String, Map<String, List<Integer>>> newLocations = new TreeMap<>();

        for (Map.Entry<String, Map<String, List<Integer>>> entry : routing.locations().entrySet()) {
            Map<String, List<Integer>> tableMap = new TreeMap<>();
            for (Map.Entry<String, List<Integer>> tableEntry : entry.getValue().entrySet()) {
                if (includeTableName.equals(tableEntry.getKey())) {
                    tableMap.put(tableEntry.getKey(), tableEntry.getValue());
                }
            }
            if (tableMap.size()>0){
                newLocations.put(entry.getKey(), tableMap);
            }

        }
        if (newLocations.size()>0) {
            return new Routing(newLocations);
        } else {
            return new Routing();
        }

    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent,
                                      @Nullable String routingPreference) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, routingPreference, null, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable String partitionIdent) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, partitionIdent, null);
    }

    public static CollectNode collect(TableInfo tableInfo,
                                      Planner.Context plannerContext,
                                      WhereClause whereClause,
                                      List<Symbol> toCollect,
                                      ImmutableList<Projection> projections,
                                      @Nullable OrderBy orderBy,
                                      @Nullable Integer limit) {
        return collect(tableInfo, plannerContext, whereClause, toCollect, projections, null, null, orderBy, limit);
    }
}


File: sql/src/main/java/io/crate/planner/PlanVisitor.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner;

import io.crate.planner.node.dml.InsertFromSubQuery;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.join.NestedLoop;
import io.crate.planner.node.management.KillPlan;
import org.elasticsearch.common.Nullable;

public class PlanVisitor<C, R> {

    public R process(Plan plan, @Nullable C context) {
        return plan.accept(this, context);
    }

    protected R visitPlan(Plan plan, C context) {
        return null;
    }

    public R visitIterablePlan(IterablePlan plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitNoopPlan(NoopPlan plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitGlobalAggregate(GlobalAggregate plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitQueryAndFetch(QueryAndFetch node, C context){
        return visitPlan(node, context);
    }

    public R visitQueryThenFetch(QueryThenFetch node, C context){
        return visitPlan(node, context);
    }

    public R visitNonDistributedGroupBy(NonDistributedGroupBy node, C context) {
        return visitPlan(node, context);
    }

    public R visitUpsert(Upsert node, C context) {
        return visitPlan(node, context);
    }

    public R visitDistributedGroupBy(DistributedGroupBy node, C context) {
        return visitPlan(node, context);
    }

    public R visitInsertByQuery(InsertFromSubQuery node, C context) {
        return visitPlan(node, context);
    }

    public R visitCollectAndMerge(CollectAndMerge plan, C context) {
        return visitPlan(plan, context);
    }

    public R visitCountPlan(CountPlan countPlan, C context) {
        return visitPlan(countPlan, context);
    }

    public R visitKillPlan(KillPlan killPlan, C context) {
        return visitPlan(killPlan, context);
    }

    public R visitNestedLoop(NestedLoop nestedLoop, C context) {
        return visitPlan(nestedLoop, context);
    }
}


File: sql/src/main/java/io/crate/planner/node/ExecutionNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node;

import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.FileUriCollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;
import org.elasticsearch.common.io.stream.Streamable;

import java.util.List;
import java.util.Set;
import java.util.UUID;

public interface ExecutionNode extends Streamable {

    String DIRECT_RETURN_DOWNSTREAM_NODE = "_response";

    int NO_EXECUTION_NODE = Integer.MAX_VALUE;

    interface ExecutionNodeFactory<T extends ExecutionNode> {
        T create();
    }

    enum Type {
        COLLECT(CollectNode.FACTORY),
        COUNT(CountNode.FACTORY),
        FILE_URI_COLLECT(FileUriCollectNode.FACTORY),
        MERGE(MergeNode.FACTORY),
        NESTED_LOOP(NestedLoopNode.FACTORY);

        private final ExecutionNodeFactory factory;

        Type(ExecutionNodeFactory factory) {
            this.factory = factory;
        }

        public ExecutionNodeFactory factory() {
            return factory;
        }
    }

    Type type();

    String name();

    int executionNodeId();

    Set<String> executionNodes();

    List<String> downstreamNodes();

    int downstreamExecutionNodeId();

    UUID jobId();

    void jobId(UUID jobId);


    <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context);
}


File: sql/src/main/java/io/crate/planner/node/ExecutionNodeVisitor.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node;

import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.CountNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.node.dql.join.NestedLoopNode;

public class ExecutionNodeVisitor<C, R> {

    public R process(ExecutionNode node, C context) {
        return node.accept(this, context);
    }

    protected R visitExecutionNode(ExecutionNode node, C context) {
        return null;
    }

    public R visitCollectNode(CollectNode node, C context) {
        return visitExecutionNode(node, context);
    }

    public R visitMergeNode(MergeNode node, C context) {
        return visitExecutionNode(node, context);
    }

    public R visitCountNode(CountNode countNode, C context) {
        return visitExecutionNode(countNode, context);
    }

    public R visitNestedLoopNode(NestedLoopNode nestedLoopNode, C context) {
        return visitExecutionNode(nestedLoopNode, context);
    }
}


File: sql/src/main/java/io/crate/planner/node/StreamerVisitor.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import io.crate.Streamer;
import io.crate.exceptions.ResourceUnknownException;
import io.crate.metadata.Functions;
import io.crate.operation.aggregation.AggregationFunction;
import io.crate.planner.node.dql.CollectNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.AggregationProjection;
import io.crate.planner.projection.GroupProjection;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.ProjectionType;
import io.crate.planner.symbol.Aggregation;
import io.crate.planner.symbol.InputColumn;
import io.crate.planner.symbol.Symbol;
import io.crate.planner.symbol.SymbolType;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import io.crate.types.UndefinedType;
import org.elasticsearch.common.inject.Inject;
import org.elasticsearch.common.inject.Singleton;

import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

/**
 * get input and output {@link io.crate.Streamer}s for {@link io.crate.planner.node.PlanNode}s
 */
@Singleton
public class StreamerVisitor {

    public static class Context {
        private List<Streamer<?>> inputStreamers = new ArrayList<>();
        private List<Streamer<?>> outputStreamers = new ArrayList<>();

        public Context() {
        }

        public Streamer<?>[] inputStreamers() {
            return inputStreamers.toArray(new Streamer<?>[inputStreamers.size()]);
        }

        public Streamer<?>[] outputStreamers() {
            return outputStreamers.toArray(new Streamer<?>[outputStreamers.size()]);
        }
    }

    private final Functions functions;
    private final PlanNodeStreamerVisitor planNodeStreamerVisitor;
    private final ExecutionNodeStreamerVisitor executionNodeStreamerVisitor;

    @Inject
    public StreamerVisitor(Functions functions) {
        this.functions = functions;
        this.planNodeStreamerVisitor = new PlanNodeStreamerVisitor();
        this.executionNodeStreamerVisitor = new ExecutionNodeStreamerVisitor();
    }

    public Context processPlanNode(PlanNode node) {
        Context context = new Context();
        planNodeStreamerVisitor.process(node, context);
        return context;
    }

    public Context processExecutionNode(ExecutionNode executionNode) {
        Context context = new Context();
        executionNodeStreamerVisitor.process(executionNode, context);
        return context;
    }

    private class PlanNodeStreamerVisitor extends PlanNodeVisitor<Context, Void> {

        @Override
        public Void visitCollectNode(CollectNode node, Context context) {
            extractFromCollectNode(node, context);
            return null;
        }

        @Override
        public Void visitMergeNode(MergeNode node, Context context) {
            extractFromMergeNode(node, context);
            return null;
        }
    }

    private class ExecutionNodeStreamerVisitor extends ExecutionNodeVisitor<Context, Void> {

        @Override
        public Void visitMergeNode(MergeNode node, Context context) {
            extractFromMergeNode(node, context);
            return null;
        }

        @Override
        public Void visitCollectNode(CollectNode collectNode, Context context) {
            extractFromCollectNode(collectNode, context);
            return null;
        }

        @Override
        protected Void visitExecutionNode(ExecutionNode node, Context context) {
            throw new UnsupportedOperationException(String.format("Got unsupported ExecutionNode %s", node.getClass().getName()));
        }
    }


    private Streamer<?> resolveStreamer(Aggregation aggregation, Aggregation.Step step) {
        Streamer<?> streamer;
        AggregationFunction<?, ?> aggFunction = (AggregationFunction<?, ?>)functions.get(aggregation.functionIdent());
        if (aggFunction == null) {
            throw new ResourceUnknownException("unknown aggregation function");
        }
        switch (step) {
            case ITER:
                assert aggFunction.info().ident().argumentTypes().size() == 1;
                streamer = aggFunction.info().ident().argumentTypes().get(0).streamer();
                break;
            case PARTIAL:
                streamer = aggFunction.partialType().streamer();
                break;
            case FINAL:
                streamer = aggFunction.info().returnType().streamer();
                break;
            default:
                throw new UnsupportedOperationException("step not supported");
        }
        return streamer;
    }

    private void extractFromCollectNode(CollectNode node, Context context) {
        // get aggregations, if any
        List<Aggregation> aggregations = ImmutableList.of();
        List<Projection> projections = Lists.reverse(node.projections());
        for(Projection projection : projections){
            if (projection.projectionType() == ProjectionType.AGGREGATION) {
                aggregations = ((AggregationProjection)projection).aggregations();
                break;
            } else if (projection.projectionType() == ProjectionType.GROUP) {
                aggregations = ((GroupProjection)projection).values();
                break;
            }
        }


        int aggIdx = 0;
        Aggregation aggregation;
        for (DataType outputType : node.outputTypes()) {
            if (outputType == null || outputType == UndefinedType.INSTANCE) {
                // get streamer for aggregation result
                try {
                    aggregation = aggregations.get(aggIdx);
                    if (aggregation != null) {
                        context.outputStreamers.add(resolveStreamer(aggregation, aggregation.toStep()));
                    }
                } catch (IndexOutOfBoundsException e) {
                    // assume this is an unknown column
                    context.outputStreamers.add(UndefinedType.INSTANCE.streamer());
                }
                aggIdx++;
            } else {
                // get streamer for outputType
                context.outputStreamers.add(outputType.streamer());
            }
        }
    }

    private void extractFromMergeNode(MergeNode node, Context context) {
        if (node.projections().isEmpty()) {
            for (DataType dataType : node.inputTypes()) {
                if (dataType != null) {
                    context.inputStreamers.add(dataType.streamer());
                } else {
                    throw new IllegalStateException("Can't resolve Streamer from null dataType");
                }
            }
            return;
        }

        Projection firstProjection = node.projections().get(0);
        setInputStreamers(node.inputTypes(), firstProjection, context);
        setOutputStreamers(node.outputTypes(), node.inputTypes(), node.projections(), context);
    }

    private void setOutputStreamers(List<DataType> outputTypes,
                                    List<DataType> inputTypes,
                                    List<Projection> projections, Context context) {
        final Streamer<?>[] streamers = new Streamer[outputTypes.size()];

        int idx = 0;
        for (DataType outputType : outputTypes) {
            if (outputType == UndefinedType.INSTANCE) {
                resolveStreamer(streamers, projections, idx, projections.size() - 1, inputTypes);
                if (streamers[idx] == null) {
                    streamers[idx] = outputType.streamer();
                }
            } else {
                streamers[idx] = outputType.streamer();
            }
            idx++;
        }

        for (Streamer<?> streamer : streamers) {
            if (streamer == null) {
                throw new IllegalStateException("Could not resolve all output streamers");
            }
        }
        Collections.addAll(context.outputStreamers, streamers);
    }

    /**
     * traverse the projections backward until a output type/streamer is found for each symbol in the last projection
     */
    private void resolveStreamer(Streamer<?>[] streamers,
                                 List<Projection> projections,
                                 int columnIdx,
                                 int projectionIdx,
                                 List<DataType> inputTypes) {
        final Projection projection = projections.get(projectionIdx);
        final Symbol symbol = projection.outputs().get(columnIdx);

        if (!symbol.valueType().equals(DataTypes.UNDEFINED)) {
            streamers[columnIdx] = symbol.valueType().streamer();
        } else if (symbol.symbolType() == SymbolType.AGGREGATION) {
            Aggregation aggregation = (Aggregation)symbol;
            streamers[columnIdx] = resolveStreamer(aggregation, aggregation.toStep());
        } else if (symbol.symbolType() == SymbolType.INPUT_COLUMN) {
            columnIdx = ((InputColumn)symbol).index();
            if (projectionIdx > 0) {
                projectionIdx--;
                resolveStreamer(streamers, projections, columnIdx, projectionIdx, inputTypes);
            } else {
                streamers[columnIdx] = inputTypes.get(((InputColumn)symbol).index()).streamer();
            }
        }
    }

    private void setInputStreamers(List<DataType> inputTypes, Projection projection, Context context) {
        List<Aggregation> aggregations;
        switch (projection.projectionType()) {
            case TOPN:
            case FETCH:
                aggregations = ImmutableList.of();
                break;
            case GROUP:
                aggregations = ((GroupProjection)projection).values();
                break;
            case AGGREGATION:
                aggregations = ((AggregationProjection)projection).aggregations();
                break;
            default:
                throw new UnsupportedOperationException("projectionType not supported");
        }

        int idx = 0;
        for (DataType inputType : inputTypes) {
            if (inputType != null && inputType != UndefinedType.INSTANCE) {
                context.inputStreamers.add(inputType.streamer());
            } else {
                Aggregation aggregation = aggregations.get(idx);
                context.inputStreamers.add(resolveStreamer(aggregation, aggregation.fromStep()));
                idx++;
            }
        }
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/AbstractDQLPlanNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Optional;
import com.google.common.collect.ImmutableList;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.projection.Projection;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;
import org.elasticsearch.common.io.stream.Streamable;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;

public abstract class AbstractDQLPlanNode implements DQLPlanNode, Streamable, ExecutionNode {

    private UUID jobId;
    private int executionNodeId;
    private String name;
    protected List<Projection> projections = ImmutableList.of();
    protected List<DataType> outputTypes = ImmutableList.of();
    private List<DataType> inputTypes;

    public AbstractDQLPlanNode() {

    }

    protected AbstractDQLPlanNode(int executionNodeId, String name) {
        this.executionNodeId = executionNodeId;
        this.name = name;
    }

    @Override
    public String name() {
        return name;
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public boolean hasProjections() {
        return projections != null && projections.size() > 0;
    }

    @Override
    public List<Projection> projections() {
        return projections;
    }

    public void projections(List<Projection> projections) {
        this.projections = projections;
    }

    @Override
    public void addProjection(Projection projection) {
        List<Projection> projections = new ArrayList<>(this.projections);
        projections.add(projection);
        this.projections = ImmutableList.copyOf(projections);
    }

    public Optional<Projection> finalProjection() {
        if (projections.size() == 0) {
            return Optional.absent();
        } else {
            return Optional.of(projections.get(projections.size()-1));
        }
    }


    public void outputTypes(List<DataType> outputTypes) {
        this.outputTypes = outputTypes;
    }

    public List<DataType> outputTypes() {
        return outputTypes;
    }

    @Override
    public void inputTypes(List<DataType> dataTypes) {
        this.inputTypes = dataTypes;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        name = in.readString();
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            outputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                outputTypes.add(DataTypes.fromStream(in));
            }
        }

        int numProjections = in.readVInt();
        if (numProjections > 0) {
            projections = new ArrayList<>(numProjections);
            for (int i = 0; i < numProjections; i++) {
                projections.add(Projection.fromStream(in));
            }
        }

    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        out.writeString(name);
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);

        int numCols = outputTypes.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            DataTypes.toStream(outputTypes.get(i), out);
        }

        if (hasProjections()) {
            out.writeVInt(projections.size());
            for (Projection p : projections) {
                Projection.toStream(p, out);
            }
        } else {
            out.writeVInt(0);
        }
    }

    @Override
    public boolean equals(Object o) {
        if (this == o) return true;
        if (o == null || getClass() != o.getClass()) return false;

        AbstractDQLPlanNode node = (AbstractDQLPlanNode) o;

        return !(name != null ? !name.equals(node.name) : node.name != null);

    }

    @Override
    public int hashCode() {
        return name != null ? name.hashCode() : 0;
    }

    @Override
    public String toString() {
        return MoreObjects.toStringHelper(this)
                .add("name", name)
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .toString();
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/CollectNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import com.google.common.collect.Sets;
import io.crate.analyze.EvaluatingNormalizer;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.projection.Projection;
import io.crate.planner.symbol.Symbol;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Set;

/**
 * A plan node which collects data.
 */
public class CollectNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<CollectNode> FACTORY = new ExecutionNodeFactory<CollectNode>() {
        @Override
        public CollectNode create() {
            return new CollectNode();
        }
    };
    private Routing routing;
    private List<Symbol> toCollect;
    private WhereClause whereClause = WhereClause.MATCH_ALL;
    private RowGranularity maxRowGranularity = RowGranularity.CLUSTER;

    @Nullable
    private List<String> downstreamNodes;

    private int downstreamExecutionNodeId = ExecutionNode.NO_EXECUTION_NODE;

    private boolean isPartitioned = false;
    private boolean keepContextForFetcher = false;
    private @Nullable String handlerSideCollect = null;

    private @Nullable Integer limit = null;
    private @Nullable OrderBy orderBy = null;

    protected CollectNode() {
        super();
    }

    public CollectNode(int executionNodeId, String name) {
        super(executionNodeId, name);
    }

    public CollectNode(int executionNodeId, String name, Routing routing) {
        this(executionNodeId, name, routing, ImmutableList.<Symbol>of(), ImmutableList.<Projection>of());
    }

    public CollectNode(int executionNodeId, String name, Routing routing, List<Symbol> toCollect, List<Projection> projections) {
        super(executionNodeId, name);
        this.routing = routing;
        this.toCollect = toCollect;
        this.projections = projections;
        this.downstreamNodes = ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public Type type() {
        return Type.COLLECT;
    }

    /**
     * @return a set of node ids where this collect operation is executed,
     *         excluding the NULL_NODE_ID for special collect purposes
     */
    @Override
    public Set<String> executionNodes() {
        if (routing != null) {
            if (routing.isNullRouting()) {
                return routing.nodes();
            } else {
                return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
            }
        } else {
            return ImmutableSet.of();
        }
    }

    public @Nullable Integer limit() {
        return limit;
    }

    public void limit(Integer limit) {
        this.limit = limit;
    }

    public @Nullable OrderBy orderBy() {
        return orderBy;
    }

    public void orderBy(@Nullable OrderBy orderBy) {
        this.orderBy = orderBy;
    }

    @Nullable
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    /**
     * This method returns true if downstreams other than
     * {@link ExecutionNode#DIRECT_RETURN_DOWNSTREAM_NODE} are defined, which means that results
     * of this collect operation should be sent to other nodes instead of being returned directly.
     */
    public boolean hasDistributingDownstreams() {
        if (downstreamNodes != null && downstreamNodes.size() > 0) {
            if (downstreamNodes.size() == 1
                    && downstreamNodes.get(0).equals(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE)) {
                return false;
            }
            return true;
        }
        return false;
    }

    public void downstreamNodes(List<String> downStreamNodes) {
        this.downstreamNodes = downStreamNodes;
    }

    public void downstreamExecutionNodeId(int executionNodeId) {
        this.downstreamExecutionNodeId = executionNodeId;
    }

    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    public void whereClause(WhereClause whereClause) {
        assert whereClause != null;
        this.whereClause = whereClause;
    }

    public Routing routing() {
        return routing;
    }

    public List<Symbol> toCollect() {
        return toCollect;
    }

    public void toCollect(List<Symbol> toCollect) {
        assert toCollect != null;
        this.toCollect = toCollect;
    }

    public boolean isRouted() {
        return routing != null && routing.hasLocations();
    }

    /**
     * Whether collect operates on a partitioned table.
     * Only used on local collect, so no serialization is needed.
     *
     * @return true if collect operates on a partitioned table, false otherwise
     */
    public boolean isPartitioned() {
        return isPartitioned;
    }

    public void isPartitioned(boolean isPartitioned) {
        this.isPartitioned = isPartitioned;
    }

    public RowGranularity maxRowGranularity() {
        return maxRowGranularity;
    }

    public void maxRowGranularity(RowGranularity newRowGranularity) {
        if (maxRowGranularity.compareTo(newRowGranularity) < 0) {
            maxRowGranularity = newRowGranularity;
        }
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCollectNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            toCollect = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                toCollect.add(Symbol.fromStream(in));
            }
        } else {
            toCollect = ImmutableList.of();
        }

        maxRowGranularity = RowGranularity.fromStream(in);

        if (in.readBoolean()) {
            routing = new Routing();
            routing.readFrom(in);
        }

        whereClause = new WhereClause(in);

        int numDownStreams = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownStreams);
        for (int i = 0; i < numDownStreams; i++) {
            downstreamNodes.add(in.readString());
        }
        keepContextForFetcher = in.readBoolean();

        if( in.readBoolean()) {
            limit = in.readVInt();
        }

        if (in.readBoolean()) {
            orderBy = OrderBy.fromStream(in);
        }
        isPartitioned = in.readBoolean();
        handlerSideCollect = in.readOptionalString();
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        int numCols = toCollect.size();
        out.writeVInt(numCols);
        for (int i = 0; i < numCols; i++) {
            Symbol.toStream(toCollect.get(i), out);
        }

        RowGranularity.toStream(maxRowGranularity, out);

        if (routing != null) {
            out.writeBoolean(true);
            routing.writeTo(out);
        } else {
            out.writeBoolean(false);
        }
        whereClause.writeTo(out);

        if (downstreamNodes != null) {
            out.writeVInt(downstreamNodes.size());
            for (String downstreamNode : downstreamNodes) {
                out.writeString(downstreamNode);
            }
        } else {
            out.writeVInt(0);
        }
        out.writeBoolean(keepContextForFetcher);
        if (limit != null ) {
            out.writeBoolean(true);
            out.writeVInt(limit);
        } else {
            out.writeBoolean(false);
        }
        if (orderBy != null) {
            out.writeBoolean(true);
            OrderBy.toStream(orderBy, out);
        } else {
            out.writeBoolean(false);
        }
        out.writeBoolean(isPartitioned);
        out.writeOptionalString(handlerSideCollect);
    }

    /**
     * normalizes the symbols of this node with the given normalizer
     *
     * @return a normalized node, if no changes occurred returns this
     */
    public CollectNode normalize(EvaluatingNormalizer normalizer) {
        assert whereClause() != null;
        CollectNode result = this;
        List<Symbol> newToCollect = normalizer.normalize(toCollect());
        boolean changed = newToCollect != toCollect();
        WhereClause newWhereClause = whereClause().normalize(normalizer);
        if (newWhereClause != whereClause()) {
            changed = changed || newWhereClause != whereClause();
        }
        if (changed) {
            result = new CollectNode(executionNodeId(), name(), routing, newToCollect, projections);
            result.downstreamNodes = downstreamNodes;
            result.maxRowGranularity = maxRowGranularity;
            result.jobId(jobId());
            result.keepContextForFetcher = keepContextForFetcher;
            result.handlerSideCollect = handlerSideCollect;
            result.isPartitioned(isPartitioned);
            result.whereClause(newWhereClause);
        }
        return result;
    }

    public void keepContextForFetcher(boolean keepContextForFetcher) {
        this.keepContextForFetcher = keepContextForFetcher;
    }

    public boolean keepContextForFetcher() {
        return keepContextForFetcher;
    }

    public void handlerSideCollect(String handlerSideCollect) {
        this.handlerSideCollect = handlerSideCollect;
    }

    @Nullable
    public String handlerSideCollect() {
        return handlerSideCollect;
    }
}

File: sql/src/main/java/io/crate/planner/node/dql/CountNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.analyze.WhereClause;
import io.crate.metadata.Routing;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.node.ExecutionNode;
import io.crate.planner.node.ExecutionNodeVisitor;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import java.io.IOException;
import java.util.List;
import java.util.Set;
import java.util.UUID;

public class CountNode implements ExecutionNode {

    public static final ExecutionNodeFactory<CountNode> FACTORY = new ExecutionNodeFactory<CountNode>() {
        @Override
        public CountNode create() {
            return new CountNode();
        }
    };
    private UUID jobId;
    private int executionNodeId;
    private Routing routing;
    private WhereClause whereClause;

    CountNode() {}

    public CountNode(int executionNodeId, Routing routing, WhereClause whereClause) {
        this.executionNodeId = executionNodeId;
        this.routing = routing;
        this.whereClause = whereClause;
    }

    @Override
    public Type type() {
        return Type.COUNT;
    }

    @Override
    public String name() {
        return "count";
    }

    @Override
    public UUID jobId() {
        return jobId;
    }

    @Override
    public void jobId(UUID jobId) {
        this.jobId = jobId;
    }

    public Routing routing() {
        return routing;
    }

    public WhereClause whereClause() {
        return whereClause;
    }

    @Override
    public int executionNodeId() {
        return executionNodeId;
    }

    @Override
    public Set<String> executionNodes() {
        if (routing.isNullRouting()) {
            return routing.nodes();
        } else {
            return Sets.filter(routing.nodes(), TableInfo.IS_NOT_NULL_NODE_ID);
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return ImmutableList.of(ExecutionNode.DIRECT_RETURN_DOWNSTREAM_NODE);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return ExecutionNode.NO_EXECUTION_NODE;
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitCountNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        jobId = new UUID(in.readLong(), in.readLong());
        executionNodeId = in.readVInt();
        routing = new Routing();
        routing.readFrom(in);
        whereClause = new WhereClause(in);
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        assert jobId != null : "jobId must not be null";
        out.writeLong(jobId.getMostSignificantBits());
        out.writeLong(jobId.getLeastSignificantBits());
        out.writeVInt(executionNodeId);
        routing.writeTo(out);
        whereClause.writeTo(out);
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/CountPlan.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

public class CountPlan extends PlanAndPlannedAnalyzedRelation{

    private final CountNode countNode;
    private final MergeNode mergeNode;

    public CountPlan(CountNode countNode, MergeNode mergeNode) {
        this.countNode = countNode;
        this.mergeNode = mergeNode;
    }

    public CountNode countNode() {
        return countNode;
    }

    public MergeNode mergeNode() {
        return mergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitCountPlan(this, context);
    }

    @Override
    public void addProjection(Projection projection) {
        mergeNode.addProjection(projection);
    }

    @Override
    public boolean resultIsDistributed() {
        return false;
    }

    @Override
    public DQLPlanNode resultNode() {
        return mergeNode;
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/MergeNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.base.MoreObjects;
import com.google.common.base.Preconditions;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import javax.annotation.Nullable;
import java.io.IOException;
import java.util.*;

/**
 * A plan node which merges results from upstreams
 */
public class MergeNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<MergeNode> FACTORY = new ExecutionNodeFactory<MergeNode>() {
        @Override
        public MergeNode create() {
            return new MergeNode();
        }
    };

    private List<DataType> inputTypes;
    private int numUpstreams;
    private Set<String> executionNodes;

    /**
     * expects sorted input and produces sorted output
     */
    private boolean sortedInputOutput = false;
    private int[] orderByIndices;
    private boolean[] reverseFlags;
    private Boolean[] nullsFirst;
    private int downstreamExecutionNodeId = NO_EXECUTION_NODE;
    private List<String> downstreamNodes = ImmutableList.of();

    public MergeNode() {
        numUpstreams = 0;
    }

    public MergeNode(int executionNodeId, String name, int numUpstreams) {
        super(executionNodeId, name);
        this.numUpstreams = numUpstreams;
    }

    public static MergeNode sortedMergeNode(int executionNodeId,
                                            String name,
                                            int numUpstreams,
                                            int[] orderByIndices,
                                            boolean[] reverseFlags,
                                            Boolean[] nullsFirst) {
        Preconditions.checkArgument(
                orderByIndices.length == reverseFlags.length && reverseFlags.length == nullsFirst.length,
                "ordering parameters must be of the same length");
        MergeNode mergeNode = new MergeNode(executionNodeId, name, numUpstreams);
        mergeNode.sortedInputOutput = true;
        mergeNode.orderByIndices = orderByIndices;
        mergeNode.reverseFlags = reverseFlags;
        mergeNode.nullsFirst = nullsFirst;
        return mergeNode;
    }

    @Override
    public Type type() {
        return Type.MERGE;
    }

    @Override
    public Set<String> executionNodes() {
        if (executionNodes == null) {
            return ImmutableSet.of();
        } else {
            return executionNodes;
        }
    }

    @Override
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    public void downstreamNodes(Set<String> nodes) {
        downstreamNodes = ImmutableList.copyOf(nodes);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public void executionNodes(Set<String> executionNodes) {
        this.executionNodes = executionNodes;
    }

    public int numUpstreams() {
        return numUpstreams;
    }

    @Override
    public List<DataType> inputTypes() {
        return inputTypes;
    }

    @Override
    public void inputTypes(List<DataType> inputTypes) {
        this.inputTypes = inputTypes;
    }

    public boolean sortedInputOutput() {
        return sortedInputOutput;
    }

    @Nullable
    public int[] orderByIndices() {
        return orderByIndices;
    }

    @Nullable
    public boolean[] reverseFlags() {
        return reverseFlags;
    }

    @Nullable
    public Boolean[] nullsFirst() {
        return nullsFirst;
    }

    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitMergeNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        downstreamExecutionNodeId = in.readVInt();

        int numDownstreamNodes = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownstreamNodes);
        for (int i = 0; i < numDownstreamNodes; i++) {
            downstreamNodes.add(in.readString());
        }

        numUpstreams = in.readVInt();

        int numCols = in.readVInt();
        if (numCols > 0) {
            inputTypes = new ArrayList<>(numCols);
            for (int i = 0; i < numCols; i++) {
                inputTypes.add(DataTypes.fromStream(in));
            }
        }
        int numExecutionNodes = in.readVInt();

        if (numExecutionNodes > 0) {
            executionNodes = new HashSet<>(numExecutionNodes);
            for (int i = 0; i < numExecutionNodes; i++) {
                executionNodes.add(in.readString());
            }
        }

        sortedInputOutput = in.readBoolean();
        if (sortedInputOutput) {
            int orderByIndicesLength = in.readVInt();
            orderByIndices = new int[orderByIndicesLength];
            reverseFlags = new boolean[orderByIndicesLength];
            nullsFirst = new Boolean[orderByIndicesLength];
            for (int i = 0; i < orderByIndicesLength; i++) {
                orderByIndices[i] = in.readVInt();
                reverseFlags[i] = in.readBoolean();
                nullsFirst[i] = in.readOptionalBoolean();
            }
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(downstreamExecutionNodeId);

        out.writeVInt(downstreamNodes.size());
        for (String downstreamNode : downstreamNodes) {
            out.writeString(downstreamNode);
        }

        out.writeVInt(numUpstreams);

        int numCols = inputTypes.size();
        out.writeVInt(numCols);
        for (DataType inputType : inputTypes) {
            DataTypes.toStream(inputType, out);
        }

        if (executionNodes == null) {
            out.writeVInt(0);
        } else {
            out.writeVInt(executionNodes.size());
            for (String node : executionNodes) {
                out.writeString(node);
            }
        }

        out.writeBoolean(sortedInputOutput);
        if (sortedInputOutput) {
            out.writeVInt(orderByIndices.length);
            for (int i = 0; i < orderByIndices.length; i++) {
                out.writeVInt(orderByIndices[i]);
                out.writeBoolean(reverseFlags[i]);
                out.writeOptionalBoolean(nullsFirst[i]);
            }
        }
    }

    @Override
    public String toString() {
        MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this)
                .add("executionNodeId", executionNodeId())
                .add("name", name())
                .add("projections", projections)
                .add("outputTypes", outputTypes)
                .add("jobId", jobId())
                .add("numUpstreams", numUpstreams)
                .add("executionNodes", executionNodes)
                .add("inputTypes", inputTypes)
                .add("sortedInputOutput", sortedInputOutput);
        if (sortedInputOutput) {
            helper.add("orderByIndices", Arrays.toString(orderByIndices))
                  .add("reverseFlags", Arrays.toString(reverseFlags))
                  .add("nullsFirst", Arrays.toString(nullsFirst));
        }
        return helper.toString();
    }

    public void downstreamExecutionNodeId(int downstreamExecutionNodeId) {
        this.downstreamExecutionNodeId = downstreamExecutionNodeId;
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/QueryAndFetch.java
package io.crate.planner.node.dql;


import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanNodeBuilder;
import io.crate.planner.PlanVisitor;
import io.crate.planner.projection.Projection;

import javax.annotation.Nullable;

public class QueryAndFetch extends PlanAndPlannedAnalyzedRelation {

    private final CollectNode collectNode;
    private MergeNode localMergeNode;

    public QueryAndFetch(CollectNode collectNode, @Nullable MergeNode localMergeNode){
        this.collectNode = collectNode;
        this.localMergeNode = localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitQueryAndFetch(this, context);
    }

    public CollectNode collectNode() {
        return collectNode;
    }

    public MergeNode localMergeNode(){
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
        DQLPlanNode node = resultNode();
        node.addProjection(projection);
        if (node instanceof CollectNode) {
            PlanNodeBuilder.setOutputTypes((CollectNode)node);
        } else if (node instanceof MergeNode) {
            PlanNodeBuilder.connectTypes(collectNode, node);
        }
    }

    @Override
    public boolean resultIsDistributed() {
        return localMergeNode == null;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? collectNode : localMergeNode;
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/join/NestedLoop.java
/*
* Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
* license agreements. See the NOTICE file distributed with this work for
* additional information regarding copyright ownership. Crate licenses
* this file to you under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License. You may
* obtain a copy of the License at
*
*   http://www.apache.org/licenses/LICENSE-2.0
*
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
* WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
* License for the specific language governing permissions and limitations
* under the License.
*
* However, if you have executed another commercial license agreement
* with Crate these terms will supersede the license and you may use the
* software solely pursuant to the terms of the relevant commercial agreement.
*/
package io.crate.planner.node.dql.join;

import io.crate.analyze.relations.PlannedAnalyzedRelation;
import io.crate.planner.PlanAndPlannedAnalyzedRelation;
import io.crate.planner.PlanVisitor;
import io.crate.planner.node.dql.DQLPlanNode;
import io.crate.planner.node.dql.MergeNode;
import io.crate.planner.projection.Projection;
import org.elasticsearch.common.Nullable;

/**
 * Plan Node that will be executed with the awesome nested loop algorithm
 * performing CROSS JOINs
 *
 * This Node makes a lot of assumptions:
 *
 * <ul>
 * <li> limit and offset are already pushed down to left and right plan nodes
 * <li> where clause is already splitted to left and right plan nodes
 * <li> order by symbols are already splitted, too
 * <li> if the first order by symbol in the whole statement is from the left node,
 *      set <code>leftOuterLoop</code> to true, otherwise to false
 *
 * </ul>
 *
 * Properties:
 *
 * <ul>
 * <li> the resulting outputs from the join operations are the same, no matter if
 *      <code>leftOuterLoop</code> is true or not - so the projections added,
 *      can assume the same order of symbols, first symbols from left, then from right.
 *      If sth. else is selected a projection has to reorder those.
 *
 */
public class NestedLoop extends PlanAndPlannedAnalyzedRelation {


    private final PlannedAnalyzedRelation left;
    private final PlannedAnalyzedRelation right;
    private final NestedLoopNode nestedLoopNode;
    @Nullable
    private MergeNode leftMergeNode;
    @Nullable
    private MergeNode rightMergeNode;
    @Nullable
    private MergeNode localMergeNode;

    private boolean leftOuterLoop = true;

    /**
     * create a new NestedLoop
     *
     * @param leftOuterLoop if true, indicating that we have to iterate the left
     * side in the outer loop, the right in the inner.
     * Resulting in rows like:
     *
     * a | 1
     * a | 2
     * a | 3
     * b | 1
     * b | 2
     * b | 3
     *
     * This is the case if the left relation is referenced
     * by the first order by symbol references. E.g.
     * for <code>ORDER BY left.a, right.b</code>
     * If false, the nested loop is executed the other way around.
     * With the following results:
     *
     * a | 1
     * b | 1
     * a | 2
     * b | 2
     * a | 3
     * b | 3
     */
    public NestedLoop(PlannedAnalyzedRelation left,
                      PlannedAnalyzedRelation right,
                      NestedLoopNode nestedLoopNode,
                      boolean leftOuterLoop) {
        this.leftOuterLoop = leftOuterLoop;
        this.left = left;
        this.right = right;
        this.nestedLoopNode = nestedLoopNode;
    }

    public PlannedAnalyzedRelation left() {
        return left;
    }

    public PlannedAnalyzedRelation right() {
        return right;
    }

    public PlannedAnalyzedRelation inner() {
        return leftOuterLoop() ? right : left;
    }

    public PlannedAnalyzedRelation outer() {
        return leftOuterLoop() ? left : right;
    }

    public boolean leftOuterLoop() {
        return leftOuterLoop;
    }

    public void leftMergeNode(MergeNode leftMergeNode) {
        this.leftMergeNode = leftMergeNode;
    }

    @Nullable
    public MergeNode leftMergeNode() {
        return leftMergeNode;
    }

    public void rightMergeNode(MergeNode rightMergeNode) {
        this.rightMergeNode = rightMergeNode;
    }

    @Nullable
    public MergeNode rightMergeNode() {
        return rightMergeNode;
    }

    public void localMergeNode(MergeNode localMergeNode) {
        this.localMergeNode = localMergeNode;
    }

    public NestedLoopNode nestedLoopNode() {
        return nestedLoopNode;
    }

    @Nullable
    public MergeNode localMergeNode() {
        return localMergeNode;
    }

    @Override
    public void addProjection(Projection projection) {
    }

    @Override
    public boolean resultIsDistributed() {
        return false;
    }

    @Override
    public DQLPlanNode resultNode() {
        return localMergeNode == null ? outer().resultNode() : localMergeNode;
    }

    @Override
    public <C, R> R accept(PlanVisitor<C, R> visitor, C context) {
        return visitor.visitNestedLoop(this, context);
    }
}


File: sql/src/main/java/io/crate/planner/node/dql/join/NestedLoopNode.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql.join;

import com.google.common.base.MoreObjects;
import com.google.common.collect.ImmutableList;
import com.google.common.collect.ImmutableSet;
import io.crate.planner.node.ExecutionNodeVisitor;
import io.crate.planner.node.PlanNodeVisitor;
import io.crate.planner.node.dql.AbstractDQLPlanNode;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.io.stream.StreamOutput;

import java.io.IOException;
import java.util.*;

public class NestedLoopNode extends AbstractDQLPlanNode {

    public static final ExecutionNodeFactory<NestedLoopNode> FACTORY = new ExecutionNodeFactory<NestedLoopNode>() {
        @Override
        public NestedLoopNode create() {
            return new NestedLoopNode();
        }
    };

    private int downstreamExecutionNodeId = NO_EXECUTION_NODE;
    private Set<String> executionNodes;
    private List<String> downstreamNodes = ImmutableList.of();

    private int leftExecutionNodeId;
    private int rightExecutionNodeId;

    private List<DataType> leftInputTypes;
    private List<DataType> rightInputTypes;

    public NestedLoopNode() {}

    public NestedLoopNode(int leftExecutionNodeId, int rightExecutionNodeId, String name) {
        super(0, name); // TODO: remove passing default execution Node id
        this.leftExecutionNodeId = leftExecutionNodeId;
        this.rightExecutionNodeId = rightExecutionNodeId;
    }

    @Override
    public Type type() {
        return Type.NESTED_LOOP;
    }

    @Override
    public Set<String> executionNodes() {
        if (executionNodes == null) {
            return ImmutableSet.of();
        } else {
            return executionNodes;
        }
    }

    public void executionNodes(Set<String> executionNodes) {
        this.executionNodes = executionNodes;
    }

    @Override
    public List<String> downstreamNodes() {
        return downstreamNodes;
    }

    public void downstreamNodes(Set<String> nodes) {
        downstreamNodes = ImmutableList.copyOf(nodes);
    }

    @Override
    public int downstreamExecutionNodeId() {
        return downstreamExecutionNodeId;
    }

    public void downstreamExecutionNodeId(int downstreamExecutionNodeId) {
        this.downstreamExecutionNodeId = downstreamExecutionNodeId;
    }

    public List<DataType> leftInputTypes() {
        return leftInputTypes;
    }

    public void leftInputTypes(List<DataType> leftInputTypes) {
        this.leftInputTypes = leftInputTypes;
    }

    public List<DataType> rightInputTypes() {
        return rightInputTypes;
    }

    public void rightInputTypes(List<DataType> rightInputTypes) {
        this.rightInputTypes = rightInputTypes;
    }

    public int leftExecutionNodeId() {
        return leftExecutionNodeId;
    }

    public int rightExecutionNodeId() {
        return rightExecutionNodeId;
    }

    @Override
    public List<DataType> inputTypes() {
        throw new UnsupportedOperationException("inputsTypes not supported. " +
                "Use leftInputTypes() or rightInputTypes()");
    }

    @Override
    public void inputTypes(List<DataType> inputTypes) {
        throw new UnsupportedOperationException("inputsTypes not supported. " +
                "Use leftInputTypes() or rightInputTypes()");
    }

    @Override
    public int executionNodeId() {
        throw  new UnsupportedOperationException("executionNodeId() not supported. " +
                "Use leftExecutionNOdeId() or rightExecutionNodeId()");
    }

    @Override
    public <C, R> R accept(ExecutionNodeVisitor<C, R> visitor, C context) {
        return visitor.visitNestedLoopNode(this, context);
    }


    @Override
    public <C, R> R accept(PlanNodeVisitor<C, R> visitor, C context) {
        return visitor.visitNestedLoopNode(this, context);
    }

    @Override
    public void readFrom(StreamInput in) throws IOException {
        super.readFrom(in);
        leftExecutionNodeId = in.readVInt();
        rightExecutionNodeId = in.readVInt();
        downstreamExecutionNodeId = in.readVInt();

        int numDownstreamNodes = in.readVInt();
        downstreamNodes = new ArrayList<>(numDownstreamNodes);
        for (int i = 0; i < numDownstreamNodes; i++) {
            downstreamNodes.add(in.readString());
        }

        int leftNumCols = in.readVInt();
        if (leftNumCols > 0) {
            leftInputTypes = new ArrayList<>(leftNumCols);
            for (int i = 0; i < leftNumCols; i++) {
                leftInputTypes.add(DataTypes.fromStream(in));
            }
        }

        int rightNumCols = in.readVInt();
        if (rightNumCols > 0) {
            rightInputTypes = new ArrayList<>(rightNumCols);
            for (int i = 0; i < rightNumCols; i++) {
                rightInputTypes.add(DataTypes.fromStream(in));
            }
        }
        int numExecutionNodes = in.readVInt();
        if (numExecutionNodes > 0) {
            executionNodes = new HashSet<>(numExecutionNodes);
            for (int i = 0; i < numExecutionNodes; i++) {
                executionNodes.add(in.readString());
            }
        }
    }

    @Override
    public void writeTo(StreamOutput out) throws IOException {
        super.writeTo(out);
        out.writeVInt(leftExecutionNodeId);
        out.writeVInt(rightExecutionNodeId);
        out.writeVInt(downstreamExecutionNodeId);
        out.writeVInt(downstreamNodes.size());
        for (String downstreamNode : downstreamNodes) {
            out.writeString(downstreamNode);
        }

        int leftNumCols = leftInputTypes().size();
        out.writeVInt(leftNumCols);
        for (DataType inputType : leftInputTypes) {
            DataTypes.toStream(inputType, out);
        }

        int rightNumCols = rightInputTypes().size();
        out.writeVInt(rightNumCols);
        for (DataType inputType : rightInputTypes) {
            DataTypes.toStream(inputType, out);
        }

        if (executionNodes == null) {
            out.writeVInt(0);
        } else {
            out.writeVInt(executionNodes.size());
            for (String node : executionNodes) {
                out.writeString(node);
            }
        }
    }

    @Override
    public String toString() {
        MoreObjects.ToStringHelper helper = MoreObjects.toStringHelper(this)
                .add("executionNodeId", executionNodeId())
                .add("name", name())
                .add("outputTypes", outputTypes)
                .add("jobId", jobId())
                .add("executionNodes", executionNodes)
                .add("leftInputTypes", leftInputTypes)
                .add("rightInputTypes", rightInputTypes);
        return helper.toString();
    }
}


File: sql/src/test/java/io/crate/executor/transport/BaseTransportExecutorTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import io.crate.analyze.QuerySpec;
import io.crate.analyze.WhereClause;
import io.crate.analyze.where.DocKeys;
import io.crate.integrationtests.SQLTransportIntegrationTest;
import io.crate.integrationtests.Setup;
import io.crate.metadata.ColumnIdent;
import io.crate.metadata.ReferenceIdent;
import io.crate.metadata.ReferenceInfo;
import io.crate.metadata.TableIdent;
import io.crate.metadata.doc.DocSchemaInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.planner.RowGranularity;
import io.crate.planner.node.dql.ESGetNode;
import io.crate.planner.symbol.Literal;
import io.crate.planner.symbol.Reference;
import io.crate.planner.symbol.Symbol;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataTypes;
import org.junit.After;
import org.junit.Before;

import java.util.ArrayList;
import java.util.List;

public class BaseTransportExecutorTest extends SQLTransportIntegrationTest {

    Setup setup = new Setup(sqlExecutor);

    static {
        ClassLoader.getSystemClassLoader().setDefaultAssertionStatus(true);
    }

    TransportExecutor executor;
    DocSchemaInfo docSchemaInfo;

    TableIdent charactersIdent = new TableIdent(null, "characters");
    TableIdent booksIdent = new TableIdent(null, "books");

    Reference idRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference nameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(charactersIdent, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference femaleRef = TestingHelpers.createReference(charactersIdent.name(), new ColumnIdent("female"), DataTypes.BOOLEAN);

    TableIdent partedTable = new TableIdent("doc", "parted");
    Reference partedIdRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "id"), RowGranularity.DOC, DataTypes.INTEGER));
    Reference partedNameRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "name"), RowGranularity.DOC, DataTypes.STRING));
    Reference partedDateRef = new Reference(new ReferenceInfo(
            new ReferenceIdent(partedTable, "date"), RowGranularity.PARTITION, DataTypes.TIMESTAMP));

    public static ESGetNode newGetNode(TableInfo tableInfo, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        QuerySpec querySpec = new QuerySpec();
        querySpec.outputs(outputs);
        List<List<Symbol>> keys = new ArrayList<>(singleStringKeys.size());
        for (String v : singleStringKeys) {
            keys.add(ImmutableList.<Symbol>of(Literal.newLiteral(v)));
        }
        WhereClause whereClause = new WhereClause(null, new DocKeys(keys, false, -1, null), null);
        querySpec.where(whereClause);
        return new ESGetNode(executionNodeId, tableInfo, querySpec);
    }

    @Before
    public void transportSetUp() {
        executor = internalCluster().getInstance(TransportExecutor.class);
        docSchemaInfo = internalCluster().getInstance(DocSchemaInfo.class);
    }

    @After
    public void transportTearDown() {
        executor = null;
        docSchemaInfo = null;
    }
}


File: sql/src/test/java/io/crate/executor/transport/TransportExecutorTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Lists;
import com.google.common.util.concurrent.ListenableFuture;
import io.crate.Constants;
import io.crate.analyze.OrderBy;
import io.crate.analyze.WhereClause;
import io.crate.core.collections.Bucket;
import io.crate.executor.Job;
import io.crate.executor.RowCountResult;
import io.crate.executor.Task;
import io.crate.executor.TaskResult;
import io.crate.executor.transport.task.KillTask;
import io.crate.executor.transport.task.SymbolBasedUpsertByIdTask;
import io.crate.executor.transport.task.elasticsearch.ESDeleteByQueryTask;
import io.crate.executor.transport.task.elasticsearch.ESGetTask;
import io.crate.metadata.*;
import io.crate.metadata.doc.DocSysColumns;
import io.crate.metadata.doc.DocTableInfo;
import io.crate.metadata.table.TableInfo;
import io.crate.operation.aggregation.impl.CountAggregation;
import io.crate.operation.operator.EqOperator;
import io.crate.operation.projectors.TopN;
import io.crate.operation.scalar.DateTruncFunction;
import io.crate.planner.*;
import io.crate.planner.node.dml.ESDeleteByQueryNode;
import io.crate.planner.node.dml.SymbolBasedUpsertByIdNode;
import io.crate.planner.node.dml.Upsert;
import io.crate.planner.node.dql.*;
import io.crate.planner.node.management.KillPlan;
import io.crate.planner.projection.*;
import io.crate.planner.symbol.*;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.cluster.routing.operation.plain.Preference;
import org.elasticsearch.common.collect.MapBuilder;
import org.elasticsearch.search.SearchHits;
import org.junit.Before;
import org.junit.Rule;
import org.junit.Test;
import org.junit.rules.ExpectedException;

import java.util.*;

import static io.crate.testing.TestingHelpers.isRow;
import static java.util.Arrays.asList;
import static org.hamcrest.Matchers.*;
import static org.hamcrest.core.Is.is;

public class TransportExecutorTest extends BaseTransportExecutorTest {

    @Rule
    public ExpectedException expectedException = ExpectedException.none();

    @Before
    public void setup() {
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, String singleStringKey, int executionNodeId) {
        return newGetNode(tableName, outputs, Collections.singletonList(singleStringKey), executionNodeId);
    }

    protected ESGetNode newGetNode(String tableName, List<Symbol> outputs, List<String> singleStringKeys, int executionNodeId) {
        return newGetNode(docSchemaInfo.getTableInfo(tableName), outputs, singleStringKeys, executionNodeId);
    }

    @Test
    public void testESGetTask() throws Exception {
        setup.setUpCharacters();

        // create plan
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);

        // validate tasks
        assertThat(job.tasks().size(), is(1));
        Task task = job.tasks().get(0);
        assertThat(task, instanceOf(ESGetTask.class));

        // execute and validate results
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    @Test
    public void testESGetTaskWithDynamicReference() throws Exception {
        setup.setUpCharacters();

        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, new DynamicReference(
                new ReferenceIdent(new TableIdent(null, "characters"), "foo"), RowGranularity.DOC));
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, "2", ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, null)));
    }

    @Test
    public void testESMultiGet() throws Exception {
        setup.setUpCharacters();
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        Planner.Context ctx = new Planner.Context(clusterService());
        ESGetNode node = newGetNode("characters", outputs, asList("1", "2"), ctx.nextExecutionNodeId());
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects.size(), is(2));
    }

    @Test
    public void testQTFTask() throws Exception {
        // select id, name from characters;
        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection((DocTableInfo) characters, (List<Symbol>) collectSymbols, (List<Symbol>) outputSymbols, (CollectNode) collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFilter() throws Exception {
        // select id, name from characters where name = 'Ford';
        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");
        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(nameRef, Literal.newLiteral("Ford")));

        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, "Ford")));
    }

    private FetchProjection getFetchProjection(DocTableInfo characters, List<Symbol> collectSymbols, List<Symbol> outputSymbols, CollectNode collectNode, Planner.Context ctx) {
        Map<Integer, List<String>> executionNodes = new HashMap<>();
        executionNodes.put(collectNode.executionNodeId(), new ArrayList<>(collectNode.executionNodes()));
        return new FetchProjection(
                ctx.jobSearchContextIdToExecutionNodeId(),
                new InputColumn(0, DataTypes.STRING), collectSymbols, outputSymbols,
                characters.partitionedByColumns(),
                executionNodes,
                5,
                false,
                ctx.jobSearchContextIdToNode(),
                ctx.jobSearchContextIdToShard()
        );
    }

    @Test
    public void testQTFTaskOrdered() throws Exception {
        // select id, name from characters order by name, female;
        setup.setUpCharacters();
        DocTableInfo characters = docSchemaInfo.getTableInfo("characters");

        OrderBy orderBy = new OrderBy(Arrays.<Symbol>asList(nameRef, femaleRef),
                new boolean[]{false, false},
                new Boolean[]{false, false});

        ReferenceInfo docIdRefInfo = characters.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        // add nameRef and femaleRef to collectSymbols because this are ordered by values
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo), nameRef, femaleRef);
        List<Symbol> outputSymbols = Lists.<Symbol>newArrayList(idRef, nameRef);

        MergeProjection mergeProjection = new MergeProjection(
                collectSymbols,
                orderBy.orderBySymbols(),
                orderBy.reverseFlags(),
                orderBy.nullsFirst()
        );
        Planner.Context ctx = new Planner.Context(clusterService());

        CollectNode collectNode = PlanNodeBuilder.collect(
                characters,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                orderBy,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.projections(ImmutableList.<Projection>of(mergeProjection));
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(characters, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.sortedLocalMerge(
                ImmutableList.<Projection>of(fetchProjection),
                orderBy,
                collectSymbols,
                null,
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(
                isRow(1, "Arthur"),
                isRow(4, "Arthur"),
                isRow(2, "Ford"),
                isRow(3, "Trillian")
        ));
    }

    @Test
    public void testQTFTaskWithFunction() throws Exception {
        // select id, date_trunc('day', date) from searchf where id = 2;
        execute("create table searchf (id int primary key, date timestamp) with (number_of_replicas=0)");
        ensureGreen();
        execute("insert into searchf (id, date) values (1, '1980-01-01'), (2, '1980-01-02')");
        refresh();

        Reference id_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "id"),
                RowGranularity.DOC,
                DataTypes.INTEGER
        ));
        Reference date_ref = new Reference(new ReferenceInfo(
                new ReferenceIdent(
                        new TableIdent(ReferenceInfos.DEFAULT_SCHEMA_NAME, "searchf"),
                        "date"),
                RowGranularity.DOC,
                DataTypes.TIMESTAMP
        ));
        Function function = new Function(new FunctionInfo(
                new FunctionIdent(DateTruncFunction.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.TIMESTAMP)),
                DataTypes.TIMESTAMP
        ), Arrays.asList(Literal.newLiteral("month"), date_ref));
        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.INTEGER, DataTypes.INTEGER)),
                DataTypes.BOOLEAN),
                Arrays.asList(id_ref, Literal.newLiteral(2))
        );

        DocTableInfo searchf = docSchemaInfo.getTableInfo("searchf");
        ReferenceInfo docIdRefInfo = searchf.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));

        Planner.Context ctx = new Planner.Context(clusterService());
        List<Symbol> collectSymbols = ImmutableList.<Symbol>of(new Reference(docIdRefInfo));
        CollectNode collectNode = PlanNodeBuilder.collect(
                searchf,
                ctx,
                new WhereClause(whereClause),
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        TopNProjection topN = new TopNProjection(2, TopN.NO_OFFSET);
        topN.outputs(Collections.<Symbol>singletonList(new InputColumn(0)));

        FetchProjection fetchProjection = getFetchProjection(searchf, collectSymbols, Arrays.asList(id_ref, function), collectNode, ctx);

        MergeNode mergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.of(topN, fetchProjection),
                collectNode,
                ctx);
        Plan plan = new QueryThenFetch(collectNode, mergeNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().size(), is(1));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, contains(isRow(2, 315532800000L)));
    }

    @Test
    public void testQTFTaskPartitioned() throws Exception {
        setup.setUpPartitionedTableWithName();
        DocTableInfo parted = docSchemaInfo.getTableInfo("parted");
        Planner.Context ctx = new Planner.Context(clusterService());

        ReferenceInfo docIdRefInfo = parted.getReferenceInfo(new ColumnIdent(DocSysColumns.DOCID.name()));
        List<Symbol> collectSymbols = Lists.<Symbol>newArrayList(new Reference(docIdRefInfo));
        List<Symbol> outputSymbols =  Arrays.<Symbol>asList(partedIdRef, partedNameRef, partedDateRef);

        CollectNode collectNode = PlanNodeBuilder.collect(
                parted,
                ctx,
                WhereClause.MATCH_ALL,
                collectSymbols,
                ImmutableList.<Projection>of(),
                null,
                Constants.DEFAULT_SELECT_LIMIT
        );
        collectNode.keepContextForFetcher(true);

        FetchProjection fetchProjection = getFetchProjection(parted, collectSymbols, outputSymbols, collectNode, ctx);

        MergeNode localMergeNode = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(fetchProjection),
                collectNode,
                ctx);

        Plan plan = new QueryThenFetch(collectNode, localMergeNode);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        Bucket rows = result.get(0).get().rows();
        assertThat(rows, containsInAnyOrder(
                isRow(3, "Ford", 1396388720242L),
                isRow(1, "Trillian", null),
                isRow(2, null, 0L)
        ));
    }

    @Test
    public void testESDeleteByQueryTask() throws Exception {
        setup.setUpCharacters();

        Function whereClause = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.STRING, DataTypes.STRING)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(idRef, Literal.newLiteral(2)));

        ESDeleteByQueryNode node = new ESDeleteByQueryNode(
                1,
                ImmutableList.of(new String[]{"characters"}),
                ImmutableList.of(new WhereClause(whereClause)));
        Plan plan = new IterablePlan(node);
        Job job = executor.newJob(plan);
        ESDeleteByQueryTask task = (ESDeleteByQueryTask) job.tasks().get(0);

        task.start();
        TaskResult taskResult = task.result().get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(-1L)));

        // verify deletion
        execute("select * from characters where id = 2");
        assertThat(response.rowCount(), is(0L));
    }

    @Test
    public void testInsertWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();

        /* insert into characters (id, name) values (99, 'Marvin'); */
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});
        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));

        // verify insertion
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "99", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(99, "Marvin")));
    }

    @Test
    public void testInsertIntoPartitionedTableWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table parted (" +
                "  id int, " +
                "  name string, " +
                "  date timestamp" +
                ") partitioned by (date)");
        ensureGreen();

        /* insert into parted (id, name, date) values(0, 'Trillian', 13959981214861); */
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                true,
                false,
                null,
                new Reference[]{idRef, nameRef});

        PartitionName partitionName = new PartitionName("parted", Arrays.asList(new BytesRef("13959981214861")));
        updateNode.add(partitionName.stringValue(), "123", "123", null, null, new Object[]{0L, new BytesRef("Trillian")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket indexResult = taskResult.rows();
        assertThat(indexResult, contains(isRow(1L)));

        refresh();

        assertTrue(
                client().admin().indices().prepareExists(partitionName.stringValue())
                        .execute().actionGet().isExists()
        );
        assertTrue(
                client().admin().indices().prepareAliasesExist("parted")
                        .execute().actionGet().exists()
        );
        SearchHits hits = client().prepareSearch(partitionName.stringValue())
                .setTypes(Constants.DEFAULT_MAPPING_TYPE)
                .addFields("id", "name")
                .setQuery(new MapBuilder<String, Object>()
                                .put("match_all", new HashMap<String, Object>())
                                .map()
                ).execute().actionGet().getHits();
        assertThat(hits.getTotalHits(), is(1L));
        assertThat((Integer) hits.getHits()[0].field("id").getValues().get(0), is(0));
        assertThat((String) hits.getHits()[0].field("name").getValues().get(0), is("Trillian"));
    }

    @Test
    public void testInsertMultiValuesWithSymbolBasedUpsertByIdTask() throws Exception {
        execute("create table characters (id int primary key, name string)");
        ensureGreen();

        /* insert into characters (id, name) values (99, 'Marvin'), (42, 'Deep Thought'); */
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                null,
                new Reference[]{idRef, nameRef});

        updateNode.add("characters", "99", "99", null, null, new Object[]{99, new BytesRef("Marvin")});
        updateNode.add("characters", "42", "42", null, null, new Object[]{42, new BytesRef("Deep Thought")});

        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));

        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(2L)));

        // verify insertion
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, Arrays.asList("99", "42"), ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(
                isRow(99, "Marvin"),
                isRow(42, "Deep Thought")
        ));
    }

    @Test
    public void testUpdateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();

        // update characters set name='Vogon lyric fan' where id=1
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(), false, false, new String[]{nameRef.ident().columnIdent().fqn()}, null);
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral("Vogon lyric fan")}, null);
        Plan plan = new IterablePlan(updateNode);

        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));

        // verify update
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Vogon lyric fan")));
    }

    @Test
    public void testInsertOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();
        /* insert into characters (id, name, female) values (5, 'Zaphod Beeblebrox', false)
           on duplicate key update set name = 'Zaphod Beeblebrox'; */
        Object[] missingAssignments = new Object[]{5, new BytesRef("Zaphod Beeblebrox"), false};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{nameRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});

        updateNode.add("characters", "5", "5", new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));

        // verify insert
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "5", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();
        assertThat(objects, contains(isRow(5, "Zaphod Beeblebrox", false)));

    }

    @Test
    public void testUpdateOnDuplicateWithSymbolBasedUpsertByIdTask() throws Exception {
        setup.setUpCharacters();
        /* insert into characters (id, name, female) values (1, 'Zaphod Beeblebrox', false)
           on duplicate key update set name = 'Zaphod Beeblebrox'; */
        Object[] missingAssignments = new Object[]{1, new BytesRef("Zaphod Beeblebrox"), true};
        Planner.Context ctx = new Planner.Context(clusterService());
        SymbolBasedUpsertByIdNode updateNode = new SymbolBasedUpsertByIdNode(
                ctx.nextExecutionNodeId(),
                false,
                false,
                new String[]{femaleRef.ident().columnIdent().fqn()},
                new Reference[]{idRef, nameRef, femaleRef});
        updateNode.add("characters", "1", "1", new Symbol[]{Literal.newLiteral(true)}, null, missingAssignments);
        Plan plan = new IterablePlan(updateNode);
        Job job = executor.newJob(plan);
        assertThat(job.tasks().get(0), instanceOf(SymbolBasedUpsertByIdTask.class));
        List<? extends ListenableFuture<TaskResult>> result = executor.execute(job);
        TaskResult taskResult = result.get(0).get();
        Bucket rows = taskResult.rows();
        assertThat(rows, contains(isRow(1L)));

        // verify update
        ImmutableList<Symbol> outputs = ImmutableList.<Symbol>of(idRef, nameRef, femaleRef);
        ESGetNode getNode = newGetNode("characters", outputs, "1", ctx.nextExecutionNodeId());
        plan = new IterablePlan(getNode);
        job = executor.newJob(plan);
        result = executor.execute(job);
        Bucket objects = result.get(0).get().rows();

        assertThat(objects, contains(isRow(1, "Arthur", true)));
    }

    @Test
    public void testBulkUpdateByQueryTask() throws Exception {
        setup.setUpCharacters();
        /* update characters set name 'Zaphod Beeblebrox' where female = false
           update characters set name 'Zaphod Beeblebrox' where female = true
         */

        List<Plan> childNodes = new ArrayList<>();
        Planner.Context plannerContext = new Planner.Context(clusterService());

        TableInfo tableInfo = docSchemaInfo.getTableInfo("characters");
        Reference uidReference = new Reference(
                new ReferenceInfo(
                        new ReferenceIdent(tableInfo.ident(), "_uid"),
                        RowGranularity.DOC, DataTypes.STRING));

        // 1st collect and merge nodes
        Function whereClause1 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        UpdateProjection updateProjection = new UpdateProjection(
                new InputColumn(0, DataTypes.STRING),
                new String[]{"name"},
                new Symbol[]{Literal.newLiteral("Zaphod Beeblebrox")},
                null);

        CollectNode collectNode1 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause1),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode1 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode1,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode1, mergeNode1));

        // 2nd collect and merge nodes
        Function whereClause2 = new Function(new FunctionInfo(
                new FunctionIdent(EqOperator.NAME, Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.BOOLEAN)),
                DataTypes.BOOLEAN),
                Arrays.<Symbol>asList(femaleRef, Literal.newLiteral(true)));

        CollectNode collectNode2 = PlanNodeBuilder.collect(
                tableInfo,
                plannerContext,
                new WhereClause(whereClause2),
                ImmutableList.<Symbol>of(uidReference),
                ImmutableList.<Projection>of(updateProjection),
                null,
                Preference.PRIMARY.type()
        );
        MergeNode mergeNode2 = PlanNodeBuilder.localMerge(
                ImmutableList.<Projection>of(CountAggregation.PARTIAL_COUNT_AGGREGATION_PROJECTION), collectNode2,
                plannerContext);
        childNodes.add(new CollectAndMerge(collectNode2, mergeNode2));

        Upsert plan = new Upsert(childNodes);
        Job job = executor.newJob(plan);

        assertThat(job.tasks().size(), is(1));
        assertThat(job.tasks().get(0), instanceOf(ExecutionNodesTask.class));
        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results.size(), is(2));

        for (int i = 0; i < results.size(); i++) {
            TaskResult result = results.get(i).get();
            assertThat(result, instanceOf(RowCountResult.class));
            // each of the bulk request hits 2 records
            assertThat(((RowCountResult)result).rowCount(), is(2L));
        }
    }

    @Test
    public void testKillTask() throws Exception {
        Job job = executor.newJob(KillPlan.INSTANCE);
        assertThat(job.tasks(), hasSize(1));
        assertThat(job.tasks().get(0), instanceOf(KillTask.class));

        List<? extends ListenableFuture<TaskResult>> results = executor.execute(job);
        assertThat(results, hasSize(1));
        results.get(0).get();
    }
}


File: sql/src/test/java/io/crate/executor/transport/distributed/DistributingDownstreamTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport.distributed;

import io.crate.Constants;
import io.crate.Streamer;
import io.crate.core.collections.Row1;
import io.crate.test.integration.CrateUnitTest;
import io.crate.testing.TestingHelpers;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.action.ActionListener;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
import org.mockito.ArgumentCaptor;
import org.mockito.Captor;
import org.mockito.MockitoAnnotations;

import java.util.Arrays;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.CancellationException;

import static org.hamcrest.Matchers.is;
import static org.mockito.Matchers.any;
import static org.mockito.Matchers.eq;
import static org.mockito.Mockito.*;

public class DistributingDownstreamTest extends CrateUnitTest {

    private TransportDistributedResultAction distributedResultAction;
    private DistributingDownstream downstream;

    @Captor
    public ArgumentCaptor<ActionListener<DistributedResultResponse>> listenerArgumentCaptor;
    private int originalPageSize;

    @Before
    public void before() throws Exception {
        originalPageSize = Constants.PAGE_SIZE;
        MockitoAnnotations.initMocks(this);

        List<String> downstreamNodes = Arrays.asList("n1", "n2");
        distributedResultAction = mock(TransportDistributedResultAction.class);
        Streamer<?>[] streamers = {DataTypes.STRING.streamer()};
        downstream = new DistributingDownstream(
                UUID.randomUUID(),
                1,
                0,
                downstreamNodes,
                distributedResultAction,
                streamers
        );
        downstream.registerUpstream(null);
    }

    @After
    public void after() throws Exception {
        Constants.PAGE_SIZE = originalPageSize;
    }

    @Test
    public void testBucketing() throws Exception {
        ArgumentCaptor<DistributedResultRequest> r1Captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(eq("n1"), r1Captor.capture(), any(ActionListener.class));

        ArgumentCaptor<DistributedResultRequest> r2Captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(eq("n2"), r2Captor.capture(), any(ActionListener.class));


        downstream.setNextRow(new Row1(new BytesRef("Trillian")));
        downstream.setNextRow(new Row1(new BytesRef("Marvin")));
        downstream.setNextRow(new Row1(new BytesRef("Arthur")));
        downstream.setNextRow(new Row1(new BytesRef("Slartibartfast")));

        downstream.finish();

        assertRows(r2Captor, "Trillian\nMarvin\n");
        assertRows(r1Captor, "Arthur\nSlartibartfast\n");
    }

    @Test
    public void testOperationIsStoppedOnFailureResponse() throws Exception {
        Constants.PAGE_SIZE = 2;

        ArgumentCaptor<DistributedResultRequest> captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(any(String.class), captor.capture(), listenerArgumentCaptor.capture());

        int iterations = 0;
        int expected = -1;
        while (true) {
            if (!downstream.setNextRow(new Row1(new BytesRef("Trillian")))) {
                break;
            }
            List<ActionListener<DistributedResultResponse>> allValues = listenerArgumentCaptor.getAllValues();
            if (allValues.size() == 1) {
                ActionListener<DistributedResultResponse> distributedResultResponseActionListener = allValues.get(0);
                distributedResultResponseActionListener.onFailure(new IllegalStateException("epic fail"));
                expected = iterations + 1;
            }
            iterations++;
        }
        assertThat(iterations, is(expected));
    }

    @Test
    public void testRequestsAreSentWithoutRows() throws Exception {
        ArgumentCaptor<DistributedResultRequest> captor = ArgumentCaptor.forClass(DistributedResultRequest.class);
        doNothing().when(distributedResultAction).pushResult(any(String.class), captor.capture(), any(ActionListener.class));

        downstream.finish();
        assertThat(captor.getAllValues().size(), is(2));
        for (DistributedResultRequest distributedResultRequest : captor.getAllValues()) {
            assertThat(distributedResultRequest.rows().size(), is(0));
        }

    }

    @Test
    public void testNoRequestsSendWhenCancelled() throws Exception {
        downstream.setNextRow(new Row1(new BytesRef("LateNightSprintFinishingAwesomeness")));
        downstream.fail(new CancellationException());

        verify(distributedResultAction, never()).pushResult(any(String.class), any(DistributedResultRequest.class), any(ActionListener.class));

    }

    private void assertRows(ArgumentCaptor<DistributedResultRequest> r2Captor, String expectedRows) {
        List<DistributedResultRequest> allRequestsForNodeN1 = r2Captor.getAllValues();
        assertThat(allRequestsForNodeN1.size(), is(1));
        DistributedResultRequest n1Request = allRequestsForNodeN1.get(0);
        assertThat(n1Request.isLast(), is(true));
        assertThat(n1Request.rowsCanBeRead(), is(true));

        assertThat(TestingHelpers.printedTable(n1Request.rows()), is(expectedRows));
    }
}

File: sql/src/test/java/io/crate/executor/transport/merge/DistributedResultRequestTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.executor.transport.merge;

import io.crate.Streamer;
import io.crate.core.collections.ArrayBucket;
import io.crate.executor.transport.distributed.DistributedResultRequest;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataTypes;
import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.junit.Test;

import java.util.UUID;

import static io.crate.testing.TestingHelpers.isNullRow;
import static io.crate.testing.TestingHelpers.isRow;
import static org.hamcrest.Matchers.contains;

public class DistributedResultRequestTest extends CrateUnitTest {

    @Test
    public void testStreaming() throws Exception {
        Streamer<?>[] streamers = new Streamer[]{DataTypes.STRING.streamer()};

        Object[][] rows = new Object[][]{
                {new BytesRef("ab")},{null},{new BytesRef("cd")}
        };
        UUID uuid = UUID.randomUUID();

        DistributedResultRequest r1 = new DistributedResultRequest(uuid, 1, 1, streamers);
        r1.rows(new ArrayBucket(rows));

        BytesStreamOutput out = new BytesStreamOutput();
        r1.writeTo(out);
        BytesStreamInput in = new BytesStreamInput(out.bytes());
        DistributedResultRequest r2 = new DistributedResultRequest();
        r2.readFrom(in);
        r2.streamers(streamers);
        assertTrue(r2.rowsCanBeRead());

        assertEquals(r1.rows().size(), r2.rows().size());

        assertThat(r2.rows(), contains(isRow("ab"), isNullRow(), isRow("cd")));
    }
}


File: sql/src/test/java/io/crate/integrationtests/Setup.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.integrationtests;

import io.crate.Constants;
import io.crate.action.sql.SQLResponse;
import io.crate.testing.SQLTransportExecutor;
import org.elasticsearch.common.settings.ImmutableSettings;

import java.util.Arrays;
import java.util.HashMap;
import java.util.Map;

import static com.google.common.collect.Maps.newHashMap;
import static org.hamcrest.MatcherAssert.assertThat;
import static org.hamcrest.Matchers.is;

public class Setup {

    private final SQLTransportExecutor transportExecutor;

    public Setup(SQLTransportExecutor transportExecutor) {
        this.transportExecutor = transportExecutor;
    }

    public void setUpLocations() throws Exception {
        transportExecutor.exec("create table locations (" +
            " id string primary key," +
            " name string," +
            " date timestamp," +
            " kind string," +
            " position integer," +
            " description string," +
            " race object," +
            " index name_description_ft using fulltext(name, description) with (analyzer='english')" +
            ") clustered by(id) into 2 shards with(number_of_replicas=0)");

        String insertStmt = "insert into locations " +
                "(id, name, date, kind, position, description, race) " +
                "values (?, ?, ?, ?, ?, ?, ?)";
        Object[][] rows = new Object[][]{
                new Object[]{"1", "North West Ripple", "1979-10-12",
                        "Galaxy", 1, "Relative to life on NowWhat, living on an affluent " +
                        "world in the North West ripple of the Galaxy is said to be easier " +
                        "by a factor of about seventeen million.", null
                },
                new Object[]{
                        "2", "Outer Eastern Rim", "1979-10-12", "Galaxy", 2, "The Outer Eastern Rim " +
                        "of the Galaxy where the Guide has supplanted the Encyclopedia Galactica " +
                        "among its more relaxed civilisations.", null
                },
                new Object[]{
                        "3","Galactic Sector QQ7 Active J Gamma", "2013-05-01",  "Galaxy",  4,
                        "Galactic Sector QQ7 Active J Gamma contains the Sun Zarss, " +
                        "the planet Preliumtarn of the famed Sevorbeupstry and " +
                        "Quentulus Quazgar Mountains.", null
                },
                new Object[]{
                        "4", "Aldebaran", "2013-07-16",  "Star System",  1,
                        "Max Quordlepleen claims that the only thing left after the end " +
                        "of the Universe will be the sweets trolley and a fine selection " +
                        "of Aldebaran liqueurs.", null
                },
                new Object[]{
                        "5",  "Algol", "2013-07-16",  "Star System",  2,
                        "Algol is the home of the Algolian Suntiger, " +
                        "the tooth of which is one of the ingredients of the " +
                        "Pan Galactic Gargle Blaster.", null
                },
                new Object[]{
                        "6",  "Alpha Centauri", "1979-10-12",  "Star System",  3,
                        "4.1 light-years northwest of earth", null
                },
                new Object[]{
                        "7",  "Altair", "2013-07-16",  "Star System",  4,
                        "The Altairian dollar is one of three freely convertible currencies in the galaxy, " +
                        "though by the time of the novels it had apparently recently collapsed.",
                        null
                },
                new Object[]{
                        "8",  "Allosimanius Syneca", "2013-07-16",  "Planet",  1,
                        "Allosimanius Syneca is a planet noted for ice, snow, " +
                        "mind-hurtling beauty and stunning cold.", null
                },
                new Object[]{
                        "9",  "Argabuthon", "2013-07-16",  "Planet",  2,
                        "It is also the home of Prak, a man placed into solitary confinement " +
                        "after an overdose of truth drug caused him to tell the Truth in its absolute " +
                        "and final form, causing anyone to hear it to go insane.", null,
                },
                new Object[]{
                        "10",  "Arkintoofle Minor", "1979-10-12",  "Planet",  3,
                        "Motivated by the fact that the only thing in the Universe that " +
                        "travels faster than light is bad news, the Hingefreel people native " +
                        "to Arkintoofle Minor constructed a starship driven by bad news.", null
                },
                new Object[]{
                        "11",  "Bartledan", "2013-07-16",  "Planet",  4,
                        "An Earthlike planet on which Arthur Dent lived for a short time, " +
                                "Bartledan is inhabited by Bartledanians, a race that appears human but only physically.",
                        new HashMap<String, Object>(){{
                            put("name", "Bartledannians");
                            put("description", "Similar to humans, but do not breathe");
                            put("interests", "netball");
                        }}
                },
                new Object[]{
                        "12",  "", "2013-07-16",  "Planet",  5,  "This Planet doesn't really exist", null
                },
                new Object[]{
                        "13",  "End of the Galaxy", "2013-07-16",  "Galaxy",  6,  "The end of the Galaxy.%", null
                }
        };
        transportExecutor.exec(insertStmt, rows);
    }

    public void groupBySetup() throws Exception {
        groupBySetup("integer");
    }

    public void groupBySetup(String numericType) throws Exception {
        transportExecutor.exec(String.format("create table characters (" +
            " race string," +
            " gender string," +
            " age %s," +
            " birthdate timestamp," +
            " name string," +
            " details object as (job string)," +
            " details_ignored object(ignored)" +
            ")", numericType));
        transportExecutor.ensureYellowOrGreen();

        Map<String, String> details = newHashMap();
        details.put("job", "Sandwitch Maker");
        transportExecutor.exec("insert into characters (race, gender, age, birthdate, name, details) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"Human", "male", 34, "1975-10-01", "Arthur Dent", details});

        details = newHashMap();
        details.put("job", "Mathematician");
        transportExecutor.exec("insert into characters (race, gender, age, birthdate, name, details) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"Human", "female", 32, "1978-10-11", "Trillian", details});
        transportExecutor.exec("insert into characters (race, gender, age, birthdate, name, details) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"Human", "female", 43, "1970-01-01", "Anjie", null});
        transportExecutor.exec("insert into characters (race, gender, age, name) values (?, ?, ?, ?)",
            new Object[]{"Human", "male", 112, "Ford Perfect"});

        transportExecutor.exec("insert into characters (race, gender, name) values ('Android', 'male', 'Marving')");
        transportExecutor.exec("insert into characters (race, gender, name) values ('Vogon', 'male', 'Jeltz')");
        transportExecutor.exec("insert into characters (race, gender, name) values ('Vogon', 'male', 'Kwaltz')");
        transportExecutor.refresh("characters");
    }

    public void setUpEmployees() {
        transportExecutor.exec("create table employees (" +
            " name string, " +
            " department string," +
            " hired timestamp, " +
            " age short," +
            " income double, " +
            " good boolean" +
            ") with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"dilbert", "engineering", "1985-01-01", 47, 4000.0, true});
        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"wally", "engineering", "2000-01-01", 54, 6000.0, true});
        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"pointy haired boss", "management", "2010-10-10", 45, Double.MAX_VALUE, false});

        transportExecutor.exec("insert into employees (name, department, hired, age, income, good) values (?, ?, ?, ?, ?, ?)",
            new Object[]{"catbert", "HR", "1990-01-01", 12, 999999999.99, false});
        transportExecutor.exec("insert into employees (name, department, income) values (?, ?, ?)",
            new Object[]{"ratbert", "HR", 0.50});
        transportExecutor.exec("insert into employees (name, department, age) values (?, ?, ?)",
            new Object[]{"asok", "internship", 28});
        transportExecutor.refresh("employees");
    }

    public void setUpObjectTable() {
        transportExecutor.exec("create table ot (" +
                "  title string," +
                "  author object(dynamic) as (" +
                "    name object(strict) as (" +
                "      first_name string," +
                "      last_name string" +
                "    )," +
                "    age integer" +
                "  )," +
                "  details object(ignored) as (" +
                "    num_pages integer" +
                "  )" +
                ") with (number_of_replicas = 0)");
        transportExecutor.exec("insert into ot (title, author, details) values (?, ?, ?)",
                new Object[]{
                        "The Hitchhiker's Guide to the Galaxy",
                        new HashMap<String, Object>() {{
                            put("name", new HashMap<String, Object>() {{
                                put("first_name", "Douglas");
                                put("last_name", "Adams");
                            }});
                            put("age", 49);
                        }},
                        new HashMap<String, Object>() {{
                            put("num_pages", 224);
                        }}
                }
        );
        transportExecutor.refresh("ot");
    }

    public void setUpObjectMappingWithUnknownTypes() throws Exception {
        transportExecutor.prepareCreate("ut")
                .setSettings(ImmutableSettings.builder().put("number_of_replicas", 0).put("number_of_shards", 2).build())
                .addMapping(Constants.DEFAULT_MAPPING_TYPE, new HashMap<String, Object>(){{
                    put("properties", new HashMap<String, Object>(){{
                        put("name", new HashMap<String, Object>(){{
                            put("type", "string");
                            put("store", "false");
                            put("index", "not_analyzed");
                        }});
                        put("location", new HashMap<String, Object>(){{
                            put("type", "geo_shape");
                        }});
                        put("o", new HashMap<String, Object>(){{
                            put("type", "object");
                        }});
                        put("population", new HashMap<String, Object>(){{
                            put("type", "long");
                            put("store", "false");
                            put("index", "not_analyzed");
                        }});
                    }});
                }}).execute().actionGet();
        transportExecutor.client().prepareIndex("ut", Constants.DEFAULT_MAPPING_TYPE, "id1")
                .setSource("{\"name\":\"Berlin\",\"location\":{\"type\": \"point\", \"coordinates\": [52.5081, 13.4416]}, \"population\":3500000}")
                .execute().actionGet();
        transportExecutor.client().prepareIndex("ut", Constants.DEFAULT_MAPPING_TYPE, "id2")
                .setSource("{\"name\":\"Dornbirn\",\"location\":{\"type\": \"point\", \"coordinates\": [47.3904,9.7562]}, \"population\":46080}")
                .execute().actionGet();
        transportExecutor.refresh("ut");
    }

    public void setUpArrayTables() {
        transportExecutor.exec("create table any_table (" +
                "  id int primary key," +
                "  temps array(double)," +
                "  names array(string)," +
                "  tags array(string)" +
                ") with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
        SQLResponse response = transportExecutor.exec("insert into any_table (id, temps, names, tags) values (?,?,?,?), (?,?,?,?), (?,?,?,?), (?,?,?,?)",
                        1, Arrays.asList(0L, 0L, 0L), Arrays.asList("Dornbirn", "Berlin", "St. Margrethen"), Arrays.asList("cool"),
                        2, Arrays.asList(0, 1, -1), Arrays.asList("Dornbirn", "Dornbirn", "Dornbirn"), Arrays.asList("cool", null),
                        3, Arrays.asList(42, -42), Arrays.asList("Hangelsberg", "Berlin"), Arrays.asList("kuhl", "cool"),
                        4, null, null, Arrays.asList("kuhl", null)
                );
        assertThat(response.rowCount(), is(4L));
        transportExecutor.refresh("any_table");
    }

    public void partitionTableSetup() {
        transportExecutor.exec("create table parted (" +
                "id int primary key," +
                "date timestamp primary key," +
                "o object(ignored)" +
                ") partitioned by (date) with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into parted (id, date) values (1, '2014-01-01')");
        transportExecutor.exec("insert into parted (id, date) values (2, '2014-01-01')");
        transportExecutor.exec("insert into parted (id, date) values (3, '2014-02-01')");
        transportExecutor.exec("insert into parted (id, date) values (4, '2014-02-01')");
        transportExecutor.exec("insert into parted (id, date) values (5, '2014-02-01')");
        transportExecutor.refresh("parted");
    }

    public void createTestTableWithPrimaryKey() {
        transportExecutor.exec("create table test (" +
                "  pk_col string primary key, " +
                "  message string" +
                ") with (number_of_replicas=0)");
        transportExecutor.ensureGreen();
    }

    public void setUpCharacters() {
        transportExecutor.exec("create table characters (id int primary key, name string, female boolean, details object)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into characters (id, name, female) values (?, ?, ?)",
                new Object[][]{
                        new Object[] { 1, "Arthur", false},
                        new Object[] { 2, "Ford", false},
                        new Object[] { 3, "Trillian", true},
                        new Object[] { 4, "Arthur", true}
                }
        );
        transportExecutor.refresh("characters");
    }

    public void setUpPartitionedTableWithName() {
        transportExecutor.exec("create table parted (id int, name string, date timestamp) partitioned by (date)");
        transportExecutor.ensureGreen();
        transportExecutor.exec("insert into parted (id, name, date) values (?, ?, ?), (?, ?, ?), (?, ?, ?)",
                new Object[]{
                        1, "Trillian", null,
                        2, null, 0L,
                        3, "Ford", 1396388720242L
                });
        transportExecutor.ensureGreen();
        transportExecutor.refresh("parted");
    }
}


File: sql/src/test/java/io/crate/planner/node/dql/NestedLoopNodeTest.java
/*
 * Licensed to CRATE Technology GmbH ("Crate") under one or more contributor
 * license agreements.  See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.  Crate licenses
 * this file to you under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.  You may
 * obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
 * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
 * License for the specific language governing permissions and limitations
 * under the License.
 *
 * However, if you have executed another commercial license agreement
 * with Crate these terms will supersede the license and you may use the
 * software solely pursuant to the terms of the relevant commercial agreement.
 */

package io.crate.planner.node.dql;

import com.google.common.collect.ImmutableList;
import com.google.common.collect.Sets;
import io.crate.planner.node.dql.join.NestedLoopNode;
import io.crate.planner.projection.Projection;
import io.crate.planner.projection.TopNProjection;
import io.crate.test.integration.CrateUnitTest;
import io.crate.types.DataType;
import io.crate.types.DataTypes;
import org.elasticsearch.common.io.stream.BytesStreamInput;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.hamcrest.core.Is;
import org.junit.Test;

import java.util.Arrays;
import java.util.UUID;

import static org.hamcrest.CoreMatchers.is;

public class NestedLoopNodeTest extends CrateUnitTest {

    @Test
    public void testSerialization() throws Exception {

        NestedLoopNode node = new NestedLoopNode(3, 2, "nestedLoop");
        node.jobId(UUID.randomUUID());
        node.executionNodes(Sets.newHashSet("node1", "node2"));
        node.leftInputTypes(Arrays.<DataType>asList(DataTypes.UNDEFINED, DataTypes.STRING));
        node.rightInputTypes(Arrays.<DataType>asList(DataTypes.BOOLEAN, DataTypes.INTEGER, DataTypes.DOUBLE));
        node.downstreamNodes(Sets.newHashSet("node3", "node4"));
        node.downstreamExecutionNodeId(5);
        TopNProjection topNProjection = new TopNProjection(10, 0);
        node.projections(ImmutableList.<Projection>of(topNProjection));

        BytesStreamOutput output = new BytesStreamOutput();
        node.writeTo(output);

        BytesStreamInput input = new BytesStreamInput(output.bytes());
        NestedLoopNode node2 = new NestedLoopNode();
        node2.readFrom(input);

        assertThat(node.downstreamExecutionNodeId(), is(node2.downstreamExecutionNodeId()));
        assertThat(node.downstreamNodes(), is(node2.downstreamNodes()));
        assertThat(node.executionNodes(), Is.is(node2.executionNodes()));
        assertThat(node.jobId(), Is.is(node2.jobId()));
        assertThat(node.leftInputTypes(), is(node2.leftInputTypes()));
        assertThat(node.rightInputTypes(), is(node2.rightInputTypes()));
        assertThat(node.leftExecutionNodeId(), is(node2.leftExecutionNodeId()));
        assertThat(node.rightExecutionNodeId(), is(node2.rightExecutionNodeId()));
        assertThat(node.name(), is(node2.name()));
        assertThat(node.outputTypes(), is(node2.outputTypes()));
    }
}
